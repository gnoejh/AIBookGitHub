{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Model Context Provider (MCP) Server Implementation\n",
    "# This is a custom abstraction layer for interacting with various LLM APIs\n",
    "# Not to be confused with any official standard - this is a custom implementation\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import openai\n",
    "from functools import wraps\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed. Using environment variables directly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    \"api\": {\n",
    "        \"openai\": {\n",
    "            \"api_key\": os.getenv(\"OPENAI_API_KEY\", \"\"),  # get frrom .env file or environment variable\n",
    "            \"default_model\": \"gpt-4o\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 1000\n",
    "        },\n",
    "        # Placeholder for other APIs that can be added later\n",
    "        \"anthropic\": {\n",
    "            \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\", \"\"),\n",
    "            \"enabled\": False,\n",
    "            \"default_model\": \"claude-3-opus-20240229\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 1000\n",
    "        },\n",
    "        \"gemini\": {\n",
    "            \"api_key\": os.getenv(\"GEMINI_API_KEY\", \"\"),\n",
    "            \"enabled\": False\n",
    "        }\n",
    "    },\n",
    "    \"server\": {\n",
    "        \"log_level\": \"INFO\",\n",
    "        \"timeout\": 30,\n",
    "        \"retry_attempts\": 3,\n",
    "        \"retry_delay\": 2,  # seconds between retry attempts\n",
    "        \"cache_enabled\": True,\n",
    "        \"cache_ttl\": 3600  # in seconds\n",
    "    }\n",
    "}\n",
    "\n",
    "# Setup logging based on configuration\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, config['server']['log_level']),\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('MCP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Client Classes\n",
    "\n",
    "# Retry decorator for API calls\n",
    "def retry_on_exception(max_attempts, delay):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempts = 0\n",
    "            last_exception = None\n",
    "            \n",
    "            while attempts < max_attempts:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    attempts += 1\n",
    "                    last_exception = e\n",
    "                    logger.warning(f\"Attempt {attempts} failed with error: {e}. Retrying in {delay} seconds...\")\n",
    "                    if attempts < max_attempts:\n",
    "                        time.sleep(delay)\n",
    "            \n",
    "            logger.error(f\"All {max_attempts} attempts failed. Last error: {last_exception}\")\n",
    "            raise last_exception\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class BaseAPIClient:\n",
    "    \"\"\"Base class for API clients\"\"\"\n",
    "    \n",
    "    def __init__(self, api_config: Dict[str, Any], server_config: Dict[str, Any] = None):\n",
    "        self.config = api_config\n",
    "        self.server_config = server_config or {\"retry_attempts\": 3, \"retry_delay\": 2, \"timeout\": 30}\n",
    "        self.name = \"base\"\n",
    "        self.logger = logging.getLogger(f\"MCP.{self.name}\")\n",
    "    \n",
    "    def validate_config(self) -> bool:\n",
    "        \"\"\"Validate if the configuration is sufficient to use this API\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"Generate text based on a prompt\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "\n",
    "class OpenAIClient(BaseAPIClient):\n",
    "    \"\"\"Client for OpenAI API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_config: Dict[str, Any], server_config: Dict[str, Any] = None):\n",
    "        super().__init__(api_config, server_config)\n",
    "        self.name = \"openai\"\n",
    "        openai.api_key = api_config.get(\"api_key\", \"\")\n",
    "    \n",
    "    def validate_config(self) -> bool:\n",
    "        \"\"\"Check if OpenAI API key is available\"\"\"\n",
    "        valid = bool(self.config.get(\"api_key\", \"\"))\n",
    "        if not valid:\n",
    "            self.logger.error(\"OpenAI API key is missing\")\n",
    "        return valid\n",
    "    \n",
    "    @retry_on_exception(max_attempts=3, delay=2)\n",
    "    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"Generate text using OpenAI API\"\"\"\n",
    "        if not self.validate_config():\n",
    "            raise ValueError(\"OpenAI API key is not configured\")\n",
    "        \n",
    "        opts = options or {}\n",
    "        model = opts.get(\"model\", self.config.get(\"default_model\", \"gpt-4o\"))\n",
    "        temperature = opts.get(\"temperature\", self.config.get(\"temperature\", 0.7))\n",
    "        max_tokens = opts.get(\"max_tokens\", self.config.get(\"max_tokens\", 1000))\n",
    "        \n",
    "        self.logger.info(f\"Generating text with OpenAI model: {model}\")\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating text with OpenAI: {e}\")\n",
    "            raise\n",
    "\n",
    "class AnthropicClient(BaseAPIClient):\n",
    "    \"\"\"Client for Anthropic Claude API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_config: Dict[str, Any], server_config: Dict[str, Any] = None):\n",
    "        super().__init__(api_config, server_config)\n",
    "        self.name = \"anthropic\"\n",
    "        self.api_key = api_config.get(\"api_key\", \"\")\n",
    "    \n",
    "    def validate_config(self) -> bool:\n",
    "        \"\"\"Check if Anthropic API key is available\"\"\"\n",
    "        valid = bool(self.api_key)\n",
    "        if not valid:\n",
    "            self.logger.error(\"Anthropic API key is missing\")\n",
    "        return valid\n",
    "    \n",
    "    @retry_on_exception(max_attempts=3, delay=2)\n",
    "    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"Generate text using Anthropic API\"\"\"\n",
    "        if not self.validate_config():\n",
    "            raise ValueError(\"Anthropic API key is not configured\")\n",
    "            \n",
    "        opts = options or {}\n",
    "        model = opts.get(\"model\", self.config.get(\"default_model\", \"claude-3-opus-20240229\"))\n",
    "        temperature = opts.get(\"temperature\", self.config.get(\"temperature\", 0.7))\n",
    "        max_tokens = opts.get(\"max_tokens\", self.config.get(\"max_tokens\", 1000))\n",
    "        \n",
    "        self.logger.info(f\"Generating text with Anthropic model: {model}\")\n",
    "        try:\n",
    "            headers = {\n",
    "                \"x-api-key\": self.api_key,\n",
    "                \"content-type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"https://api.anthropic.com/v1/messages\",\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=self.server_config.get(\"timeout\", 30)\n",
    "            )\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"content\"][0][\"text\"]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating text with Anthropic: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class APIClientFactory:\n",
    "    \"\"\"Factory class to create the appropriate API client\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_client(api_name: str, config: Dict[str, Any]) -> BaseAPIClient:\n",
    "        \"\"\"Create an API client based on the name\"\"\"\n",
    "        server_config = config.get(\"server\", {})\n",
    "        \n",
    "        if api_name == \"openai\":\n",
    "            return OpenAIClient(config[\"api\"][\"openai\"], server_config)\n",
    "        elif api_name == \"anthropic\" and config[\"api\"][\"anthropic\"].get(\"enabled\", False):\n",
    "            return AnthropicClient(config[\"api\"][\"anthropic\"], server_config)\n",
    "        # Add more clients here as they become available\n",
    "        else:\n",
    "            logger.error(f\"Unsupported API client: {api_name}\")\n",
    "            raise ValueError(f\"Unsupported API client: {api_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Classes\n",
    "\n",
    "class Model:\n",
    "    \"\"\"Model class that handles interaction with the API clients\"\"\"\n",
    "    \n",
    "    def __init__(self, api_client: BaseAPIClient):\n",
    "        self.api_client = api_client\n",
    "        self.logger = logging.getLogger(f\"MCP.Model.{api_client.name}\")\n",
    "    \n",
    "    def generate(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"Generate a response using the API client\"\"\"\n",
    "        self.logger.debug(f\"Generating response for prompt: {prompt[:50]}...\")\n",
    "        try:\n",
    "            return self.api_client.generate_text(prompt, options)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate response: {e}\")\n",
    "            return f\"Error: Failed to generate response. {str(e)}\"\n",
    "\n",
    "\n",
    "class Controller:\n",
    "    \"\"\"Controller class that manages the flow of requests and responses\"\"\"\n",
    "    \n",
    "    def __init__(self, model: Model, config: Dict[str, Any]):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.cache = {} if config[\"server\"].get(\"cache_enabled\", False) else None\n",
    "        self.logger = logging.getLogger(\"MCP.Controller\")\n",
    "    \n",
    "    def process_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a request and return a response\"\"\"\n",
    "        try:\n",
    "            prompt = request_data.get(\"prompt\", \"\")\n",
    "            options = request_data.get(\"options\", {})\n",
    "            \n",
    "            if not prompt:\n",
    "                self.logger.warning(\"Received empty prompt\")\n",
    "                return {\n",
    "                    \"response\": \"Error: Empty prompt\",\n",
    "                    \"error\": True,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "            \n",
    "            self.logger.info(f\"Processing request with prompt: {prompt[:50]}...\")\n",
    "            \n",
    "            # Check cache if enabled\n",
    "            cache_key = f\"{prompt}:{json.dumps(options)}\"\n",
    "            if self.cache is not None and cache_key in self.cache:\n",
    "                cache_entry = self.cache[cache_key]\n",
    "                if time.time() - cache_entry[\"timestamp\"] < self.config[\"server\"].get(\"cache_ttl\", 3600):\n",
    "                    self.logger.info(\"Returning cached response\")\n",
    "                    return {\n",
    "                        \"response\": cache_entry[\"response\"],\n",
    "                        \"cached\": True,\n",
    "                        \"timestamp\": time.time()\n",
    "                    }\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.model.generate(prompt, options)\n",
    "            \n",
    "            # Update cache if enabled\n",
    "            if self.cache is not None:\n",
    "                self.cache[cache_key] = {\n",
    "                    \"response\": response,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"cached\": False,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing request: {e}\")\n",
    "            return {\n",
    "                \"response\": f\"Error: {str(e)}\",\n",
    "                \"error\": True,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Pipeline class that orchestrates the entire process\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.controller = None\n",
    "        self.logger = logging.getLogger(\"MCP.Pipeline\")\n",
    "        self.initialize()\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the pipeline with the configured API\"\"\"\n",
    "        try:\n",
    "            # Check enabled APIs and select the first available one\n",
    "            api_priority = [\"openai\", \"anthropic\", \"gemini\"]\n",
    "            selected_api = None\n",
    "            \n",
    "            for api_name in api_priority:\n",
    "                api_config = self.config[\"api\"].get(api_name, {})\n",
    "                if api_name == \"openai\" or api_config.get(\"enabled\", False):\n",
    "                    if api_config.get(\"api_key\"):\n",
    "                        selected_api = api_name\n",
    "                        break\n",
    "            \n",
    "            if selected_api is None:\n",
    "                raise ValueError(\"No valid API configuration found\")\n",
    "                \n",
    "            self.logger.info(f\"Initializing pipeline with {selected_api} API\")\n",
    "            api_client = APIClientFactory.create_client(selected_api, self.config)\n",
    "            model = Model(api_client)\n",
    "            self.controller = Controller(model, self.config)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize pipeline: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def process(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a request through the pipeline\"\"\"\n",
    "        if self.controller is None:\n",
    "            self.logger.error(\"Pipeline has not been initialized\")\n",
    "            raise ValueError(\"Pipeline has not been initialized\")\n",
    "        \n",
    "        return self.controller.process_request(request_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def load_config_from_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load configuration from a JSON file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading config from {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def merge_config(base_config: Dict[str, Any], override_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Merge override configuration into base configuration\"\"\"\n",
    "    merged = base_config.copy()\n",
    "    \n",
    "    for key, value in override_config.items():\n",
    "        if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):\n",
    "            merged[key] = merge_config(merged[key], value)\n",
    "        else:\n",
    "            merged[key] = value\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GeminiClient(BaseAPIClient):\\n    \"\"\"Client for Google Gemini API\"\"\"\\n    \\n    def __init__(self, api_config: Dict[str, Any], server_config: Dict[str, Any] = None):\\n        super().__init__(api_config, server_config)\\n        self.name = \"gemini\"\\n        self.api_key = api_config.get(\"api_key\", \"\")\\n    \\n    def validate_config(self) -> bool:\\n        \"\"\"Check if Gemini API key is available\"\"\"\\n        valid = bool(self.api_key)\\n        if not valid:\\n            self.logger.error(\"Gemini API key is missing\")\\n        return valid\\n    \\n    @retry_on_exception(max_attempts=3, delay=2)\\n    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:\\n        \"\"\"Generate text using Gemini API\"\"\"\\n        if not self.validate_config():\\n            raise ValueError(\"Gemini API key is not configured\")\\n        \\n        # Implement Gemini API call here\\n        # ...\\n        return \"Gemini response\"\\n\\n# To update the factory, add:\\n# elif api_name == \"gemini\" and config[\"api\"][\"gemini\"].get(\"enabled\", False):\\n#     return GeminiClient(config[\"api\"][\"gemini\"], server_config)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: How to add a new API Client\n",
    "\n",
    "# This is an example implementation of another API client (e.g., for Gemini)\n",
    "'''\n",
    "class GeminiClient(BaseAPIClient):\n",
    "    \"\"\"Client for Google Gemini API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_config: Dict[str, Any], server_config: Dict[str, Any] = None):\n",
    "        super().__init__(api_config, server_config)\n",
    "        self.name = \"gemini\"\n",
    "        self.api_key = api_config.get(\"api_key\", \"\")\n",
    "    \n",
    "    def validate_config(self) -> bool:\n",
    "        \"\"\"Check if Gemini API key is available\"\"\"\n",
    "        valid = bool(self.api_key)\n",
    "        if not valid:\n",
    "            self.logger.error(\"Gemini API key is missing\")\n",
    "        return valid\n",
    "    \n",
    "    @retry_on_exception(max_attempts=3, delay=2)\n",
    "    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"Generate text using Gemini API\"\"\"\n",
    "        if not self.validate_config():\n",
    "            raise ValueError(\"Gemini API key is not configured\")\n",
    "        \n",
    "        # Implement Gemini API call here\n",
    "        # ...\n",
    "        return \"Gemini response\"\n",
    "\n",
    "# To update the factory, add:\n",
    "# elif api_name == \"gemini\" and config[\"api\"][\"gemini\"].get(\"enabled\", False):\n",
    "#     return GeminiClient(config[\"api\"][\"gemini\"], server_config)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 11:58:22,873 - MCP - INFO - Initializing MCP pipeline\n",
      "2025-04-06 11:58:22,874 - MCP.Pipeline - INFO - Initializing pipeline with openai API\n",
      "2025-04-06 11:58:22,874 - MCP - INFO - Processing request 1\n",
      "2025-04-06 11:58:22,875 - MCP.Controller - INFO - Processing request with prompt: Explain what an MCP server is in simple terms...\n",
      "2025-04-06 11:58:22,876 - MCP.base - INFO - Generating text with OpenAI model: gpt-4o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Request 1 ---\n",
      "Prompt: Explain what an MCP server is in simple terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 11:58:26,431 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 11:58:26,435 - MCP - INFO - Processing request 2\n",
      "2025-04-06 11:58:26,436 - MCP.Controller - INFO - Processing request with prompt: Write a haiku about artificial intelligence...\n",
      "2025-04-06 11:58:26,436 - MCP.base - INFO - Generating text with OpenAI model: gpt-4o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response ---\n",
      "Cached: False\n",
      "Error: False\n",
      "Response:\n",
      "An MCP server typically refers to a \"Minecraft Server\" or \"Multi-Channel Processor\" server, depending on the context. However, in most casual discussions, an MCP server usually means a Minecraft Server.\n",
      "\n",
      "In simple terms, a Minecraft Server is a computer or a program that allows multiple people to connect and play Minecraft together over the internet or a local network. It hosts the game world, manages player connections, and ensures that everyone sees the same game state. Players can join the server to explore, build, and interact in the shared Minecraft environment.\n",
      "\n",
      "If you're referring to a Multi-Channel Processor server, it would be related to a system that manages multiple data streams or channels, often used in telecommunications or broadcasting. However, this is less common in casual discussions.\n",
      "\n",
      "If you have a specific context in mind, let me know, and I can provide a more precise explanation!\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Request 2 ---\n",
      "Prompt: Write a haiku about artificial intelligence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 11:58:27,386 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response ---\n",
      "Cached: False\n",
      "Error: False\n",
      "Response:\n",
      "Circuits hum with thought,  \n",
      "Silicon dreams awaken—  \n",
      "Mind of code and light.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize the pipeline with our config\n",
    "        logger.info(\"Initializing MCP pipeline\")\n",
    "        pipeline = Pipeline(config)\n",
    "        \n",
    "        # Example requests\n",
    "        example_requests = [\n",
    "            {\n",
    "                \"prompt\": \"Explain what an MCP server is in simple terms\",\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.5,\n",
    "                    \"max_tokens\": 200\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": \"Write a haiku about artificial intelligence\",\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.9,\n",
    "                    \"max_tokens\": 50\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process each request and print results\n",
    "        for i, request in enumerate(example_requests):\n",
    "            logger.info(f\"Processing request {i+1}\")\n",
    "            print(f\"\\n--- Request {i+1} ---\")\n",
    "            print(f\"Prompt: {request['prompt']}\")\n",
    "            \n",
    "            response_data = pipeline.process(request)\n",
    "            \n",
    "            print(\"\\n--- Response ---\")\n",
    "            print(f\"Cached: {response_data.get('cached', False)}\")\n",
    "            print(f\"Error: {response_data.get('error', False)}\")\n",
    "            print(f\"Response:\\n{response_data['response']}\")\n",
    "            print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main execution failed: {e}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 11:58:27,401 - MCP - INFO - Initializing MCP pipeline\n",
      "2025-04-06 11:58:27,402 - MCP.Pipeline - INFO - Initializing pipeline with openai API\n",
      "2025-04-06 11:58:27,402 - MCP - INFO - Processing request 1\n",
      "2025-04-06 11:58:27,403 - MCP.Controller - INFO - Processing request with prompt: Explain what an MCP server is in simple terms...\n",
      "2025-04-06 11:58:27,404 - MCP.base - INFO - Generating text with OpenAI model: gpt-4o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Request 1 ---\n",
      "Prompt: Explain what an MCP server is in simple terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 11:58:29,857 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 11:58:29,869 - MCP - INFO - Processing request 2\n",
      "2025-04-06 11:58:29,870 - MCP.Controller - INFO - Processing request with prompt: Write a haiku about artificial intelligence...\n",
      "2025-04-06 11:58:29,870 - MCP.base - INFO - Generating text with OpenAI model: gpt-4o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response ---\n",
      "Cached: False\n",
      "Error: False\n",
      "Response:\n",
      "An MCP server typically refers to a server that runs the \"MCP\" software or protocol. In the context of computing, MCP can stand for different things depending on the specific technology or company. \n",
      "\n",
      "One common reference is to the Master Control Program (MCP), which is the operating system used by Unisys ClearPath mainframe computers. In this context, an MCP server would be a server running the MCP operating system, which is designed to manage and execute tasks on a mainframe computer, providing a stable and secure environment for large-scale computing needs.\n",
      "\n",
      "Another context where MCP might be used is in gaming or other software systems, but the specific meaning can vary. If you have a specific context or application in mind, providing more details would help clarify the exact type of MCP server you're referring to.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Request 2 ---\n",
      "Prompt: Write a haiku about artificial intelligence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 11:58:30,567 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response ---\n",
      "Cached: False\n",
      "Error: False\n",
      "Response:\n",
      "Machines learn and grow,  \n",
      "Whispers of code spark new thoughts—  \n",
      "Metal minds, alive.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
