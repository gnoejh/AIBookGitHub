{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Generative/theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI: Understanding Model Types\n",
    "\n",
    "In machine learning, models can be broadly categorized into two major frameworks: **discriminative models** and **generative models**. These represent fundamentally different approaches to understanding and working with data.\n",
    "\n",
    "## Discriminative vs. Generative Models\n",
    "\n",
    "| Aspect | Discriminative Models | Generative Models |\n",
    "|--------|----------------------|-------------------|\n",
    "| **Core Objective** | Learn the boundary between classes | Learn the underlying data distribution |\n",
    "| **Mathematical Focus** | Model $P(Y\\|X)$ (conditional probability) | Model $P(X,Y)$ (joint probability) or $P(X)$ |\n",
    "| **What They Learn** | Decision boundaries between data categories | How to generate new data samples |\n",
    "| **Primary Use** | Classification and regression | Data generation and density estimation |\n",
    "| **Output** | Labels, categories, or numerical predictions | New data samples resembling training distribution |\n",
    "| **Typical Tasks** | Image classification, sentiment analysis, spam detection | Image generation, text synthesis, anomaly detection |\n",
    "| **Common Examples** | SVMs, Neural Networks, Random Forests, Logistic Regression, Transformers (for classification), Reinforcement Learning (for policy learning) | VAEs, GANs, Diffusion Models, Normalizing Flows, Transformers (for generation), Reinforcement Learning (for world modeling) |\n",
    "| **Training Process** | Often simpler and more direct optimization | Can involve complex adversarial training or likelihood estimation |\n",
    "| **Inference** | Generally faster inference | Often requires iterative processes for generation |\n",
    "| **Evaluation** | Clear metrics (accuracy, precision, recall, F1) | Less standardized metrics (FID, IS, log-likelihood) |\n",
    "| **Sample Efficiency** | Generally more sample efficient | Often requires more training data |\n",
    "| **Interpretability** | Can focus on decision boundaries | Can visualize the learned data manifold |\n",
    "| **Applications** | Decision making, categorization, prediction | Content creation, data augmentation, compression |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Models\n",
    "\n",
    "Discriminative models focus on learning the boundaries between different classes or predicting specific outputs given inputs. They answer the question: \"Given these features, what is the most likely label or value?\"\n",
    "\n",
    "- **Strengths**:\n",
    "  - Often achieve superior performance on classification tasks\n",
    "  - More directly optimized for prediction accuracy\n",
    "  - Generally simpler to train and deploy\n",
    "  - Require fewer assumptions about the data\n",
    "\n",
    "- **Limitations**:\n",
    "  - Cannot generate new data samples\n",
    "  - Don't explicitly model the data distribution\n",
    "  - May struggle with imbalanced datasets\n",
    "  - Limited ability to handle missing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Models\n",
    "\n",
    "Generative models learn to capture the entire data distribution, allowing them to generate new samples that resemble the training data. They answer the question: \"How was this data generated?\" or \"What does typical data from this distribution look like?\"\n",
    "\n",
    "- **Strengths**:\n",
    "  - Can generate novel, realistic data samples\n",
    "  - Model the full data distribution\n",
    "  - Can work with unlabeled data (unsupervised learning)\n",
    "  - Useful for data augmentation and synthesis\n",
    "\n",
    "- **Limitations**:\n",
    "  - Often more complex to train successfully\n",
    "  - May require more computational resources\n",
    "  - Evaluation can be subjective or challenging\n",
    "  - Sometimes less accurate for pure classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of Generative Models\n",
    "![Evolution of Generative Models](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png)\n",
    "The field of generative models has seen remarkable advancement over recent years:\n",
    "\n",
    "1. **Early Probabilistic Models (pre-2010)**: \n",
    "   - Hidden Markov Models and Naive Bayes\n",
    "   - Simple yet effective for structured data\n",
    "   - Limited expressiveness for complex distributions\n",
    "\n",
    "2. **Deep Belief Networks & Restricted Boltzmann Machines (2006-2014)**:\n",
    "   - First successful deep generative models\n",
    "   - Used layer-by-layer pretraining\n",
    "   - Challenged by difficult training dynamics and evaluation\n",
    "\n",
    "3. **Variational Autoencoders (VAEs) (2013)**:\n",
    "   - Introduced trainable deep generative models with clear probabilistic foundations\n",
    "   - Combined variational inference with neural networks\n",
    "   - Enabled efficient inference and generation\n",
    "   - Often produce blurry samples due to their objective function\n",
    "\n",
    "4. **Generative Adversarial Networks (GANs) (2014)**:\n",
    "   - Revolutionary approach using adversarial training between generator and discriminator\n",
    "   - Produced remarkably sharp samples\n",
    "   - Suffered from training instability, mode collapse, and evaluation challenges\n",
    "   - Led to numerous architectural variants (DCGAN, StyleGAN, BigGAN)\n",
    "\n",
    "5. **Autoregressive Models (2016-present)**:\n",
    "   - Generate data sequentially, one element at a time\n",
    "   - Examples include PixelCNN, PixelRNN, and Transformer-based models like GPT\n",
    "   - Exact likelihood evaluation but slow sequential generation\n",
    "   - Dominated NLP applications through transformer architectures\n",
    "\n",
    "6. **Flow-based Models (2016-present)**:\n",
    "   - Based on invertible transformations with exact likelihood computation\n",
    "   - Models include RealNVP, Glow, and Flow++\n",
    "   - Allow both efficient sampling and density estimation\n",
    "   - Often trade simplicity of flows for expressiveness\n",
    "\n",
    "7. **Diffusion Models (2020-present)**:\n",
    "   - Current state-of-the-art for many generation tasks\n",
    "   - Based on gradually removing noise from a signal\n",
    "   - Stable training with high sample quality\n",
    "   - Used in commercial systems like DALL-E 2, Stable Diffusion, and Midjourney\n",
    "\n",
    "8. **Consistency Models (2023-present)**:\n",
    "   - Further refinement of diffusion approaches focusing on sampling efficiency\n",
    "   - Enable one-step or few-step generation\n",
    "   - Maintain quality while dramatically reducing computation\n",
    "\n",
    "Each generation of models has addressed limitations of previous approaches while opening new possibilities for generative AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Major Generative Architectures\n",
    "\n",
    "Understanding the strengths and limitations of different generative architectures helps contextualize why diffusion models have become dominant in certain domains.\n",
    "\n",
    "<!-- The image at https://i.imgur.com/vqjYRuq.png appears to be missing or inaccessible. -->\n",
    "<!-- Please replace with an alternative comparison image of generative models -->\n",
    "![Comparison of Generative Models](https://miro.medium.com/v2/resize:fit:1400/1*9sV5tj6XJ8M23FAa_jzLqQ.png)\n",
    "\n",
    "| Architecture | Core Mechanism | Strengths | Limitations | Key Applications |\n",
    "|--------------|----------------|-----------|-------------|------------------|\n",
    "| **VAEs** | Encode data to latent distribution, then decode | - Stable training<br>- Explicit latent space<br>- Fast sampling | - Blurry samples<br>- Limited expressiveness | - Representation learning<br>- Feature extraction<br>- Anomaly detection |\n",
    "| **GANs** | Adversarial game between generator and discriminator | - Sharp, realistic samples<br>- Fast sampling<br>- Efficient architecture | - Training instability<br>- Mode collapse<br>- No likelihood estimation | - Photorealistic imagery<br>- Style transfer<br>- Data augmentation |\n",
    "| **Autoregressive** | Sequential generation conditioned on previous elements | - Tractable likelihood<br>- Strong at capturing dependencies<br>- Natural for sequential data | - Slow sampling<br>- No explicit latent space | - Text generation<br>- Speech synthesis<br>- Music generation |\n",
    "| **Flow Models** | Series of invertible transformations | - Exact likelihood computation<br>- Fast sampling<br>- Invertible mappings | - Architecture constraints<br>- Complex designs for high performance | - Density estimation<br>- Anomaly detection<br>- Variational inference |\n",
    "| **Diffusion Models** | Progressive denoising from random noise | - High sample quality<br>- Training stability<br>- Flexible conditioning | - Slow sampling process<br>- Compute intensive | - Image generation<br>- Audio synthesis<br>- 3D content creation |\n",
    "\n",
    "### Latent Space Visualization\n",
    "\n",
    "Different generative architectures organize their latent spaces in distinct ways, which impacts how they represent and manipulate data:\n",
    "\n",
    "![Latent Space Comparison](https://i.imgur.com/8KvUfJL.png)\n",
    "\n",
    "- **VAEs**: Enforce a structured (typically Gaussian) latent space that enables smooth interpolation but may lose information\n",
    "- **GANs**: Implicitly learn a latent space mapping to the data manifold, often allowing meaningful semantic operations\n",
    "- **Diffusion Models**: Operate in the data space directly, with intermediate steps representing progressively noisier versions of data\n",
    "\n",
    "This difference in latent representation explains many of the practical differences in generation quality, editability, and sampling behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models\n",
    "\n",
    "Diffusion models are a powerful class of generative models that have recently achieved state-of-the-art results in generating high-quality images, audio, and other data types. These models work by gradually adding noise to data and then learning to reverse this process, offering a novel approach to generation that differs fundamentally from GANs and VAEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Overview\n",
    "\n",
    "Diffusion models operate based on two primary processes:\n",
    "\n",
    "1. **Forward Process (Diffusion)**: Gradually adds Gaussian noise to the data through a fixed sequence of steps until it becomes pure noise following a standard normal distribution. This process systematically destroys structure in the data.\n",
    "\n",
    "2. **Reverse Process (Denoising)**: Learns to gradually remove noise to recover the original data distribution, starting from random noise. This is the generative process that creates new samples.\n",
    "\n",
    "![Diffusion Process](https://theaisummer.com/static/d007d60f773b61f4585cbec3869490d5/a878e/score-sde.png)\n",
    "\n",
    "The key insight behind diffusion models is that while destroying information (adding noise) is straightforward and can be defined analytically, the model learns the challenging task of restoring information from noise. This separation creates a more stable training process compared to adversarial methods like GANs.\n",
    "\n",
    "### Intuitive Understanding\n",
    "\n",
    "Think of the forward process as slowly dissolving an image in acid (noise) over time until nothing recognizable remains. The reverse process then learns how to reconstruct the image by understanding how different elements of the image dissolve at different rates and in different patterns.\n",
    "\n",
    "![Diffusion Step by Step](https://d33wubrfki0l68.cloudfront.net/54c4449f1cad42913a0718bb16182452df2f3874/3c816/assets/posts/annotated-diffusion/denoising_animation.gif)\n",
    "\n",
    "This animation shows how a diffusion model progressively denoises an image, starting from pure noise and gradually revealing structure until producing a clean sample. The process moves from high entropy (disorder) to low entropy (ordered structure) by iteratively applying learned denoising operations.\n",
    "\n",
    "This approach allows the model to learn the complex structure and details of the data distribution in a progressive manner, focusing on different levels of detail at different denoising steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "### Forward Process\n",
    "\n",
    "The forward diffusion process is defined as a Markov chain that gradually adds Gaussian noise to the data:\n",
    "\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)$$\n",
    "\n",
    "where $\\beta_t$ is a variance schedule that controls the noise level at each step $t$. This schedule is typically designed to start small and increase over time, allowing for gradual noise addition.\n",
    "\n",
    "An important mathematical property allows us to sample $x_t$ directly from $x_0$ without going through all intermediate steps (called the reparameterization trick):\n",
    "\n",
    "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$$\n",
    "\n",
    "where $\\bar{\\alpha}_t = \\prod_{i=1}^{t} (1 - \\beta_i)$.\n",
    "\n",
    "This means we can directly compute any noised version $x_t$ given the original data $x_0$, which significantly simplifies the training process.\n",
    "\n",
    "### Reverse Process\n",
    "\n",
    "The goal is to learn the reverse process, which gradually denoises the data:\n",
    "\n",
    "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
    "\n",
    "The neural network is trained to predict the parameters of this distribution. In practice, the model is often simplified to predict either:\n",
    "\n",
    "1. The noise component added at step $t$\n",
    "2. The clean data $x_0$ directly\n",
    "3. The mean $\\mu_\\theta$ of the reverse process distribution\n",
    "\n",
    "For most implementations, predicting the added noise has proven most effective and stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objective\n",
    "\n",
    "The training objective for diffusion models is derived from variational inference principles. While the full objective involves complex terms related to the ELBO (Evidence Lower Bound), it can be simplified to a more tractable form.\n",
    "\n",
    "### Simplified Objective\n",
    "\n",
    "The most common simplified objective is the noise prediction objective:\n",
    "\n",
    "$$L_{simple} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ ||\\epsilon - \\epsilon_\\theta(x_t, t)||^2 \\right]$$\n",
    "\n",
    "where:\n",
    "- $\\epsilon$ is the actual noise added during the forward process\n",
    "- $\\epsilon_\\theta$ is the model's prediction of that noise\n",
    "- $t$ is randomly sampled from $\\{1, 2, ..., T\\}$ during training\n",
    "- $x_0$ is a sample from the training data\n",
    "\n",
    "This objective asks the model to predict what noise was added to create $x_t$ from $x_0$. When the model can accurately predict this noise, it can effectively reverse the noise addition process during sampling.\n",
    "\n",
    "### Weighting Strategies\n",
    "\n",
    "In practice, various weighting strategies have been proposed to improve training stability and sample quality:\n",
    "\n",
    "- **Original DDPM weighting**: $L = ||\\epsilon - \\epsilon_\\theta(x_t, t)||^2$\n",
    "- **Improved DDPM weighting**: $L = ||w(t) \\cdot (\\epsilon - \\epsilon_\\theta(x_t, t))||^2$\n",
    "\n",
    "where $w(t)$ is a weighting function that emphasizes certain timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The backbone of most diffusion models is a U-Net architecture with several important modifications:\n",
    "\n",
    "1. **Time Embedding**: Since the model needs to know which denoising step it's performing, the timestep $t$ is embedded and injected into the network.\n",
    "\n",
    "2. **Attention Mechanisms**: Self-attention layers are typically incorporated to capture global context, which is crucial for coherent generation.\n",
    "\n",
    "3. **Residual Connections**: Extensive use of residual connections helps information flow through the deep network.\n",
    "\n",
    "4. **Conditioning Mechanisms**: For conditional generation (e.g., based on class labels or text), various conditioning techniques are employed, often using cross-attention mechanisms.\n",
    "\n",
    "The most successful diffusion models typically employ large U-Net architectures with hundreds of millions of parameters, though smaller models can still produce impressive results for specific domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Architecture Diagram\n",
    "\n",
    "The diagram below illustrates the key components of a typical diffusion model architecture:\n",
    "\n",
    "![DDPM U-Net Architecture](https://theaisummer.com/static/ecb7a31540b18a8cbd18eedb446b468e/40ffe/diffusion-models.png)\n",
    "\n",
    "Key architectural elements include:\n",
    "\n",
    "1. **Encoder Path**: Progressively reduces spatial dimensions while increasing feature channels\n",
    "   - Uses strided convolutions or downsampling blocks\n",
    "   - Captures hierarchical features at multiple scales\n",
    "\n",
    "2. **Timestep Embedding**: Encodes the denoising timestep\n",
    "   - Usually implemented with sinusoidal position embeddings\n",
    "   - Injected at multiple points in the network via adaptive normalization or addition\n",
    "\n",
    "3. **Attention Blocks**: Captures global context\n",
    "   - Self-attention mechanisms help model long-range dependencies\n",
    "   - Particularly important for coherent image generation\n",
    "\n",
    "4. **Decoder Path**: Progressively increases spatial dimensions\n",
    "   - Uses transposed convolutions or upsampling blocks\n",
    "   - Skip connections from encoder preserve spatial information\n",
    "\n",
    "5. **Conditioning Inputs**: Optional pathways for controlling generation\n",
    "   - Can include class embeddings, text features, or other conditioning signals\n",
    "   - Often incorporated through cross-attention or adaptive normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Process\n",
    "\n",
    "Once trained, generating new samples with a diffusion model involves the following steps:\n",
    "\n",
    "1. **Start with pure noise**: Sample $x_T \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "2. **Iterative denoising**: For $t$ from $T$ down to 1:\n",
    "   - Predict noise or clean data component using the model\n",
    "   - Compute the parameters of $p_\\theta(x_{t-1}|x_t)$\n",
    "   - Sample $x_{t-1}$ from this distribution\n",
    "\n",
    "3. **Return final sample**: $x_0$ is the generated sample\n",
    "\n",
    "This sampling process moves gradually from pure noise to a coherent sample from the learned data distribution. The step size and number of steps significantly impact both quality and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Variants of Diffusion Models\n",
    "\n",
    "### DDPM (Denoising Diffusion Probabilistic Models)\n",
    "- The original formulation introduced by Ho et al. (2020)\n",
    "- Uses a fixed variance schedule and Markovian sampling process\n",
    "- Excellent quality but requires many sampling steps (typically 1000)\n",
    "- Forms the foundation for most subsequent diffusion models\n",
    "\n",
    "### DDIM (Denoising Diffusion Implicit Models)\n",
    "- Extends DDPM with a non-Markovian process that allows for deterministic sampling\n",
    "- Enables faster sampling with fewer steps (10-50 steps often sufficient)\n",
    "- Maintains high sample quality even with accelerated sampling\n",
    "- Introduces the concept of \"skip connections\" between timesteps\n",
    "\n",
    "### Latent Diffusion Models (LDMs)\n",
    "- Apply diffusion in a compressed latent space instead of pixel space\n",
    "- Use an autoencoder to reduce the dimensionality of the data before diffusion\n",
    "- Significantly more efficient for high-resolution images (10-100x speedup)\n",
    "- Used in Stable Diffusion for text-to-image generation\n",
    "- Enables practical applications on consumer hardware\n",
    "\n",
    "### Score-Based Generative Models\n",
    "- Alternative mathematical formulation using score matching principles\n",
    "- Models the gradient of the log probability density (score function)\n",
    "- Shown to be equivalent to diffusion models under certain conditions\n",
    "- Often uses stochastic differential equations (SDEs) for sampling\n",
    "- Provides a continuous-time perspective on the diffusion process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning Techniques\n",
    "\n",
    "Diffusion models can be conditioned on various inputs to control generation. The primary conditioning methods include:\n",
    "\n",
    "### Class Conditioning\n",
    "- Used to generate samples from specific classes or categories\n",
    "- Class labels are typically embedded and combined with timestep embeddings\n",
    "- Can be implemented via addition, concatenation, or conditional batch normalization\n",
    "- Enables controlled generation of specific categories\n",
    "\n",
    "### Text Conditioning\n",
    "- Used in text-to-image models like Stable Diffusion, DALL-E 2, and Midjourney\n",
    "- Leverages pre-trained text encoders (like CLIP or T5) to extract text embeddings\n",
    "- Typically injects text information via cross-attention mechanisms\n",
    "- Allows for rich, detailed control over generation process\n",
    "- Can be combined with other conditioning techniques\n",
    "\n",
    "### Image Conditioning\n",
    "- Used for image-to-image translation, inpainting, and super-resolution\n",
    "- Input image can be incorporated through concatenation or more sophisticated fusion\n",
    "- Enables applications like style transfer, colorization, and image editing\n",
    "- Often combined with masking for targeted image manipulation\n",
    "\n",
    "### Layout Conditioning\n",
    "- Conditions generation on spatial layouts, bounding boxes, or segmentation maps\n",
    "- Useful for controlled scene generation with specific object arrangements\n",
    "- Often combines spatial information with semantic labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Diffusion Models\n",
    "\n",
    "### Image Generation and Manipulation\n",
    "- Text-to-image synthesis (Stable Diffusion, DALL-E 2, Midjourney)\n",
    "- High-resolution image creation (up to gigapixel images)\n",
    "- Image editing, inpainting, and outpainting\n",
    "- Style transfer and domain adaptation\n",
    "- Synthetic data generation for training other models\n",
    "\n",
    "### Audio Generation\n",
    "- Speech synthesis and voice cloning (AudioLM, VALL-E)\n",
    "- Music generation and instrument synthesis\n",
    "- Sound effects creation and audio enhancement\n",
    "- Text-to-speech with emotional control\n",
    "- Audio inpainting and restoration\n",
    "\n",
    "### Video Generation\n",
    "- Text-to-video synthesis (Imagen Video, Make-A-Video, Gen-2)\n",
    "- Video prediction and future frame generation\n",
    "- Video editing and manipulation\n",
    "- Motion transfer and video style transfer\n",
    "- Slow-motion generation and frame interpolation\n",
    "\n",
    "### 3D Content Creation\n",
    "- Text-to-3D model generation (DreamFusion, Magic3D)\n",
    "- 3D asset creation for games and virtual environments\n",
    "- Neural radiance field (NeRF) generation\n",
    "- 3D completion from partial observations\n",
    "- Texture synthesis for 3D models\n",
    "\n",
    "### Scientific Applications\n",
    "- Protein structure generation and drug discovery\n",
    "- Molecule design and optimization\n",
    "- Material property prediction and design\n",
    "- Weather forecasting and climate modeling\n",
    "- Medical image synthesis and enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Challenges\n",
    "\n",
    "### Advantages\n",
    "- **High Quality**: Produces some of the highest quality generative outputs across domains\n",
    "- **Training Stability**: More stable training compared to GANs, without mode collapse issues\n",
    "- **Sample Diversity**: Naturally generates diverse outputs without additional mechanisms\n",
    "- **Flexible Conditioning**: Easily adaptable to various conditioning inputs\n",
    "- **Theoretical Foundation**: Well-understood probabilistic foundation with clear optimization objectives\n",
    "- **Controllability**: Allows for fine-grained control over the generation process\n",
    "- **Adaptability**: Can be applied to multiple data modalities with minimal architecture changes\n",
    "\n",
    "### Challenges\n",
    "- **Computational Efficiency**: Sampling can be slow due to the sequential nature of the process\n",
    "- **Resource Requirements**: Training high-quality models requires significant computational resources\n",
    "- **Hyperparameter Sensitivity**: Performance can be affected by choice of noise schedule and model architecture\n",
    "- **Evaluation Metrics**: Difficult to quantitatively evaluate generative performance\n",
    "- **Mode Coverage vs. Fidelity**: Trade-off between covering all modes and sample quality\n",
    "- **Long-Range Coherence**: Can struggle with global coherence in very large outputs\n",
    "- **Ethical Considerations**: Potential for generating misleading or harmful content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent Advances and Future Directions\n",
    "\n",
    "### Sampling Efficiency Improvements\n",
    "- **Consistency Models**: Single-step generation with distillation from diffusion models\n",
    "- **Diffusion Distillation**: Training smaller, faster models that mimic diffusion outputs\n",
    "- **Flow Matching**: Alternative formulation that can enable faster sampling\n",
    "- **Progressive Distillation**: Gradually reducing the number of required sampling steps\n",
    "\n",
    "### Architectural Innovations\n",
    "- **Rectified Flow**: Combining flow-based models with diffusion principles\n",
    "- **DiT (Diffusion Transformers)**: Replacing U-Nets with transformer architectures\n",
    "- **Mamba-based Diffusion**: Using state space models instead of attention mechanisms\n",
    "- **Mixture of Experts**: Specialized sub-networks for different parts of the denoising process\n",
    "\n",
    "### Multi-Modal Applications\n",
    "- **Video Diffusion Models**: Extending image diffusion to the temporal dimension\n",
    "- **Multi-Modal Conditioning**: Combining text, image, audio, and video inputs\n",
    "- **3D-Aware Generation**: Models that understand 3D structure from 2D training\n",
    "- **Cross-Modal Generation**: Converting between different modalities (e.g., text-to-audio)\n",
    "\n",
    "### Personalization Techniques\n",
    "- **DreamBooth**: Few-shot personalization of text-to-image models\n",
    "- **LoRA/Custom Diffusion**: Parameter-efficient fine-tuning methods\n",
    "- **Textual Inversion**: Learning new concepts from just a few examples\n",
    "- **Concept Libraries**: Reusable personalized concepts across generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "\n",
    "### Foundation Papers\n",
    "- Ho, J., et al. (2020). [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239). NeurIPS.\n",
    "- Song, J., et al. (2020). [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502). ICLR.\n",
    "- Rombach, R., et al. (2022). [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752). CVPR.\n",
    "- Dhariwal, P., & Nichol, A. (2021). [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233). NeurIPS.\n",
    "- Song, Y., & Ermon, S. (2019). [Generative Modeling by Estimating Gradients of the Data Distribution](https://arxiv.org/abs/1907.05600). NeurIPS.\n",
    "\n",
    "### Recent Advances\n",
    "- Karras, T., et al. (2022). [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364).\n",
    "- Song, Y., et al. (2023). [Consistency Models: Learning SDEs from Diffusion Models](https://arxiv.org/abs/2303.01469).\n",
    "- Peebles, W., & Xie, S. (2023). [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748).\n",
    "- Ruiz, N., et al. (2023). [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242).\n",
    "\n",
    "### Books and Tutorials\n",
    "- Luo, C. (2022). [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970).\n",
    "- Weng, L. (2021). [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n",
    "\n",
    "### Implementations and Resources\n",
    "- [HuggingFace Diffusers Library](https://github.com/huggingface/diffusers)\n",
    "- [Stable Diffusion](https://github.com/CompVis/stable-diffusion)\n",
    "- [Annotated Diffusion Model Implementation](https://huggingface.co/blog/annotated-diffusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
