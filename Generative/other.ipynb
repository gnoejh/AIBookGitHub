{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c07ad19",
   "metadata": {},
   "source": [
    "# Multimodal Generative AI with Hugging Face\n",
    "\n",
    "This notebook explores the various modalities of generative AI beyond just text-to-image generation, using Hugging Face's transformers and other libraries with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f68fdd",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's install the necessary libraries for working with different modalities using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "691a7105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: diffusers in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: librosa in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: soundfile in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (4.45.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: tensorflow 2.19.0\n",
      "Uninstalling tensorflow-2.19.0:\n",
      "  Successfully uninstalled tensorflow-2.19.0\n",
      "Found existing installation: keras 3.9.2\n",
      "Uninstalling keras-3.9.2:\n",
      "  Successfully uninstalled keras-3.9.2\n",
      "Found existing installation: tf_keras 2.19.0\n",
      "Uninstalling tf_keras-2.19.0:\n",
      "  Successfully uninstalled tf_keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: tensorflow 2.19.0\n",
      "Uninstalling tensorflow-2.19.0:\n",
      "  Successfully uninstalled tensorflow-2.19.0\n",
      "Found existing installation: keras 3.9.2\n",
      "Uninstalling keras-3.9.2:\n",
      "  Successfully uninstalled keras-3.9.2\n",
      "Found existing installation: tf_keras 2.19.0\n",
      "Uninstalling tf_keras-2.19.0:\n",
      "  Successfully uninstalled tf_keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages - PyTorch-only version (no TensorFlow/Keras dependencies)\n",
    "%pip install torch torchvision torchaudio \"transformers[torch]\" datasets accelerate diffusers librosa scipy soundfile matplotlib tqdm sentence-transformers --no-dependencies\n",
    "%pip uninstall -y tensorflow keras tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e098109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to disable TensorFlow warnings and force PyTorch-only usage\n",
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"  # Disable TensorFlow in transformers library\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"  # Explicitly tell transformers not to import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "049b4804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA GeForce RTX 4060\n",
      "CUDA version: 12.4\n",
      "PyTorch CUDA enabled: True\n",
      "Current device: 0\n",
      "Memory allocated: 496.22 MB\n",
      "Memory reserved: 1212.00 MB\n",
      "\n",
      "PyTorch version: 2.5.1\n",
      "Using device: cuda\n",
      "Enabling TF32 for faster computation on Ampere or newer GPUs\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability for PyTorch\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU available: {gpu_name}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch CUDA enabled: {torch.cuda.is_available()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"Memory reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"No GPU available, using CPU. Some models will run slowly or require reduced precision.\")\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set default tensor type for better performance\n",
    "if device == \"cuda\":\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    # Enable TF32 for faster computation on Ampere GPUs (RTX 30xx series and newer)\n",
    "    if torch.cuda.get_device_capability(0)[0] >= 8:\n",
    "        print(\"Enabling TF32 for faster computation on Ampere or newer GPUs\")\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85098753",
   "metadata": {},
   "source": [
    "## PyTorch Configuration and Best Practices\n",
    "\n",
    "Let's configure PyTorch for optimal performance and check available hardware acceleration features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ec36b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Configuration:\n",
      "  device: cuda\n",
      "  has_mps: False\n",
      "  supports_half: True\n",
      "  supports_bfloat16: True\n",
      "  cudnn_enabled: True\n",
      "  cudnn_benchmark: True\n",
      "  gpu_name: NVIDIA GeForce RTX 4060\n",
      "  gpu_capability: (8, 9)\n",
      "  gpu_count: 1\n",
      "\n",
      "Using bfloat16 precision for faster computation and better numeric stability\n"
     ]
    }
   ],
   "source": [
    "# Configure PyTorch for optimal performance\n",
    "def configure_pytorch():\n",
    "    config = {\"device\": device}\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        # Check for MPS (Metal Performance Shaders for Mac)\n",
    "        config[\"has_mps\"] = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "        \n",
    "        # Check half-precision support\n",
    "        config[\"supports_half\"] = torch.cuda.is_available()\n",
    "        config[\"supports_bfloat16\"] = torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False\n",
    "        \n",
    "        # Check for CUDA optimization features\n",
    "        config[\"cudnn_enabled\"] = torch.backends.cudnn.enabled\n",
    "        config[\"cudnn_benchmark\"] = torch.backends.cudnn.benchmark\n",
    "        \n",
    "        # Enable cuDNN benchmark mode for potentially faster runtime\n",
    "        # This is good when input sizes don't vary much\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        # Get GPU details\n",
    "        config[\"gpu_name\"] = torch.cuda.get_device_name(0)\n",
    "        config[\"gpu_capability\"] = torch.cuda.get_device_capability(0)\n",
    "        config[\"gpu_count\"] = torch.cuda.device_count()\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Apply and print configuration\n",
    "pytorch_config = configure_pytorch()\n",
    "print(\"PyTorch Configuration:\")\n",
    "for key, value in pytorch_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Set the default dtype based on hardware capabilities for better model loading\n",
    "if device == \"cuda\" and pytorch_config.get(\"supports_half\", False):\n",
    "    if pytorch_config.get(\"supports_bfloat16\", False):\n",
    "        print(\"\\nUsing bfloat16 precision for faster computation and better numeric stability\")\n",
    "        default_dtype = torch.bfloat16\n",
    "    else:\n",
    "        print(\"\\nUsing float16 precision for faster computation\")\n",
    "        default_dtype = torch.float16\n",
    "else:\n",
    "    print(\"\\nUsing standard float32 precision\")\n",
    "    default_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47485ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline\n",
    "import IPython.display as ipd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa71158",
   "metadata": {},
   "source": [
    "## 1. Text Generation\n",
    "\n",
    "Let's start with text generation using a pre-trained language model from Hugging Face with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7da46261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2 model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "Generating text...\n",
      "\n",
      "Generated output:\n",
      "Multimodal AI can help us to understand the world around us.\n",
      "\n",
      "The AI is able to predict the future, and it can also predict the future of the world.\n",
      "\n",
      "The AI can also predict the future of the world.\n",
      "\n",
      "Generated output:\n",
      "Multimodal AI can help us to understand the world around us.\n",
      "\n",
      "The AI is able to predict the future, and it can also predict the future of the world.\n",
      "\n",
      "The AI can also predict the future of the world.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch text generation\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "print(f\"Loading {model_name} model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Multimodal AI can help us\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate with different parameters\n",
    "print(\"Generating text...\")\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.8,  # Control randomness (higher = more random)\n",
    "    top_k=50,         # Sample from top K likely tokens\n",
    "    top_p=0.95        # Nucleus sampling - sample from tokens comprising top p probability mass\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated output:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3d8535c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU acceleration failed: Failed to import transformers.models.gpt2.modeling_tf_gpt2 because of the following error (look up to see its traceback):\n",
      "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "Falling back to CPU\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.gpt2.modeling_tf_gpt2 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1764\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     29\u001b[0m     TFCausalLMOutputWithCrossAttentions,\n\u001b[0;32m     30\u001b[0m     TFSequenceClassifierOutputWithPast,\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 3\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     text_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     use_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:896\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    895\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 896\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\pipelines\\base.py:263\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 263\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1755\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1754\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1767\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1768\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1769\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.gpt2.modeling_tf_gpt2 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1764\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     29\u001b[0m     TFCausalLMOutputWithCrossAttentions,\n\u001b[0;32m     30\u001b[0m     TFSequenceClassifierOutputWithPast,\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU acceleration failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFalling back to CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     text_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     use_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:896\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 896\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    907\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\pipelines\\base.py:263\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 263\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1755\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1754\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1752\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1767\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1768\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1769\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.gpt2.modeling_tf_gpt2 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "# Alternative: Use Hugging Face pipeline (PyTorch-based)\n",
    "try:\n",
    "    text_generator = pipeline('text-generation', model='gpt2', device=0 if device == 'cuda' else -1)\n",
    "    use_cuda = True\n",
    "except RuntimeError as e:\n",
    "    print(f\"GPU acceleration failed: {e}\\nFalling back to CPU\")\n",
    "    text_generator = pipeline('text-generation', model='gpt2', device=-1)\n",
    "    use_cuda = False\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Multimodal AI can help us\"\n",
    "results = text_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"Using {'GPU' if use_cuda else 'CPU'} for inference\")\n",
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f7e3bd",
   "metadata": {},
   "source": [
    "## 2. Image Generation\n",
    "\n",
    "Next, let's explore image generation using Stable Diffusion from Hugging Face's Diffusers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a stable diffusion model with appropriate device and memory optimization\n",
    "def load_image_generator(model_id=\"runwayml/stable-diffusion-v1-5\"):\n",
    "    if device == \"cuda\":\n",
    "        try:\n",
    "            # Try half precision first for better memory efficiency\n",
    "            return StableDiffusionPipeline.from_pretrained(\n",
    "                model_id, torch_dtype=default_dtype).to(device)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Half precision failed: {e}\\nTrying full precision\")\n",
    "            try:\n",
    "                # Try full precision\n",
    "                return StableDiffusionPipeline.from_pretrained(model_id).to(device)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Full precision failed: {e}\\nFalling back to CPU\")\n",
    "                return StableDiffusionPipeline.from_pretrained(model_id).to(\"cpu\")\n",
    "    else:\n",
    "        print(\"Using CPU for image generation (will be slow)\")\n",
    "        return StableDiffusionPipeline.from_pretrained(model_id).to(\"cpu\")\n",
    "\n",
    "# Generate an image with progress bar\n",
    "def generate_image(prompt, num_steps=30):\n",
    "    print(f\"Generating image for prompt: '{prompt}'\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"Warning: Using CPU for image generation may take several minutes!\")\n",
    "    \n",
    "    generator = load_image_generator()\n",
    "    \n",
    "    # Create progress bar callback\n",
    "    progress_bar = tqdm(total=num_steps)\n",
    "    def callback_fn(step, timestep, latents):\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Generate the image\n",
    "    with torch.inference_mode():  # More efficient than no_grad for inference\n",
    "        image = generator(prompt, num_inference_steps=num_steps, callback=callback_fn).images[0]\n",
    "    progress_bar.close()\n",
    "    return image\n",
    "\n",
    "# Generate and display an image\n",
    "try:\n",
    "    prompt = \"A beautiful digital painting of a futuristic city with flying vehicles\"\n",
    "    image = generate_image(prompt)\n",
    "    display(image)\n",
    "except Exception as e:\n",
    "    print(f\"Image generation failed: {e}\")\n",
    "    print(\"Try using a simpler model or reducing the image size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9be64e",
   "metadata": {},
   "source": [
    "## 3. Audio Generation and Speech Synthesis\n",
    "\n",
    "Now let's look at audio generation and text-to-speech synthesis using PyTorch-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Speech using SpeechT5 (PyTorch-based)\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "# Function to synthesize speech with error handling - PyTorch focused\n",
    "def synthesize_speech(text, output_file=\"speech.wav\"):\n",
    "    try:\n",
    "        print(f\"Loading TTS models to {device}...\")\n",
    "        # Load processor, model, and vocoder\n",
    "        processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "        vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n",
    "        \n",
    "        # Load speaker embeddings\n",
    "        print(\"Loading speaker embeddings...\")\n",
    "        embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "        speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Synthesize speech\n",
    "        print(f\"Synthesizing speech for text: '{text}'\")\n",
    "        inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Use inference mode for better memory efficiency\n",
    "        with torch.inference_mode():\n",
    "            speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "        \n",
    "        # Save and play audio\n",
    "        sf.write(output_file, speech.cpu().numpy(), samplerate=16000)\n",
    "        print(f\"Speech saved to {output_file}\")\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"Speech synthesis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate speech\n",
    "text = \"Multimodal AI is revolutionizing how we interact with computers.\"\n",
    "speech_file = synthesize_speech(text)\n",
    "if speech_file:\n",
    "    ipd.display(ipd.Audio(speech_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797745a1",
   "metadata": {},
   "source": [
    "## 4. Text-to-Video Generation\n",
    "\n",
    "Let's explore text-to-video generation using PyTorch-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f71cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-video generation with PyTorch-optimized implementation\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "def generate_video(prompt, num_steps=25, output_file=\"generated_video.mp4\"):\n",
    "    try:\n",
    "        print(f\"Loading text-to-video model on {device}...\")\n",
    "        if device == \"cpu\":\n",
    "            print(\"WARNING: Video generation on CPU may be extremely slow or fail due to memory constraints\")\n",
    "            pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\")\n",
    "        else:  # Using CUDA\n",
    "            try:\n",
    "                # Use optimal precision based on GPU capabilities\n",
    "                pipe = DiffusionPipeline.from_pretrained(\n",
    "                    \"damo-vilab/text-to-video-ms-1.7b\", \n",
    "                    torch_dtype=default_dtype\n",
    "                )\n",
    "                # Enable memory-efficient attention if available\n",
    "                if hasattr(pipe, \"enable_xformers_memory_efficient_attention\"):\n",
    "                    pipe.enable_xformers_memory_efficient_attention()\n",
    "            except Exception as e:\n",
    "                print(f\"Loading with optimized settings failed: {e}\\nTrying with default precision\")\n",
    "                pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\")\n",
    "        \n",
    "        pipe = pipe.to(device)\n",
    "        \n",
    "        # Add progress bar\n",
    "        progress_bar = tqdm(total=num_steps)\n",
    "        def callback_fn(step, timestep, latents):\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Generate video with PyTorch's inference_mode for efficiency\n",
    "        print(f\"Generating video for prompt: '{prompt}'\")\n",
    "        with torch.inference_mode():\n",
    "            video_frames = pipe(prompt, num_inference_steps=num_steps, callback=callback_fn).frames\n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Export and return video path\n",
    "        export_to_video(video_frames, output_file)\n",
    "        print(f\"Video saved to {output_file}\")\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"Video generation failed: {e}\")\n",
    "        print(\"This might be due to insufficient memory. Try reducing resolution or inference steps.\")\n",
    "        return None\n",
    "\n",
    "# Generate a short video (use a smaller number of steps for faster generation)\n",
    "prompt = \"A rocket launching from Earth into space\"\n",
    "video_file = generate_video(prompt, num_steps=15)\n",
    "if video_file:\n",
    "    ipd.display(ipd.Video(video_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298449b6",
   "metadata": {},
   "source": [
    "## 5. Music Generation\n",
    "\n",
    "Now let's generate music using PyTorch-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d13508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music generation using Facebook's MusicGen model (PyTorch-based)\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "def generate_music(prompt, duration_seconds=5, output_file=\"generated_music.wav\"):\n",
    "    try:\n",
    "        print(f\"Loading music generation model on {device}...\")\n",
    "        model_id = \"facebook/musicgen-small\"  # Use small model for reduced memory usage\n",
    "        \n",
    "        # Load model and processor with optimal settings for PyTorch\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        model = MusicgenForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=default_dtype if device == \"cuda\" else torch.float32\n",
    "        ).to(device)\n",
    "        \n",
    "        # Calculate tokens based on duration (approximate conversion)\n",
    "        max_tokens = int(duration_seconds * 50)  # ~50 tokens per second\n",
    "        \n",
    "        # Generate music with progress tracking\n",
    "        print(f\"Generating music for prompt: '{prompt}' ({duration_seconds} seconds)\")\n",
    "        inputs = processor(\n",
    "            text=[prompt],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate with PyTorch inference_mode\n",
    "        with torch.inference_mode():\n",
    "            audio_values = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        \n",
    "        # Convert to numpy and save\n",
    "        sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "        audio_data = audio_values[0, 0].cpu().numpy()\n",
    "        sf.write(output_file, audio_data, sampling_rate)\n",
    "        \n",
    "        print(f\"Music saved to {output_file}\")\n",
    "        return audio_data, sampling_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Music generation failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Generate music\n",
    "music_prompt = \"An electronic dance track with a strong beat\"\n",
    "audio_data, sampling_rate = generate_music(music_prompt)\n",
    "if audio_data is not None:\n",
    "    ipd.display(ipd.Audio(audio_data, rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e1239f",
   "metadata": {},
   "source": [
    "## 6. Cross-Modal Generation: Image-to-Text (Image Captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image captioning using BLIP (PyTorch-based)\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "def caption_image(image_path=None, image_url=None):\n",
    "    try:\n",
    "        # Load model and processor\n",
    "        print(f\"Loading image captioning model on {device}...\")\n",
    "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        model = BlipForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip-image-captioning-base\", \n",
    "            torch_dtype=default_dtype if device == \"cuda\" else torch.float32\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get image from URL or path\n",
    "        if image_url:\n",
    "            print(f\"Downloading image from {image_url}\")\n",
    "            image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "        elif image_path:\n",
    "            print(f\"Loading image from {image_path}\")\n",
    "            image = Image.open(image_path)\n",
    "        else:\n",
    "            # Use a default example image\n",
    "            print(\"Using default example image\")\n",
    "            url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "            image = Image.open(requests.get(url, stream=True).raw)\n",
    "        \n",
    "        # Generate caption with PyTorch's inference_mode\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            out = model.generate(**inputs)\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        \n",
    "        return image, caption\n",
    "    except Exception as e:\n",
    "        print(f\"Image captioning failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Generate caption for the default example image\n",
    "image, caption = caption_image()\n",
    "if image:\n",
    "    display(image)\n",
    "    print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1d134",
   "metadata": {},
   "source": [
    "## 7. Visual Question Answering (VQA)\n",
    "\n",
    "Let's explore how PyTorch-based models can answer questions about images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Question Answering with PyTorch optimization\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "\n",
    "def answer_visual_question(image, question):\n",
    "    try:\n",
    "        print(f\"Loading VQA model on {device}...\")\n",
    "        processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "        model = ViltForQuestionAnswering.from_pretrained(\n",
    "            \"dandelin/vilt-b32-finetuned-vqa\",\n",
    "            torch_dtype=default_dtype if device == \"cuda\" else torch.float32\n",
    "        ).to(device)\n",
    "        \n",
    "        # Prepare inputs\n",
    "        print(f\"Answering: '{question}'\")\n",
    "        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Run inference with memory optimization\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        idx = logits.argmax(-1).item()\n",
    "        return model.config.id2label[idx]\n",
    "    except Exception as e:\n",
    "        print(f\"VQA failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the same image from the previous cell for VQA\n",
    "if 'image' in locals() and image is not None:\n",
    "    questions = [\n",
    "        \"How many cats are in the image?\",\n",
    "        \"What color are the cats?\",\n",
    "        \"Where are the cats sitting?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Visual Question Answering results:\")\n",
    "    for question in questions:\n",
    "        answer = answer_visual_question(image, question)\n",
    "        if answer:\n",
    "            print(f\"Q: {question}\")\n",
    "            print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ade4d",
   "metadata": {},
   "source": [
    "## 8. Practical Applications and Multimodal Combinations\n",
    "\n",
    "Let's explore a more complex example combining multiple modalities with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate an image from text, then generate a caption for verification\n",
    "def generate_and_caption_image(prompt):\n",
    "    print(f\"\\n=== Starting multimodal pipeline for: '{prompt}' ===\")\n",
    "    \n",
    "    # 1. Generate image from text\n",
    "    print(\"\\n[STEP 1/2] Generating image from text prompt...\")\n",
    "    generated_image = None\n",
    "    try:\n",
    "        image_generator = StableDiffusionPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\", \n",
    "            torch_dtype=default_dtype if device == \"cuda\" else torch.float32\n",
    "        ).to(device)\n",
    "        with torch.inference_mode():\n",
    "            generated_image = image_generator(prompt).images[0]\n",
    "        print(\" Image generated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\" Image generation failed: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # 2. Generate caption from the image\n",
    "    print(\"\\n[STEP 2/2] Generating caption for the image...\")\n",
    "    try:\n",
    "        caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        caption_model = BlipForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip-image-captioning-base\",\n",
    "            torch_dtype=default_dtype if device == \"cuda\" else torch.float32\n",
    "        ).to(device)\n",
    "        inputs = caption_processor(generated_image, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            out = caption_model.generate(**inputs)\n",
    "        generated_caption = caption_processor.decode(out[0], skip_special_tokens=True)\n",
    "        print(\" Caption generated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\" Caption generation failed: {e}\")\n",
    "        return generated_image, None, None\n",
    "    \n",
    "    # 3. Calculate similarity between original prompt and generated caption\n",
    "    similarity_score = None\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "        print(\"\\n[BONUS] Calculating semantic similarity between prompt and caption...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2').to(device if device == \"cuda\" else \"cpu\")\n",
    "        with torch.inference_mode():\n",
    "            embedding1 = model.encode(prompt, convert_to_tensor=True)\n",
    "            embedding2 = model.encode(generated_caption, convert_to_tensor=True)\n",
    "        similarity_score = float(util.pytorch_cos_sim(embedding1, embedding2)[0][0])\n",
    "        print(f\" Similarity score calculated: {similarity_score:.4f} (0-1 scale)\")\n",
    "    except Exception as e:\n",
    "        print(f\" Similarity calculation failed: {e}\")\n",
    "    \n",
    "    print(\"\\n=== Multimodal pipeline complete! ===\")\n",
    "    return generated_image, generated_caption, similarity_score\n",
    "\n",
    "# Run the multimodal pipeline\n",
    "original_prompt = \"A surreal painting of a floating island with waterfalls\"\n",
    "generated_image, generated_caption, similarity = generate_and_caption_image(original_prompt)\n",
    "\n",
    "if generated_image is not None:\n",
    "    # Display results\n",
    "    print(f\"\\nOriginal prompt: {original_prompt}\")\n",
    "    display(generated_image)\n",
    "    print(f\"Generated caption: {generated_caption}\")\n",
    "    if similarity:\n",
    "        print(f\"Semantic similarity: {similarity:.4f}\")\n",
    "        # Interpret similarity\n",
    "        if similarity > 0.7:\n",
    "            print(\" High similarity: The image closely matches the original prompt\")\n",
    "        elif similarity > 0.5:\n",
    "            print(\" Moderate similarity: The image somewhat matches the original prompt\")\n",
    "        else:\n",
    "            print(\" Low similarity: The image may not match the original prompt well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e1a29",
   "metadata": {},
   "source": [
    "## 9. Multimodal Chatbots\n",
    "\n",
    "This section demonstrates how to create a simple multimodal chatbot using PyTorch-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113db209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch-based multimodal chatbot\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "def create_multimodal_chatbot(model_id=\"llava-hf/llava-1.5-7b-hf\"):\n",
    "    try:\n",
    "        print(f\"Loading multimodal chatbot model on {device}...\")\n",
    "        print(\"This may take a while as the model is large.\")\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            # Use optimal dtype and quantization for memory efficiency\n",
    "            dtype = default_dtype\n",
    "            processor = AutoProcessor.from_pretrained(model_id)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=\"auto\",  # Automatically handle device placement\n",
    "                load_in_4bit=True,  # Enable 4-bit quantization for memory efficiency\n",
    "            )\n",
    "        else:\n",
    "            print(\"Warning: Running without GPU will be very slow and may fail\")\n",
    "            processor = AutoProcessor.from_pretrained(model_id)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "            \n",
    "        print(\"Multimodal chatbot loaded successfully!\")\n",
    "        return processor, model\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load multimodal chatbot: {e}\")\n",
    "        print(\"Consider using a smaller model or enabling GPU acceleration.\")\n",
    "        return None, None\n",
    "\n",
    "def chat_with_image(processor, model, image_url, question):\n",
    "    if processor is None or model is None:\n",
    "        print(\"Chatbot not loaded successfully.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # Download image from URL\n",
    "        image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "        display(image)\n",
    "        \n",
    "        # Process inputs\n",
    "        prompt = f\"<image>\\nUser: {question}\\nAssistant:\"\n",
    "        inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        print(f\"User: {question}\")\n",
    "        print(\"Assistant: \", end=\"\")\n",
    "        \n",
    "        # Generate with PyTorch's inference_mode\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "            )\n",
    "        \n",
    "        # Process output\n",
    "        response = processor.decode(output[0], skip_special_tokens=True)\n",
    "        response = response.split(\"Assistant:\")[-1].strip()\n",
    "        \n",
    "        print(response)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in multimodal chat: {e}\")\n",
    "        return None\n",
    "\n",
    "# Note: Uncomment this section if you have sufficient GPU memory\n",
    "# print(\"Loading multimodal chatbot (this may take a minute)...\")\n",
    "# processor, model = create_multimodal_chatbot()\n",
    "# if processor and model:\n",
    "#     # Example usage with image URL and question\n",
    "#     image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "#     question = \"What do you see in this image and how many cats are there?\"\n",
    "#     chat_with_image(processor, model, image_url, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18404496",
   "metadata": {},
   "source": [
    "## 10. PyTorch Model Optimization Techniques\n",
    "\n",
    "This section demonstrates various PyTorch optimization techniques for improving inference speed and reducing memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8626bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch optimization techniques for inference\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def demonstrate_pytorch_optimizations():\n",
    "    print(\"PyTorch Model Optimization Techniques\")\n",
    "    print(\"====================================\\n\")\n",
    "    \n",
    "    if device != \"cuda\":\n",
    "        print(\"These optimizations are most effective with GPU acceleration.\")\n",
    "    \n",
    "    # 1. Model Quantization\n",
    "    print(\"1. Model Quantization\")\n",
    "    print(\"--------------------\")\n",
    "    print(\"Loading a small model to demonstrate quantization...\")\n",
    "    try:\n",
    "        # Load a small model\n",
    "        model_id = \"gpt2\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        # Standard FP32 model (baseline)\n",
    "        model_fp32 = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        \n",
    "        # Dynamic Quantization (CPU only)\n",
    "        model_int8 = torch.quantization.quantize_dynamic(\n",
    "            model_fp32,  # the original model\n",
    "            {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "            dtype=torch.qint8  # the target dtype for quantized weights\n",
    "        )\n",
    "        \n",
    "        # Compare model sizes\n",
    "        def get_model_size(model):\n",
    "            param_size = 0\n",
    "            for param in model.parameters():\n",
    "                param_size += param.nelement() * param.element_size()\n",
    "            buffer_size = 0\n",
    "            for buffer in model.buffers():\n",
    "                buffer_size += buffer.nelement() * buffer.element_size()\n",
    "            return (param_size + buffer_size) / 1024 / 1024  # Size in MB\n",
    "        \n",
    "        print(f\"FP32 model size: {get_model_size(model_fp32):.2f} MB\")\n",
    "        print(f\"INT8 model size: {get_model_size(model_int8):.2f} MB\")\n",
    "        print(f\"Memory savings: {(1 - get_model_size(model_int8) / get_model_size(model_fp32)) * 100:.1f}%\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Quantization demonstration failed: {e}\\n\")\n",
    "    \n",
    "    # 2. Inference Mode vs No Grad\n",
    "    print(\"2. Inference Mode vs No Grad\")\n",
    "    print(\"---------------------------\")\n",
    "    try:\n",
    "        # Create a sample model and input\n",
    "        sample_model = torch.nn.Linear(1000, 1000).to(device)\n",
    "        sample_input = torch.randn(32, 1000).to(device)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = sample_model(sample_input)\n",
    "        \n",
    "        # Time with torch.no_grad\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):\n",
    "            with torch.no_grad():\n",
    "                _ = sample_model(sample_input)\n",
    "        no_grad_time = time.time() - start_time\n",
    "        \n",
    "        # Time with torch.inference_mode\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):\n",
    "            with torch.inference_mode():\n",
    "                _ = sample_model(sample_input)\n",
    "        inference_mode_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"torch.no_grad() time: {no_grad_time:.4f} seconds\")\n",
    "        print(f\"torch.inference_mode() time: {inference_mode_time:.4f} seconds\")\n",
    "        print(f\"Speedup: {no_grad_time / inference_mode_time:.2f}x\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Inference mode comparison failed: {e}\\n\")\n",
    "        \n",
    "    # 3. Memory-efficient transformers\n",
    "    print(\"3. Memory Efficient Attention\")\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Memory-efficient attention methods like FlashAttention, xFormers,\")\n",
    "    print(\"or PyTorch's scaled_dot_product_attention can significantly reduce memory usage\")\n",
    "    print(\"and improve inference speed in transformer models.\")\n",
    "    \n",
    "    # Check if xformers is installed\n",
    "    try:\n",
    "        import xformers\n",
    "        print(f\"\\nxFormers version {xformers.__version__} is installed.\")\n",
    "        print(\"You can enable memory-efficient attention with:\")\n",
    "        print(\"pipe.enable_xformers_memory_efficient_attention()\")\n",
    "    except ImportError:\n",
    "        print(\"\\nxFormers is not installed. For better performance, install it with:\")\n",
    "        print(\"pip install xformers\")\n",
    "    \n",
    "    # Check if Flash Attention 2 is available\n",
    "    try:\n",
    "        from transformers.utils import is_flash_attn_2_available\n",
    "        if is_flash_attn_2_available():\n",
    "            print(\"\\nFlash Attention 2 is available for use!\")\n",
    "            print(\"It can be enabled automatically when loading models with:\")\n",
    "            print(\"attn_implementation='flash_attention_2'\")\n",
    "        else:\n",
    "            print(\"\\nFlash Attention 2 is not available.\")\n",
    "    except ImportError:\n",
    "        print(\"\\nCould not check Flash Attention 2 availability.\")\n",
    "\n",
    "# Run the optimization demonstrations\n",
    "demonstrate_pytorch_optimizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a804de",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the capabilities of multimodal generative AI using PyTorch-based models from Hugging Face's transformers and related libraries. We explored:\n",
    "\n",
    "- Text generation with transformer models\n",
    "- Image generation with diffusion models\n",
    "- Text-to-speech synthesis\n",
    "- Text-to-video generation\n",
    "- Music generation\n",
    "- Cross-modal tasks like image captioning and VQA\n",
    "- Combining multiple modalities in practical applications\n",
    "- Building multimodal chatbots that understand both text and images\n",
    "- PyTorch optimization techniques for inference\n",
    "\n",
    "The PyTorch ecosystem provides powerful tools for developing and deploying state-of-the-art multimodal AI systems with excellent performance and GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24180e2",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [Hugging Face Documentation](https://huggingface.co/docs)\n",
    "- [Diffusers Library Documentation](https://huggingface.co/docs/diffusers/index)\n",
    "- [Transformers Library Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [Hugging Face Model Hub](https://huggingface.co/models)\n",
    "- [Audiocraft GitHub Repository](https://github.com/facebookresearch/audiocraft)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
