{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Introduction/introduction.ipynb) [![View HTML](https://img.shields.io/badge/View-HTML-orange)](https://htmlpreview.github.io/?https://github.com/gnoejh/ict1022/blob/main/Introduction/introduction.html)\n",
        "\n",
        "> **Note**: For proper Mermaid diagram rendering, use the HTML version. For interactive code execution, use the Colab version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnoejh/AIBookGitHub/blob/main/Introduction/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# 1. Introduction to Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 1.1 What is Deep Learning?\n",
        "\n",
        "Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to learn hierarchical representations of data. In 2025, deep learning has evolved far beyond its original scope, powering everything from conversational AI assistants to autonomous systems, scientific discovery, and creative applications.\n",
        "\n",
        "**Key Characteristics of Modern Deep Learning:**\n",
        "- **Foundation Models**: Large-scale pre-trained models that can be adapted to diverse tasks\n",
        "- **Multimodal Capabilities**: Integration of text, vision, audio, and other data types\n",
        "- **Emergent Abilities**: Complex behaviors that arise from scale and training\n",
        "- **Efficient Architectures**: Optimized models for edge deployment and real-time inference\n",
        "- **Alignment & Safety**: Focus on creating beneficial and controllable AI systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Deep Learning Pipeline\n",
        "\n",
        "<div class=\"zoomable-mermaid\">\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    subgraph Input\n",
        "        A[Raw Data]\n",
        "    end\n",
        "    subgraph Hidden Layers\n",
        "        B[Simple Features]\n",
        "        C[Complex Features]\n",
        "        D[Abstract Concepts]\n",
        "    end\n",
        "    subgraph Output\n",
        "        E[Predictions]\n",
        "    end\n",
        "    A --> B --> C --> D --> E\n",
        "```\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "### Neural Networks Architecture\n",
        "\n",
        "A neural network consists of:\n",
        "1. Input Layer: Receives raw data\n",
        "2. Hidden Layers: Processes and transforms data\n",
        "3. Output Layer: Produces final predictions\n",
        "4. Weights & Biases: Learnable parameters\n",
        "5. Activation Functions: Non-linear transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Model:\n",
            "ModernNN(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): SiLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (6): SiLU()\n",
            "    (7): Dropout(p=0.1, inplace=False)\n",
            "    (8): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Parameters: 671,754\n",
            "\n",
            "Transformer Model:\n",
            "ModernNN(\n",
            "  (embedding): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (transformer): ModernTransformerBlock(\n",
            "    (attention): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "      (3): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "Parameters: 993,290\n",
            "\n",
            "MLP Output shape: torch.Size([32, 10])\n",
            "Transformer Output shape: torch.Size([32, 10])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LayerNorm, Dropout\n",
        "\n",
        "class ModernTransformerBlock(nn.Module):\n",
        "    \"\"\"Modern transformer block with best practices (2025)\"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        \n",
        "        # Feed-forward network with SwiGLU activation (used in modern LLMs)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.SiLU(),  # SwiGLU variant\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # Pre-normalization (standard in modern architectures)\n",
        "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = x + attn_out  # Residual connection\n",
        "        x = self.norm1(x)\n",
        "        \n",
        "        ff_out = self.ff(x)\n",
        "        x = x + ff_out  # Residual connection\n",
        "        x = self.norm2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class ModernNN(nn.Module):\n",
        "    \"\"\"Modern neural network with current best practices\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_classes, use_transformer=False):\n",
        "        super(ModernNN, self).__init__()\n",
        "        self.use_transformer = use_transformer\n",
        "        \n",
        "        if use_transformer:\n",
        "            # Modern transformer-based architecture\n",
        "            self.embedding = nn.Linear(input_size, hidden_size)\n",
        "            self.transformer = ModernTransformerBlock(hidden_size, n_heads=8, d_ff=hidden_size*4)\n",
        "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        else:\n",
        "            # Modern MLP with improvements\n",
        "            self.layers = nn.Sequential(\n",
        "                nn.Linear(input_size, hidden_size),\n",
        "                LayerNorm(hidden_size),  # Layer normalization instead of batch norm\n",
        "                nn.SiLU(),  # SiLU activation (better than ReLU)\n",
        "                Dropout(0.1),  # Lower dropout rate\n",
        "                \n",
        "                nn.Linear(hidden_size, hidden_size),\n",
        "                LayerNorm(hidden_size),\n",
        "                nn.SiLU(),\n",
        "                Dropout(0.1),\n",
        "                \n",
        "                nn.Linear(hidden_size, num_classes)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.use_transformer:\n",
        "            # Transformer path\n",
        "            x = self.embedding(x.unsqueeze(1))  # Add sequence dimension\n",
        "            x = self.transformer(x)\n",
        "            x = x.mean(dim=1)  # Global average pooling\n",
        "            return self.classifier(x)\n",
        "        else:\n",
        "            # MLP path\n",
        "            return self.layers(x)\n",
        "\n",
        "# Example usage with modern practices\n",
        "def demonstrate_modern_nn():\n",
        "    # Create models\n",
        "    mlp_model = ModernNN(784, 512, 10, use_transformer=False)\n",
        "    transformer_model = ModernNN(784, 256, 10, use_transformer=True)\n",
        "    \n",
        "    print(\"MLP Model:\")\n",
        "    print(mlp_model)\n",
        "    print(f\"Parameters: {sum(p.numel() for p in mlp_model.parameters()):,}\")\n",
        "    \n",
        "    print(\"\\nTransformer Model:\")\n",
        "    print(transformer_model)\n",
        "    print(f\"Parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")\n",
        "    \n",
        "    # Example forward pass\n",
        "    x = torch.randn(32, 784)  # Batch of 32 samples\n",
        "    \n",
        "    with torch.no_grad():  # Inference mode\n",
        "        mlp_output = mlp_model(x)\n",
        "        transformer_output = transformer_model(x)\n",
        "        \n",
        "    print(f\"\\nMLP Output shape: {mlp_output.shape}\")\n",
        "    print(f\"Transformer Output shape: {transformer_output.shape}\")\n",
        "    \n",
        "    # Modern training setup would include:\n",
        "    # - AdamW optimizer with weight decay\n",
        "    # - Cosine learning rate scheduling\n",
        "    # - Mixed precision training (torch.cuda.amp)\n",
        "    # - Gradient clipping\n",
        "    \n",
        "    return mlp_model, transformer_model\n",
        "\n",
        "# Run demonstration\n",
        "mlp_model, transformer_model = demonstrate_modern_nn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "### Types of Deep Learning\n",
        "\n",
        "| Type | Description | Common Applications | Key Architectures |\n",
        "|------|-------------|---------------------|-------------------|\n",
        "| Supervised | Learning from labeled data | Classification, Regression | CNN, RNN |\n",
        "| Unsupervised | Finding patterns in unlabeled data | Clustering, Dimensionality Reduction | Autoencoder, GAN |\n",
        "| Self-Supervised | Learning from data's inherent structure | Pre-training, Representation Learning | BERT, SimCLR |\n",
        "| Reinforcement | Learning through environment interaction | Game AI, Robotics | DQN, PPO |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "### Evolution of Modern AI (2017-Present)\n",
        "\n",
        "<div class=\"zoomable-mermaid\">\n",
        "\n",
        "```mermaid\n",
        "timeline\n",
        "    title Major Deep Learning & AI Breakthroughs\n",
        "    2017 : Transformer Architecture\n",
        "         : \"Attention Is All You Need\"\n",
        "    2018 : BERT & GPT-1\n",
        "         : Transfer Learning in NLP\n",
        "    2019 : GPT-2\n",
        "         : Large Language Models Emerge\n",
        "    2020 : GPT-3 & DDPM\n",
        "         : Few-shot Learning & Diffusion Models\n",
        "    2021 : DALL-E & GitHub Copilot\n",
        "         : Text-to-Image & Code Generation\n",
        "    2022 : ChatGPT & Stable Diffusion\n",
        "         : AI Goes Mainstream\n",
        "    2023 : GPT-4 & Multimodal Models\n",
        "         : Advanced Reasoning & Vision\n",
        "    2024 : GPT-4o & Claude 3.5 Sonnet\n",
        "         : Real-time Multimodal Interaction\n",
        "         : Sora (Text-to-Video)\n",
        "         : Agent Systems & Tool Use\n",
        "    2025 : Advanced Reasoning Models\n",
        "         : Scientific AI & Discovery\n",
        "         : Edge AI & Efficient Models\n",
        "         : AI Safety & Alignment Progress\n",
        "```\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#### Key Modern AI Paradigms\n",
        "\n",
        "| Year | Technology | Impact | Key Innovation |\n",
        "|------|------------|---------|----------------|\n",
        "| 2017-2019 | Transformers & BERT | NLP Revolution | Attention Mechanism |\n",
        "| 2020-2022 | Large Language Models | General AI Assistants | Scale & Transfer Learning |\n",
        "| 2022-2023 | Diffusion Models | Creative AI | Controlled Generation |\n",
        "| 2023-2024 | Multimodal AI | Cross-domain Understanding | Multi-task Learning |\n",
        "| 2024-2025 | **Agentic AI** | **Autonomous Task Execution** | **Tool Use & Planning** |\n",
        "\n",
        "### Agentic AI: The 2024-2025 Breakthrough\n",
        "\n",
        "**Agentic AI** represents AI systems that can:\n",
        "- **Plan and Execute**: Break down complex tasks into steps\n",
        "- **Use Tools**: Access APIs, databases, web browsing, file systems\n",
        "- **Iterative Problem Solving**: Learn from mistakes and refine approaches\n",
        "- **Multi-step Reasoning**: Chain together multiple actions to achieve goals\n",
        "\n",
        "**Key Examples:**\n",
        "- **OpenAI GPTs with Actions**: Custom agents that can use external tools\n",
        "- **Anthropic's Claude with Computer Use**: AI that can interact with computer interfaces\n",
        "- **AutoGPT & LangChain Agents**: Autonomous task completion systems\n",
        "- **GitHub Copilot Workspace**: AI agents for entire software development workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#### Modern AI Capabilities (2025)\n",
        "\n",
        "<div class=\"zoomable-mermaid\">\n",
        "\n",
        "```mermaid\n",
        "mindmap\n",
        "  root((AI Systems 2025))\n",
        "    Language & Reasoning\n",
        "      Advanced Reasoning\n",
        "      Mathematical Problem Solving\n",
        "      Code Generation & Debugging\n",
        "      Scientific Literature Analysis\n",
        "      Multimodal Conversation\n",
        "    Vision & Perception\n",
        "      Real-time Object Detection\n",
        "      3D Scene Understanding\n",
        "      Medical Image Analysis\n",
        "      Satellite & Aerial Imagery\n",
        "      Video Understanding\n",
        "    Audio & Speech\n",
        "      Real-time Translation\n",
        "      Voice Cloning & Synthesis\n",
        "      Music Generation\n",
        "      Audio Editing & Enhancement\n",
        "      Podcast Summarization\n",
        "    Creative Generation\n",
        "      Text-to-Image (Photorealistic)\n",
        "      Text-to-Video (High Quality)\n",
        "      3D Model Generation\n",
        "      Interactive Storytelling\n",
        "      Art Style Transfer\n",
        "    Scientific Discovery\n",
        "      Protein Structure Prediction\n",
        "      Drug Discovery & Design\n",
        "      Climate Modeling\n",
        "      Materials Discovery\n",
        "      Astronomical Analysis\n",
        "    Autonomous Systems\n",
        "      Self-Driving Vehicles\n",
        "      Robotics & Manipulation\n",
        "      Drone Navigation\n",
        "      Smart Home Automation\n",
        "      Industrial Process Control\n",
        "    Agent Capabilities\n",
        "      Tool Use & API Calls\n",
        "      Multi-step Planning\n",
        "      Web Browsing & Research\n",
        "      File System Interaction\n",
        "      Database Querying\n",
        "```\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#### Emerging Trends\n",
        "- Agent-based AI: Autonomous systems that can plan and execute complex tasks\n",
        "- Multimodal Learning: Integration of different types of data and modalities\n",
        "## 1.2 Deep Learning vs Traditional Machine Learning\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "```mermaid\n",
        "graph TB\n",
        "    subgraph Traditional ML\n",
        "        A1[Feature Extraction] --> B1[Feature Engineering]\n",
        "        B1 --> C1[Model Training]\n",
        "    end\n",
        "    subgraph Deep Learning\n",
        "        A2[Raw Data] --> B2[Automatic Feature Learning]\n",
        "        B2 --> C2[End-to-End Training]\n",
        "    end\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "| Aspect | Traditional ML (2025) | Deep Learning (2025) | Foundation Models |\n",
        "|--------|----------------------|----------------------|-------------------|\n",
        "| **Feature Engineering** | Manual, domain expertise | Automatic, learned | Self-supervised, emergent |\n",
        "| **Data Requirements** | Small to medium (1K-100K) | Large (100K-1M+) | Massive (100M-1T+ tokens) |\n",
        "| **Interpretability** | High, explicit rules | Medium, attention maps | Low, but improving tools |\n",
        "| **Training Time** | Minutes to hours | Hours to days | Days to months |\n",
        "| **Hardware** | CPU sufficient | GPU recommended | GPU clusters, TPUs |\n",
        "| **Transfer Learning** | Limited, task-specific | Good, pre-trained models | Excellent, few-shot learning |\n",
        "| **Generalization** | Task-specific | Domain-specific | Cross-domain, emergent abilities |\n",
        "| **Cost** | Low ($1-$100) | Medium ($100-$10K) | High ($10K-$1M+) |\n",
        "| **Examples** | Random Forest, SVM | ResNet, BERT | GPT-4, Claude, Gemini |\n",
        "\n",
        "### Modern Hybrid Approaches\n",
        "\n",
        "In 2025, the boundaries between traditional ML and deep learning have blurred:\n",
        "\n",
        "- **ML-Enhanced DL**: Using traditional ML for preprocessing and post-processing\n",
        "- **DL-Enhanced ML**: Feature extraction with neural networks, classification with traditional methods\n",
        "- **Ensemble Methods**: Combining multiple model types for robust predictions\n",
        "- **AutoML**: Automated selection of appropriate techniques based on data characteristics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 1.3 Modern AI Applications (2025)\n",
        "\n",
        "### 🤖 AI Agents & Assistants\n",
        "- **Conversational AI**: ChatGPT, Claude, Gemini for complex reasoning\n",
        "- **Code Assistants**: GitHub Copilot, Cursor, Replit AI for programming\n",
        "- **Research Assistants**: Scientific literature analysis and hypothesis generation\n",
        "- **Personal Assistants**: Calendar management, email composition, task planning\n",
        "\n",
        "### 🎨 Creative AI & Content Generation\n",
        "- **Text-to-Image**: DALL-E 3, Midjourney, Stable Diffusion for artwork\n",
        "- **Text-to-Video**: Sora, Runway, Pika for video generation\n",
        "- **Music Generation**: Suno, Udio for AI-composed music\n",
        "- **3D Content**: 3D model generation and scene creation\n",
        "\n",
        "### 🧬 Scientific Discovery & Research\n",
        "- **Protein Folding**: AlphaFold 3 for molecular structure prediction\n",
        "- **Drug Discovery**: AI-designed pharmaceuticals and clinical trials\n",
        "- **Materials Science**: Novel material discovery and optimization\n",
        "- **Climate Modeling**: Weather prediction and climate change analysis\n",
        "\n",
        "### 🚗 Autonomous Systems\n",
        "- **Self-Driving Cars**: Tesla FSD, Waymo, Cruise autonomous vehicles\n",
        "- **Robotics**: Humanoid robots, warehouse automation, surgical robots\n",
        "- **Drones**: Autonomous navigation and delivery systems\n",
        "- **Smart Cities**: Traffic optimization and urban planning\n",
        "\n",
        "### 💼 Business & Enterprise\n",
        "- **Customer Service**: AI chatbots and support automation\n",
        "- **Financial Services**: Fraud detection, algorithmic trading, risk analysis\n",
        "- **Healthcare**: Medical imaging, diagnosis assistance, personalized medicine\n",
        "- **Education**: Personalized tutoring and adaptive learning systems\n",
        "\n",
        "### 🔬 Emerging Applications\n",
        "- **Digital Twins**: Virtual replicas of physical systems\n",
        "- **Quantum-AI Hybrid**: Quantum machine learning algorithms\n",
        "- **Brain-Computer Interfaces**: Neural signal processing and control\n",
        "- **Space Exploration**: Autonomous spacecraft and mission planning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modern Vision Model (EfficientNetV2-S):\n",
            "Parameters: 21,458,488\n",
            "Input shape: torch.Size([4, 3, 384, 384])\n",
            "Output shape: torch.Size([4, 1000])\n",
            "Top prediction probabilities: tensor([0.0391, 0.0379, 0.0632, 0.0458])\n",
            "\n",
            "Vision Transformer (ViT-B/16):\n",
            "Parameters: 86,567,656\n",
            "ViT Input shape: torch.Size([2, 3, 224, 224])\n",
            "ViT Output shape: torch.Size([2, 1000])\n",
            "\n",
            "🎯 Key Takeaways:\n",
            "- EfficientNets offer better accuracy/efficiency trade-offs than ResNets\n",
            "- Vision Transformers (ViTs) are becoming dominant for many vision tasks\n",
            "- Modern models use larger input resolutions for better performance\n",
            "- Always use torch.no_grad() for inference to save memory\n",
            "\n",
            "Vision Transformer (ViT-B/16):\n",
            "Parameters: 86,567,656\n",
            "ViT Input shape: torch.Size([2, 3, 224, 224])\n",
            "ViT Output shape: torch.Size([2, 1000])\n",
            "\n",
            "🎯 Key Takeaways:\n",
            "- EfficientNets offer better accuracy/efficiency trade-offs than ResNets\n",
            "- Vision Transformers (ViTs) are becoming dominant for many vision tasks\n",
            "- Modern models use larger input resolutions for better performance\n",
            "- Always use torch.no_grad() for inference to save memory\n"
          ]
        }
      ],
      "source": [
        "# Example: Using modern pre-trained vision models (2025)\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
        "\n",
        "def demonstrate_modern_vision():\n",
        "    # Use EfficientNetV2 - more efficient than ResNet for many tasks\n",
        "    model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "    model.eval()\n",
        "    \n",
        "    # Modern preprocessing pipeline\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(384),  # EfficientNet uses larger input sizes\n",
        "        transforms.CenterCrop(384),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "    \n",
        "    print(\"Modern Vision Model (EfficientNetV2-S):\")\n",
        "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    # Example with fake image data\n",
        "    batch_size = 4\n",
        "    fake_images = torch.randn(batch_size, 3, 384, 384)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(fake_images)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        \n",
        "    print(f\"Input shape: {fake_images.shape}\")\n",
        "    print(f\"Output shape: {outputs.shape}\")  # Should be [4, 1000] for ImageNet classes\n",
        "    print(f\"Top prediction probabilities: {probabilities.max(dim=1).values}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Alternative: Vision Transformer (ViT) - modern attention-based vision\n",
        "def demonstrate_vision_transformer():\n",
        "    try:\n",
        "        from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "        \n",
        "        model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "        model.eval()\n",
        "        \n",
        "        print(\"\\nVision Transformer (ViT-B/16):\")\n",
        "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "        \n",
        "        # ViT uses patches, typically 224x224 input\n",
        "        fake_images = torch.randn(2, 3, 224, 224)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(fake_images)\n",
        "            \n",
        "        print(f\"ViT Input shape: {fake_images.shape}\")\n",
        "        print(f\"ViT Output shape: {outputs.shape}\")\n",
        "        \n",
        "        return model\n",
        "    except ImportError:\n",
        "        print(\"Vision Transformer not available in this torchvision version\")\n",
        "        return None\n",
        "\n",
        "# Run demonstrations\n",
        "efficient_model = demonstrate_modern_vision()\n",
        "vit_model = demonstrate_vision_transformer()\n",
        "\n",
        "print(\"\\n🎯 Key Takeaways:\")\n",
        "print(\"- EfficientNets offer better accuracy/efficiency trade-offs than ResNets\")\n",
        "print(\"- Vision Transformers (ViTs) are becoming dominant for many vision tasks\")\n",
        "print(\"- Modern models use larger input resolutions for better performance\")\n",
        "print(\"- Always use torch.no_grad() for inference to save memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "### Natural Language Processing\n",
        "- Machine Translation\n",
        "- Text Generation\n",
        "- Sentiment Analysis\n",
        "- Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'I love deep learning!'\n",
            "Result: {'label': 'POSITIVE', 'score': 0.7999999999999999}\n",
            "\n",
            "Text: 'Deep learning is revolutionizing AI!'\n",
            "Result: {'label': 'POSITIVE', 'score': 0.7999999999999999}\n",
            "\n",
            "Text: 'This is a neutral statement.'\n",
            "Result: {'label': 'NEUTRAL', 'score': 0.5}\n"
          ]
        }
      ],
      "source": [
        "# Example: Simple sentiment analysis implementation\n",
        "# Note: Transformers library has dependency conflicts in this environment\n",
        "\n",
        "def simple_sentiment_analysis(text):\n",
        "    \"\"\"Simple rule-based sentiment analysis\"\"\"\n",
        "    positive_words = ['love', 'great', 'amazing', 'excellent', 'revolutionizing', \n",
        "                     'fantastic', 'wonderful', 'good', 'awesome', 'brilliant']\n",
        "    negative_words = ['hate', 'bad', 'terrible', 'awful', 'horrible', \n",
        "                     'disappointing', 'poor', 'worst', 'failed']\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    pos_count = sum(1 for word in positive_words if word in text_lower)\n",
        "    neg_count = sum(1 for word in negative_words if word in text_lower)\n",
        "    \n",
        "    if pos_count > neg_count:\n",
        "        return {'label': 'POSITIVE', 'score': min(0.7 + pos_count * 0.1, 0.99)}\n",
        "    elif neg_count > pos_count:\n",
        "        return {'label': 'NEGATIVE', 'score': min(0.7 + neg_count * 0.1, 0.99)}\n",
        "    else:\n",
        "        return {'label': 'NEUTRAL', 'score': 0.5}\n",
        "\n",
        "# Test the sentiment analysis\n",
        "result1 = simple_sentiment_analysis('I love deep learning!')\n",
        "print(f\"Text: 'I love deep learning!'\")\n",
        "print(f\"Result: {result1}\")\n",
        "\n",
        "result2 = simple_sentiment_analysis('Deep learning is revolutionizing AI!')\n",
        "print(f\"\\nText: 'Deep learning is revolutionizing AI!'\")\n",
        "print(f\"Result: {result2}\")\n",
        "\n",
        "result3 = simple_sentiment_analysis('This is a neutral statement.')\n",
        "print(f\"\\nText: 'This is a neutral statement.'\")\n",
        "print(f\"Result: {result3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: The above code is commented out due to dependency conflicts.\n",
            "For production use, ensure proper installation of transformers and its dependencies.\n"
          ]
        }
      ],
      "source": [
        "### 🚀 Emerging Applications & Future Directions\n",
        "\n",
        "- **Autonomous Vehicles**: Full self-driving with advanced perception and planning\n",
        "- **Drug Discovery**: AI-designed molecules and accelerated clinical trials  \n",
        "- **Climate Modeling**: Enhanced weather prediction and climate change mitigation\n",
        "- **Creative Arts**: AI collaboration in music, art, writing, and filmmaking\n",
        "- **Space Exploration**: Autonomous spacecraft navigation and planetary analysis\n",
        "- **Digital Twins**: Real-time virtual replicas of physical systems\n",
        "- **Personalized Education**: Adaptive learning systems tailored to individual needs\n",
        "- **Smart Manufacturing**: Predictive maintenance and quality control optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 1.4 Modern AI Architectures (2025)\n",
        "\n",
        "### Transformer Variants & Innovations\n",
        "\n",
        "**Standard Transformers** remain the foundation, but with significant improvements:\n",
        "- **Mixture of Experts (MoE)**: Sparse activation for efficiency\n",
        "- **Ring Attention**: Handling extremely long sequences\n",
        "- **Mamba/State Space Models**: Alternative to attention mechanisms\n",
        "- **RetNet**: Improved training and inference efficiency\n",
        "\n",
        "### Multimodal Architectures\n",
        "\n",
        "**Vision-Language Models:**\n",
        "- **CLIP-style encoders**: Joint vision-text representations\n",
        "- **Vision Transformers (ViT)**: Image processing with transformers\n",
        "- **Flamingo/BLIP architectures**: Few-shot multimodal learning\n",
        "\n",
        "**Audio-Language Integration:**\n",
        "- **Whisper architecture**: Speech recognition and translation\n",
        "- **AudioLM**: Audio generation and continuation\n",
        "- **SpeechT5**: Unified speech-text processing\n",
        "\n",
        "### Generative Model Architectures\n",
        "\n",
        "**Diffusion Models:**\n",
        "- **DDPM/DDIM**: Denoising diffusion probabilistic models\n",
        "- **Latent Diffusion**: Stable Diffusion architecture\n",
        "- **Flow Matching**: Improved training dynamics\n",
        "- **Consistency Models**: Fast single-step generation\n",
        "\n",
        "**Autoregressive Models:**\n",
        "- **GPT architecture**: Decoder-only transformers\n",
        "- **PaLM architecture**: Pathways Language Model design\n",
        "- **Chinchilla scaling**: Optimal compute-parameter ratios\n",
        "\n",
        "### Efficient Architectures\n",
        "\n",
        "**Model Compression:**\n",
        "- **Knowledge Distillation**: Teacher-student training\n",
        "- **Quantization**: 8-bit, 4-bit, and sub-bit models\n",
        "- **Pruning**: Structured and unstructured sparsity\n",
        "- **Low-Rank Adaptation (LoRA)**: Parameter-efficient fine-tuning\n",
        "\n",
        "**Edge-Optimized Models:**\n",
        "- **MobileNets**: Depthwise separable convolutions\n",
        "- **EfficientNets**: Compound scaling laws\n",
        "- **Phi models**: Small language models with strong performance\n",
        "- **TinyML**: Ultra-low power model deployment\n",
        "\n",
        "### Emerging Paradigms\n",
        "\n",
        "**Neural Architecture Search (NAS):**\n",
        "- Automated discovery of optimal architectures\n",
        "- Hardware-aware architecture optimization\n",
        "- Evolutionary and reinforcement learning approaches\n",
        "\n",
        "**Neuro-Symbolic AI:**\n",
        "- Integration of symbolic reasoning with neural networks\n",
        "- Program synthesis and verification\n",
        "- Compositional generalization\n",
        "\n",
        "**Test-Time Compute:**\n",
        "- Models that can \"think\" longer for harder problems\n",
        "- Chain-of-thought and tree-of-thought reasoning\n",
        "- Iterative refinement and self-correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 1.5 Large Language Models & Foundation Models (2025)\n",
        "\n",
        "| Model | Company | Type | Key Capabilities |\n",
        "|-------|---------|------|------------------|\n",
        "| **GPT-4o** | OpenAI | Multimodal LLM | Real-time voice, vision, text interaction |\n",
        "| **Claude 3.5 Sonnet** | Anthropic | LLM | Advanced reasoning, coding, analysis |\n",
        "| **Gemini Ultra** | Google | Multimodal | Scientific reasoning, mathematics |\n",
        "| **LLaMA 3.1** | Meta | Open LLM | Code generation, multilingual |\n",
        "| **Mistral Large 2** | Mistral AI | LLM | Efficient reasoning, function calling |\n",
        "| **DeepSeek V3** | DeepSeek | LLM | Mathematical reasoning, code generation |\n",
        "| **Phi-4** | Microsoft | Small LLM | Efficient performance on mobile devices |\n",
        "| **Qwen 2.5** | Alibaba | Multilingual | Strong performance in Asian languages |\n",
        "\n",
        "### Foundation Models by Modality\n",
        "\n",
        "| **Vision Models** | **Audio Models** | **Video Models** | **Code Models** |\n",
        "|-------------------|------------------|------------------|-----------------|\n",
        "| DALL-E 3 | Whisper Large V3 | Sora | GitHub Copilot |\n",
        "| Midjourney V6 | ElevenLabs | Runway Gen-3 | CodeT5+ |\n",
        "| Stable Diffusion 3 | AudioCraft | Pika Labs | StarCoder 2 |\n",
        "| Florence-2 | Bark | Stable Video | DeepSeek Coder |\n",
        "\n",
        "### Key Trends in 2025\n",
        "- **Mixture of Experts (MoE)**: More efficient large-scale models\n",
        "- **Multimodal Integration**: Seamless text, vision, audio processing\n",
        "- **Agent Capabilities**: Models that can use tools and plan actions\n",
        "- **Scientific AI**: Models specialized for research and discovery\n",
        "- **Edge Deployment**: Efficient models for mobile and IoT devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 1.6 AI Hardware & Compute Infrastructure (2025)\n",
        "\n",
        "### GPU & AI Accelerators\n",
        "\n",
        "| Chip | Manufacturer | Key Features | Use Case |\n",
        "|------|--------------|---------------|----------|\n",
        "| **H200** | NVIDIA | 141GB HBM3e, 4.8TB/s bandwidth | Large model training |\n",
        "| **B200 Blackwell** | NVIDIA | 20 petaFLOPS, 208GB HBM3e | Next-gen AI training |\n",
        "| **MI300X** | AMD | 192GB HBM3, 5.3TB/s bandwidth | GPU alternative |\n",
        "| **TPU v5e** | Google | Cost-optimized, cloud inference | Efficient inference |\n",
        "| **Trainium2** | AWS | 4x performance vs Trainium1 | AWS cloud training |\n",
        "| **Gaudi3** | Intel | Ethernet-based scaling | Cost-effective training |\n",
        "| **M4 Ultra** | Apple | Unified memory, edge AI | Mobile AI applications |\n",
        "\n",
        "### Specialized AI Chips\n",
        "\n",
        "| **Category** | **Examples** | **Applications** |\n",
        "|--------------|--------------|------------------|\n",
        "| **Edge AI** | Qualcomm NPU, Apple Neural Engine | Mobile devices, IoT |\n",
        "| **Automotive** | Tesla Dojo, Mobileye EyeQ | Autonomous vehicles |\n",
        "| **Datacenter** | Cerebras WSE-3, SambaNova | Large-scale training |\n",
        "| **Quantum-Classical** | IBM Quantum, IonQ | Hybrid algorithms |\n",
        "\n",
        "### Memory & Storage Innovations\n",
        "- **HBM4**: Next-generation high-bandwidth memory\n",
        "- **CXL Memory**: Disaggregated memory architectures\n",
        "- **Storage-Class Memory**: Ultra-fast persistent storage for AI workloads\n",
        "- **Optical Interconnects**: High-speed chip-to-chip communication\n",
        "\n",
        "### Infrastructure Trends\n",
        "- **AI Supercomputers**: Frontier, Aurora, El Capitan\n",
        "- **Edge Computing**: Distributed AI processing\n",
        "- **Quantum-AI Hybrid**: Classical-quantum computing integration\n",
        "- **Green AI**: Energy-efficient model architectures and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 1.7 AI Development Ecosystem (2025)\n",
        "\n",
        "### 🛠️ Frameworks & Libraries\n",
        "\n",
        "**Deep Learning Frameworks:**\n",
        "- **PyTorch 2.5**: Dynamic neural networks, improved compilation\n",
        "- **TensorFlow/JAX**: Google's ecosystem for research and production\n",
        "- **Hugging Face Transformers**: State-of-the-art model library\n",
        "- **LangChain/LlamaIndex**: LLM application development\n",
        "- **OpenAI SDK**: GPT integration and function calling\n",
        "\n",
        "**Specialized Libraries:**\n",
        "- **Diffusers**: Hugging Face diffusion models library\n",
        "- **Whisper**: OpenAI speech recognition\n",
        "- **CLIP**: Vision-language understanding\n",
        "- **Detectron2**: Meta's computer vision platform\n",
        "\n",
        "### 💻 Development Environments\n",
        "\n",
        "**AI-Enhanced IDEs:**\n",
        "- **Cursor**: AI-first code editor with GPT-4 integration\n",
        "- **GitHub Copilot**: AI pair programming in VS Code\n",
        "- **Replit**: Cloud-based AI-powered development\n",
        "- **Jupyter Lab**: Interactive data science notebooks\n",
        "- **Google Colab**: Free GPU/TPU access for research\n",
        "\n",
        "**Cloud Platforms:**\n",
        "- **Hugging Face Spaces**: Model deployment and sharing\n",
        "- **Replicate**: API for running open-source models\n",
        "- **RunPod**: GPU cloud for AI training\n",
        "- **Lambda Labs**: GPU clusters for deep learning\n",
        "\n",
        "### 🚀 Model Deployment & Serving\n",
        "\n",
        "**Inference Platforms:**\n",
        "- **vLLM**: High-performance LLM serving\n",
        "- **TensorRT-LLM**: NVIDIA optimized inference\n",
        "- **Ollama**: Local LLM deployment\n",
        "- **Modal**: Serverless AI infrastructure\n",
        "- **BentoML**: Model serving and deployment framework\n",
        "\n",
        "**Edge Deployment:**\n",
        "- **ONNX Runtime**: Cross-platform model optimization\n",
        "- **TensorFlow Lite**: Mobile and IoT deployment\n",
        "- **Core ML**: Apple ecosystem optimization\n",
        "- **OpenVINO**: Intel edge AI toolkit\n",
        "\n",
        "### 📊 MLOps & Experiment Management\n",
        "\n",
        "**Training & Monitoring:**\n",
        "- **Weights & Biases**: Experiment tracking and visualization\n",
        "- **MLflow**: Open-source ML lifecycle management\n",
        "- **ClearML**: Full MLOps pipeline automation\n",
        "- **Neptune**: Metadata management for ML teams\n",
        "\n",
        "**Data & Model Management:**\n",
        "- **DVC**: Data version control\n",
        "- **Pachyderm**: Data pipelines and versioning\n",
        "- **LakeFS**: Data lakehouse versioning\n",
        "- **Activeloop**: Deep learning data management\n",
        "\n",
        "### 🔧 Specialized Tools\n",
        "\n",
        "**Model Training:**\n",
        "- **DeepSpeed**: Microsoft's training optimization\n",
        "- **FairScale**: Meta's distributed training\n",
        "- **Accelerate**: Hugging Face training utilities\n",
        "- **Lightning**: PyTorch training framework\n",
        "\n",
        "**Model Optimization:**\n",
        "- **Optimum**: Hugging Face model optimization\n",
        "- **TensorRT**: NVIDIA inference optimization\n",
        "- **OpenVINO**: Intel model optimization\n",
        "- **ONNX**: Model interoperability standard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 1.7 AI Developer Tools\n",
        "\n",
        "### 1.7.1 Frameworks and Libraries\n",
        "- TensorFlow: An open-source platform for machine learning.\n",
        "- PyTorch: An open-source machine learning library based on the Torch library.\n",
        "- Keras: A high-level neural networks API, written in Python and capable of running on top of TensorFlow.\n",
        "- Scikit-learn: A machine learning library for the Python programming language.\n",
        "- Hugging Face Transformers: A library for state-of-the-art NLP models.\n",
        "\n",
        "### 1.7.2 Development Environments\n",
        "- Jupyter Notebook: An open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.\n",
        "- Google Colab: A free Jupyter notebook environment that runs entirely in the cloud.\n",
        "- VS Code: A source-code editor made by Microsoft for Windows, Linux, and macOS.\n",
        "- PyCharm: An integrated development environment (IDE) used in computer programming, specifically for the Python language.\n",
        "\n",
        "### 1.7.3 Model Deployment and Serving\n",
        "- TensorFlow Serving: A flexible, high-performance serving system for machine learning models, designed for production environments.\n",
        "- TorchServe: A flexible and easy-to-use tool for serving PyTorch models.\n",
        "- ONNX Runtime: A cross-platform, high-performance scoring engine for Open Neural Network Exchange (ONNX) models.\n",
        "- FastAPI: A modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.7.4 Experiment Tracking and Management\n",
        "- MLflow: An open-source platform for managing the end-to-end machine learning lifecycle.\n",
        "- Weights & Biases: A tool for experiment tracking, model optimization, and dataset versioning.\n",
        "- Neptune.ai: A metadata store for MLOps, built for research and production teams that run a lot of experiments.\n",
        "- MLflow: An open-source platform for managing the end-to-end machine learning lifecycle.\n",
        "- Weights & Biases: A tool for experiment tracking, model optimization, and dataset versioning.\n",
        "- Neptune.ai: A metadata store for MLOps, built for research and production teams that run a lot of experiments.\n",
        "- Comet.ml: A machine learning platform that allows data scientists and AI practitioners to track, compare, explain, and optimize experiments and models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Discussions & Future Directions (2025)\n",
        "\n",
        "### Summary of Key Developments\n",
        "\n",
        "- **Foundation Models**: Large-scale pre-trained models have become the dominant paradigm\n",
        "- **Multimodal AI**: Integration of text, vision, audio, and other modalities in single systems\n",
        "- **Agent Capabilities**: AI systems can now use tools, browse the web, and execute complex tasks\n",
        "- **Efficiency Breakthroughs**: Smaller models achieving strong performance through better architectures\n",
        "- **Safety Focus**: Increased emphasis on alignment, safety, and responsible AI development\n",
        "\n",
        "### Critical Questions for 2025 and Beyond\n",
        "\n",
        "1. **Scaling vs. Efficiency**: Will continued scaling lead to AGI, or do we need fundamentally new architectures?\n",
        "\n",
        "2. **Multimodal Integration**: How can we better integrate different modalities for more human-like understanding?\n",
        "\n",
        "3. **AI Safety & Alignment**: How do we ensure increasingly capable AI systems remain beneficial and controllable?\n",
        "\n",
        "4. **Scientific Discovery**: Can AI accelerate scientific breakthroughs in climate, medicine, and physics?\n",
        "\n",
        "5. **Economic Impact**: How will AI transform work, education, and economic structures?\n",
        "\n",
        "6. **Edge Computing**: How can we deploy powerful AI capabilities on mobile and IoT devices?\n",
        "\n",
        "7. **Interpretability**: Can we understand and explain the decisions of complex AI systems?\n",
        "\n",
        "8. **Data Quality**: How do we handle data scarcity, bias, and quality in training foundation models?\n",
        "\n",
        "### Emerging Research Directions\n",
        "\n",
        "- **Test-Time Compute**: Models that can \"think\" longer for harder problems\n",
        "- **Agent Systems**: AI that can plan, use tools, and interact with environments\n",
        "- **Neuro-Symbolic AI**: Combining neural networks with symbolic reasoning\n",
        "- **Quantum-AI Hybrid**: Leveraging quantum computing for machine learning\n",
        "- **Embodied AI**: AI systems that interact with the physical world\n",
        "- **Federated Learning**: Training models across distributed, private datasets\n",
        "- **Continual Learning**: AI systems that learn continuously without forgetting\n",
        "\n",
        "### Call to Action\n",
        "\n",
        "The field of AI is evolving rapidly. Whether you're a researcher, developer, or simply an interested observer, staying informed about these developments is crucial. Consider:\n",
        "\n",
        "- **Learning**: Continuously update your knowledge of AI developments\n",
        "- **Building**: Create applications that solve real-world problems responsibly\n",
        "- **Contributing**: Participate in open-source projects and research\n",
        "- **Advocating**: Support responsible AI development and deployment practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.9 AI Safety & Alignment (2025)\n",
        "\n",
        "As AI systems become more capable, ensuring they are safe, beneficial, and aligned with human values has become a critical priority.\n",
        "\n",
        "### Key Safety Challenges\n",
        "\n",
        "| Challenge | Description | Current Approaches |\n",
        "|-----------|-------------|-------------------|\n",
        "| **Alignment** | Ensuring AI systems pursue intended goals | Constitutional AI, RLHF, DPO |\n",
        "| **Robustness** | Reliable performance across diverse conditions | Adversarial training, uncertainty quantification |\n",
        "| **Interpretability** | Understanding how AI systems make decisions | Mechanistic interpretability, attention visualization |\n",
        "| **Controllability** | Ability to direct and constrain AI behavior | Fine-tuning, prompt engineering, guardrails |\n",
        "\n",
        "### Safety Techniques\n",
        "\n",
        "**Reinforcement Learning from Human Feedback (RLHF):**\n",
        "- Training models to align with human preferences\n",
        "- Used in ChatGPT, Claude, and other conversational AI\n",
        "- Iterative improvement through human feedback\n",
        "\n",
        "**Constitutional AI:**\n",
        "- Training models with explicit principles and values\n",
        "- Self-correction and reasoning about harmful outputs\n",
        "- Developed by Anthropic for Claude models\n",
        "\n",
        "**Red Teaming & Evaluation:**\n",
        "- Systematic testing for harmful or unintended behaviors\n",
        "- Adversarial prompting and stress testing\n",
        "- Multi-stakeholder evaluation frameworks\n",
        "\n",
        "### Emerging Safety Research\n",
        "\n",
        "**Mechanistic Interpretability:**\n",
        "- Understanding neural network internal representations\n",
        "- Circuit analysis and feature visualization\n",
        "- Tools: TransformerLens, Baukit, Captum\n",
        "\n",
        "**AI Governance & Policy:**\n",
        "- Regulatory frameworks for AI development\n",
        "- International cooperation on AI safety standards\n",
        "- Ethics boards and responsible AI practices\n",
        "\n",
        "**Technical Safety Research:**\n",
        "- Specification gaming and reward hacking prevention\n",
        "- Mesa-optimization and inner alignment\n",
        "- Scalable oversight and weak-to-strong generalization\n",
        "\n",
        "### Industry Initiatives\n",
        "\n",
        "- **OpenAI**: GPT-4 safety evaluations, preparedness framework\n",
        "- **Anthropic**: Constitutional AI, AI safety research\n",
        "- **DeepMind**: Sparrow, alignment research, AI safety unit\n",
        "- **Partnership on AI**: Cross-industry collaboration on AI safety\n",
        "- **AI Safety Institute**: Government initiatives for AI evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussions\n",
        "\n",
        "Summary:\n",
        "- This chapter introduced fundamental deep learning concepts and related technologies.\n",
        "- We explored modern applications across business and emerging technologies.\n",
        "\n",
        "Questions:\n",
        "1. How do diffusion models differ from transformer models?\n",
        "2. What makes Transformer architectures a breakthrough compared to older NLP models?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aibookgithub",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "rise": {
      "autolaunch": false,
      "center": true,
      "controls": true,
      "enable_chalkboard": true,
      "enable_spotlight": true,
      "height": 800,
      "history": true,
      "overlay": "<div class='my-top-bar'>Deep Learning</div>",
      "progress": true,
      "scroll": true,
      "slideNumber": true,
      "theme": "simple",
      "transition": "slide",
      "width": 1200
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
