{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Introduction/1 introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.1 What is Deep Learning?\n",
    "\n",
    "Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers (typically 3 or more hidden layers) to learn hierarchical representations of data. Unlike traditional machine learning approaches that require manual feature engineering, deep learning systems automatically discover intricate patterns and abstractions from raw data through end-to-end learning.\n",
    "\n",
    "### Core Principles\n",
    "\n",
    "**Hierarchical Learning**: Deep networks learn features at multiple levels of abstraction, from simple edges and shapes in early layers to complex concepts in deeper layers.\n",
    "\n",
    "**Representation Learning**: The network automatically learns meaningful representations of the input data without explicit programming of features.\n",
    "\n",
    "**End-to-End Optimization**: The entire system is trained jointly, allowing for optimal feature extraction and decision-making in a unified framework.\n",
    "\n",
    "### Why \"Deep\"?\n",
    "\n",
    "The term \"deep\" refers to the depth of the neural network - the number of layers between input and output. Modern deep learning models can have hundreds or even thousands of layers, enabling them to model extremely complex relationships in data.\n",
    "\n",
    "### Modern Context (2025)\n",
    "\n",
    "As of 2025, deep learning has evolved beyond traditional neural networks to encompass:\n",
    "- **Foundation Models**: Large-scale pre-trained models that can be adapted for multiple tasks\n",
    "- **Multimodal AI**: Systems that process and understand multiple types of data (text, images, audio, video) simultaneously\n",
    "- **Agent-Based AI**: Systems that can plan, reason, and execute complex multi-step tasks autonomously\n",
    "- **Emergent Capabilities**: Behaviors that arise naturally from scale and training, not explicitly programmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Learning Pipeline\n",
    "\n",
    "<div class=\"zoomable-mermaid\">\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input\n",
    "        A[Raw Data]\n",
    "    end\n",
    "    subgraph Hidden Layers\n",
    "        B[Simple Features]\n",
    "        C[Complex Features]\n",
    "        D[Abstract Concepts]\n",
    "    end\n",
    "    subgraph Output\n",
    "        E[Predictions]\n",
    "    end\n",
    "    A --> B --> C --> D --> E\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Networks Architecture\n",
    "\n",
    "A neural network consists of:\n",
    "1. Input Layer: Receives raw data\n",
    "2. Hidden Layers: Processes and transforms data\n",
    "3. Output Layer: Produces final predictions\n",
    "4. Weights & Biases: Learnable parameters\n",
    "5. Activation Functions: Non-linear transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (5): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModernNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ModernNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Example usage\n",
    "model = ModernNN(784, 256, 10)  # MNIST-like architecture\n",
    "print(model)  # Print the model architecture\n",
    "x = torch.randn(64, 784)  # 64 samples with 784 features each\n",
    "y = model(x)  # Forward pass\n",
    "print(y.shape)  # torch.Size([64, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Deep Learning\n",
    "\n",
    "| Type | Description | Common Applications | Key Architectures |\n",
    "|------|-------------|---------------------|-------------------|\n",
    "| Supervised | Learning from labeled data | Classification, Regression | CNN, RNN |\n",
    "| Unsupervised | Finding patterns in unlabeled data | Clustering, Dimensionality Reduction | Autoencoder, GAN |\n",
    "| Self-Supervised | Learning from data's inherent structure | Pre-training, Representation Learning | BERT, SimCLR |\n",
    "| Reinforcement | Learning through environment interaction | Game AI, Robotics | DQN, PPO |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evolution of Modern AI (2012-Present)\n",
    "\n",
    "<div class=\"zoomable-mermaid\">\n",
    "\n",
    "```mermaid\n",
    "timeline\n",
    "    title Major Deep Learning & AI Breakthroughs\n",
    "    2012 : AlexNet\n",
    "         : Deep Learning Revolution Begins\n",
    "    2014 : GANs Introduced\n",
    "         : Deep Learning for Image Generation\n",
    "    2017 : Transformer Architecture\n",
    "         : Attention Is All You Need\n",
    "    2018 : BERT\n",
    "         : Transfer Learning in NLP\n",
    "    2019 : GPT-2\n",
    "         : Large Language Models Emerge\n",
    "    2020 : GPT-3 & DDPM\n",
    "         : Few-shot Learning & Novel Diffusion Models\n",
    "    2021 : DALL-E & CLIP\n",
    "         : Text-to-Image Generation & Vision-Language Models\n",
    "    2022 : ChatGPT & Stable Diffusion\n",
    "         : AI Goes Mainstream\n",
    "    2023 : GPT-4 & Multimodal AI\n",
    "         : Advanced Reasoning & Cross-Modal Understanding\n",
    "    2024 : Sora & Claude 3.5 & o1\n",
    "         : Text-to-Video & Advanced Reasoning & Deep Thinking\n",
    "    2025 : Agent AI & Gemini 2.0 & GPT-4o\n",
    "         : Autonomous Systems & Real-time Multimodal AI\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Key Modern AI Paradigms\n",
    "\n",
    "| Year | Technology | Impact | Key Innovation |\n",
    "|------|------------|---------|----------------|\n",
    "| 2017-2019 | Transformers & BERT | NLP Revolution | Attention Mechanism & Transfer Learning |\n",
    "| 2020-2022 | Large Language Models | General AI Assistants | Scale & Few-shot Learning |\n",
    "| 2022-2023 | Diffusion Models & ChatGPT | Creative AI & Conversational AI | Controlled Generation & Human Feedback |\n",
    "| 2023-2024 | Multimodal AI & Reasoning Models | Cross-domain Understanding | Multi-task Learning & Chain-of-Thought |\n",
    "| 2024-2025 | Agent AI & o1 Models | Autonomous Systems & Deep Reasoning | Multi-step Planning & System 2 Thinking |\n",
    "| 2025 | Real-time Multimodal AI | Interactive AI Systems | Live audio/video processing & instant response |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modern AI Capabilities (2025)\n",
    "\n",
    "<div class=\"zoomable-mermaid\">\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Modern AI 2025))\n",
    "    Language\n",
    "      Advanced Reasoning\n",
    "      Code Generation\n",
    "      Multilingual Translation\n",
    "      Scientific Writing\n",
    "    Vision\n",
    "      Video Generation\n",
    "      3D Scene Creation\n",
    "      Real-time Enhancement\n",
    "      Medical Imaging\n",
    "    Audio\n",
    "      Real-time Translation\n",
    "      Music Composition\n",
    "      Voice Cloning\n",
    "      Spatial Audio\n",
    "    Multimodal\n",
    "      Live Video Chat\n",
    "      Text-to-Everything\n",
    "      Cross-modal Search\n",
    "      Unified Understanding\n",
    "    Agents\n",
    "      Task Planning\n",
    "      Tool Usage\n",
    "      Multi-step Execution\n",
    "      Autonomous Decision Making\n",
    "    Reasoning\n",
    "      Mathematical Proofs\n",
    "      Scientific Discovery\n",
    "      Complex Problem Solving\n",
    "      Chain-of-Thought\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Emerging Trends\n",
    "- Agent-based AI: Autonomous systems that can plan and execute complex tasks\n",
    "- Multimodal Learning: Integration of different types of data and modalities\n",
    "## 1.2 Deep Learning vs Traditional Machine Learning\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph Traditional ML\n",
    "        A1[Feature Extraction] --> B1[Feature Engineering]\n",
    "        B1 --> C1[Model Training]\n",
    "    end\n",
    "    subgraph Deep Learning\n",
    "        A2[Raw Data] --> B2[Automatic Feature Learning]\n",
    "        B2 --> C2[End-to-End Training]\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Aspect | Traditional ML | Deep Learning |\n",
    "|--------|----------------|---------------|\n",
    "| Feature Engineering | Manual | Automatic |\n",
    "| Data Requirements | Small to Medium | Large |\n",
    "| Interpretability | Higher | Lower |\n",
    "| Training Time | Faster | Slower |\n",
    "| Hardware Requirements | CPU sufficient | GPU/TPU preferred |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.3 Modern Applications (2025)\n",
    "\n",
    "### Computer Vision & Media Generation\n",
    "- **Advanced Object Detection**: Real-time detection with 99%+ accuracy\n",
    "- **Semantic Segmentation**: Pixel-level understanding for autonomous vehicles\n",
    "- **Face Recognition**: Privacy-preserving biometric systems\n",
    "- **Medical Imaging**: AI-assisted diagnosis with FDA approvals\n",
    "- **Video Generation**: Text-to-video with 60+ second clips (Sora, Runway)\n",
    "- **3D Content Creation**: Text-to-3D models and scenes\n",
    "- **Real-time Video Enhancement**: Live upscaling and restoration\n",
    "\n",
    "### Natural Language & Code\n",
    "- **Advanced Code Generation**: Full applications from natural language\n",
    "- **Mathematical Reasoning**: Complex problem solving (o1, Claude 3.5)\n",
    "- **Multilingual Translation**: Real-time, context-aware translation\n",
    "- **Document Analysis**: Multi-page PDF understanding and summarization\n",
    "- **Scientific Research**: Literature review and hypothesis generation\n",
    "- **Legal Analysis**: Contract review and legal document processing\n",
    "\n",
    "### Autonomous Systems & Robotics\n",
    "- **AI Agents**: Multi-step task execution and planning\n",
    "- **Robotic Process Automation**: Intelligent workflow automation\n",
    "- **Autonomous Vehicles**: Level 4+ self-driving capabilities\n",
    "- **Smart Manufacturing**: Predictive maintenance and quality control\n",
    "- **Personal Assistants**: Proactive task management and scheduling\n",
    "\n",
    "### Creative & Entertainment\n",
    "- **AI Filmmaking**: Full video production with AI actors and scenes\n",
    "- **Music Generation**: Studio-quality compositions in any style\n",
    "- **Game Development**: Procedural world and character generation\n",
    "- **Interactive Storytelling**: Adaptive narratives and characters\n",
    "- **Virtual Production**: Real-time rendering for film and TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Modern Vision Models (2025) - UV Environment Compatible\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "# Check and install vision packages if needed\n",
    "def ensure_vision_packages():\n",
    "    \"\"\"Ensure vision packages are available in uv environment\"\"\"\n",
    "    try:\n",
    "        import torchvision\n",
    "        from transformers import CLIPModel\n",
    "        print(\"✓ Vision packages already available\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"Installing vision packages...\")\n",
    "        packages = [\"torchvision\", \"transformers\", \"pillow\"]\n",
    "        for package in packages:\n",
    "            try:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                             capture_output=True, check=True)\n",
    "                print(f\"✓ Installed {package}\")\n",
    "            except:\n",
    "                print(f\"⚠ Could not install {package}\")\n",
    "        return False\n",
    "\n",
    "# Ensure packages are available\n",
    "ensure_vision_packages()\n",
    "\n",
    "try:\n",
    "    # Example 1: State-of-the-art image classification\n",
    "    import torchvision.models as models\n",
    "    \n",
    "    # Use a lighter model that's more likely to work\n",
    "    model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "    model.eval()\n",
    "    print(\"✓ EfficientNet B0 loaded successfully\")\n",
    "    \n",
    "    # Example 2: Check if CLIP is available\n",
    "    try:\n",
    "        from transformers import CLIPModel, CLIPProcessor\n",
    "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        print(\"✓ CLIP model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ CLIP model not available: {e}\")\n",
    "        print(\"This is normal - some models may not load in all environments\")\n",
    "\n",
    "    print(\"\\n=== Modern Vision Capabilities ===\")\n",
    "    print(\"- EfficientNet: Efficient architecture with high accuracy\")\n",
    "    print(\"- CLIP: Zero-shot image classification with text descriptions\")\n",
    "    print(\"- Vision Transformers: Attention-based image understanding\")\n",
    "    print(\"- Multimodal models: Combined vision and language reasoning\")\n",
    "    \n",
    "    # Show model info\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"- GPU acceleration available: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"- Running on CPU (GPU not available)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Vision models not available: {e}\")\n",
    "    print(\"This may be due to package compatibility in the uv environment\")\n",
    "    print(\"Try installing packages manually: uv add torchvision transformers pillow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Natural Language Processing\n",
    "- Machine Translation\n",
    "- Text Generation\n",
    "- Sentiment Analysis\n",
    "- Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up UV Environment for AI Development\n",
    "\n",
    "If you're using a `uv` environment (like this notebook), you may need to install packages differently:\n",
    "\n",
    "**Option 1: Terminal Setup (Recommended)**\n",
    "```bash\n",
    "# Navigate to your project directory\n",
    "cd w:\\AIBookGitHub\n",
    "\n",
    "# Install packages with uv\n",
    "uv add transformers torch accelerate huggingface_hub[hf_xet]\n",
    "uv add ipykernel jupyter\n",
    "\n",
    "# Sync the environment\n",
    "uv sync\n",
    "```\n",
    "\n",
    "**Option 2: Alternative Installation**\n",
    "```bash\n",
    "# If uv add doesn't work, try:\n",
    "uv pip install transformers torch accelerate huggingface_hub[hf_xet]\n",
    "```\n",
    "\n",
    "**Common Issues & Solutions:**\n",
    "- **No module named pip**: This is normal for uv environments\n",
    "- **Tokenizer errors**: Some models may have compatibility issues - the code below includes fallbacks\n",
    "- **CUDA not available**: The code will automatically fallback to CPU\n",
    "- **Package conflicts**: Use `uv lock --upgrade` to update dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages for uv environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\threading.py\"\u001b[0m, line \u001b[35m1041\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"w:\\AIBookGitHub\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\"\u001b[0m, line \u001b[35m772\u001b[0m, in \u001b[35mrun_closure\u001b[0m\n",
      "    \u001b[31m_threading_Thread_run\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\threading.py\"\u001b[0m, line \u001b[35m992\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1612\u001b[0m, in \u001b[35m_readerthread\u001b[0m\n",
      "    buffer.append(\u001b[31mfh.read\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "                  \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mUnicodeDecodeError\u001b[0m: \u001b[35m'cp949' codec can't decode byte 0xec in position 444: illegal multibyte sequence\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Could not install transformers>=4.35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\threading.py\"\u001b[0m, line \u001b[35m1041\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"w:\\AIBookGitHub\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\"\u001b[0m, line \u001b[35m772\u001b[0m, in \u001b[35mrun_closure\u001b[0m\n",
      "    \u001b[31m_threading_Thread_run\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\threading.py\"\u001b[0m, line \u001b[35m992\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1612\u001b[0m, in \u001b[35m_readerthread\u001b[0m\n",
      "    buffer.append(\u001b[31mfh.read\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "                  \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mUnicodeDecodeError\u001b[0m: \u001b[35m'cp949' codec can't decode byte 0xeb in position 1167: illegal multibyte sequence\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Could not install torch>=2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\threading.py\"\u001b[0m, line \u001b[35m1041\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"w:\\AIBookGitHub\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\"\u001b[0m, line \u001b[35m772\u001b[0m, in \u001b[35mrun_closure\u001b[0m\n",
      "    \u001b[31m_threading_Thread_run\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\threading.py\"\u001b[0m, line \u001b[35m992\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1612\u001b[0m, in \u001b[35m_readerthread\u001b[0m\n",
      "    buffer.append(\u001b[31mfh.read\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "                  \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mUnicodeDecodeError\u001b[0m: \u001b[35m'cp949' codec can't decode byte 0xeb in position 1167: illegal multibyte sequence\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Could not install accelerate>=0.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-18 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\threading.py\"\u001b[0m, line \u001b[35m1041\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"w:\\AIBookGitHub\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\"\u001b[0m, line \u001b[35m772\u001b[0m, in \u001b[35mrun_closure\u001b[0m\n",
      "    \u001b[31m_threading_Thread_run\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\threading.py\"\u001b[0m, line \u001b[35m992\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1612\u001b[0m, in \u001b[35m_readerthread\u001b[0m\n",
      "    buffer.append(\u001b[31mfh.read\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "                  \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mUnicodeDecodeError\u001b[0m: \u001b[35m'cp949' codec can't decode byte 0xeb in position 1165: illegal multibyte sequence\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Could not install huggingface_hub[hf_xet]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to compare versions for numpy>=1.17: need=1.17 found=None. This is unusual. Consider reinstalling numpy.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Import libraries\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ PyTorch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\AIBookGitHub\\.venv\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     is_pretty_midi_available,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\AIBookGitHub\\.venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:57\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m     55\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, check dependency_versions_table.py\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\AIBookGitHub\\.venv\\Lib\\site-packages\\transformers\\utils\\versions.py:117\u001b[39m, in \u001b[36mrequire_version_core\u001b[39m\u001b[34m(requirement)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m hint = \u001b[33m\"\u001b[39m\u001b[33mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.[dev]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m` if you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre working with git main\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\AIBookGitHub\\.venv\\Lib\\site-packages\\transformers\\utils\\versions.py:111\u001b[39m, in \u001b[36mrequire_version\u001b[39m\u001b[34m(requirement, hint)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted.items():\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\AIBookGitHub\\.venv\\Lib\\site-packages\\transformers\\utils\\versions.py:39\u001b[39m, in \u001b[36m_compare_versions\u001b[39m\u001b[34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compare_versions\u001b[39m(op, got_ver, want_ver, requirement, pkg, hint):\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m got_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     40\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to compare versions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: need=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwant_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m found=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is unusual. Consider\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m reinstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m         )\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version.parse(got_ver), version.parse(want_ver)):\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     45\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Unable to compare versions for numpy>=1.17: need=1.17 found=None. This is unusual. Consider reinstalling numpy."
     ]
    }
   ],
   "source": [
    "# Modern NLP with transformers (2025) - UV Environment Compatible\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "# Function to install packages in uv environment\n",
    "def install_with_uv(packages):\n",
    "    \"\"\"Install packages using uv instead of pip for uv environments\"\"\"\n",
    "    for package in packages:\n",
    "        try:\n",
    "            result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"✓ Installed {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            # Fallback to uv add if pip fails\n",
    "            try:\n",
    "                result = subprocess.run([\"uv\", \"add\", package], \n",
    "                                      capture_output=True, text=True, check=True)\n",
    "                print(f\"✓ Installed {package} with uv\")\n",
    "            except:\n",
    "                print(f\"⚠ Could not install {package}\")\n",
    "\n",
    "# Install required packages\n",
    "required_packages = [\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"torch>=2.0.0\", \n",
    "    \"accelerate>=0.20.0\",\n",
    "    \"huggingface_hub[hf_xet]\"  # For better HF downloads\n",
    "]\n",
    "\n",
    "print(\"Installing packages for uv environment...\")\n",
    "install_with_uv(required_packages)\n",
    "\n",
    "# Import libraries\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    import torch\n",
    "    print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ Import error: {e}\")\n",
    "    print(\"Please restart the kernel and try again\")\n",
    "\n",
    "# Example 1: Simple sentiment analysis (more reliable)\n",
    "print(\"\\n=== Simple Sentiment Analysis ===\")\n",
    "try:\n",
    "    classifier = pipeline(\n",
    "        'sentiment-analysis',\n",
    "        model='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    texts = [\n",
    "        \"I'm absolutely thrilled about the advancements in AI this year!\",\n",
    "        \"The new multimodal models are revolutionary but concerning.\"\n",
    "    ]\n",
    "    \n",
    "    for text in texts:\n",
    "        try:\n",
    "            result = classifier(text)\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Sentiment: {result[0]['label']} (confidence: {result[0]['score']:.3f})\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Could not load sentiment classifier: {e}\")\n",
    "\n",
    "# Example 2: Fallback for emotion detection\n",
    "print(\"\\n=== Alternative Approach (if above fails) ===\")\n",
    "try:\n",
    "    # Use a more stable model\n",
    "    emotion_classifier = pipeline(\n",
    "        'text-classification',\n",
    "        model='SamLowe/roberta-base-go_emotions',\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    test_text = \"Deep learning is absolutely fascinating and revolutionary!\"\n",
    "    result = emotion_classifier(test_text)\n",
    "    print(f\"Text: {test_text}\")\n",
    "    print(f\"Top emotions: {result[:3]}\")  # Show top 3 emotions\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Emotion classifier not available: {e}\")\n",
    "    print(\"This is normal - some models may have compatibility issues\")\n",
    "\n",
    "print(\"\\n=== Environment Info ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Using uv environment: {'uv' in sys.executable or '.venv' in sys.executable}\")\n",
    "print(\"For best results, ensure all packages are installed in the uv environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.4 Modern Architectures (Updated 2025)\n",
    "\n",
    "### Transformer Architecture Evolution\n",
    "\n",
    "The Transformer architecture has evolved significantly since 2017:\n",
    "\n",
    "**Original Transformer (2017)**:\n",
    "- Self-Attention Mechanism\n",
    "- Positional Encoding\n",
    "- Multi-Head Attention\n",
    "- Feed-Forward Networks\n",
    "\n",
    "**Modern Transformer Innovations (2023-2025)**:\n",
    "- **Mixture of Experts (MoE)**: Sparse activation for efficient scaling\n",
    "- **Ring Attention**: Distributed attention for ultra-long sequences\n",
    "- **Mamba/State Space Models**: Linear scaling for sequence length\n",
    "- **Retrieval-Augmented Generation (RAG)**: External knowledge integration\n",
    "- **Chain-of-Thought Reasoning**: Step-by-step problem decomposition\n",
    "\n",
    "### Advanced Generative Models\n",
    "\n",
    "**Diffusion Models (2024-2025)**:\n",
    "- **Consistency Models**: Single-step generation\n",
    "- **Flow Matching**: Improved training dynamics\n",
    "- **Rectified Flow**: Straight trajectory generation\n",
    "- **Video Diffusion**: Temporal consistency in video generation\n",
    "\n",
    "**New Generative Paradigms**:\n",
    "- **Autoregressive Video Models**: Token-based video generation\n",
    "- **Neural Radiance Fields (NeRF)**: 3D scene reconstruction\n",
    "- **Gaussian Splatting**: Real-time 3D rendering\n",
    "- **World Models**: Environment simulation and prediction\n",
    "\n",
    "### Multimodal Architectures\n",
    "\n",
    "**Vision-Language Models**:\n",
    "- **CLIP Variants**: Enhanced vision-text understanding\n",
    "- **DALL-E 3**: Improved text-to-image generation\n",
    "- **GPT-4V**: Multimodal reasoning capabilities\n",
    "- **Flamingo**: Few-shot learning across modalities\n",
    "\n",
    "**Audio-Visual Models**:\n",
    "- **Whisper V3**: Multilingual speech recognition\n",
    "- **MusicLM**: Text-to-music generation\n",
    "- **SpeechT5**: Unified speech processing\n",
    "\n",
    "### Reasoning Architectures\n",
    "\n",
    "**System 2 Thinking Models (2024-2025)**:\n",
    "- **o1 Series**: Deep reasoning with chain-of-thought\n",
    "- **AlphaCode 2**: Advanced code generation with search\n",
    "- **Minerva**: Mathematical reasoning capabilities\n",
    "- **Tool-using Models**: API integration and function calling\n",
    "\n",
    "Applications:\n",
    "- Mathematical problem solving\n",
    "- Scientific reasoning\n",
    "- Complex planning tasks\n",
    "- Multi-step code generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.5 Large Models (Updated September 2025)\n",
    "\n",
    "### Foundation Models & Language Models\n",
    "\n",
    "| Model | Company | Size (Parameters) | Properties | Release |\n",
    "|-------|---------|-------------------|------------|---------|\n",
    "| GPT-4o | OpenAI | 1.76 trillion | Multimodal, Real-time audio/video, Advanced reasoning | 2024-2025 |\n",
    "| o1-preview | OpenAI | ~200 billion | Deep reasoning, Chain-of-thought, Problem solving | 2024 |\n",
    "| Claude 3.5 Sonnet | Anthropic | ~185 billion | Advanced reasoning, Coding, Long context (200K tokens) | 2024 |\n",
    "| Gemini 2.0 Flash | Google | 1.5 trillion | Ultra-fast inference, Multimodal, Agent capabilities | 2025 |\n",
    "| Llama 3.1 405B | Meta | 405 billion | Open-source, Multilingual, Code generation | 2024 |\n",
    "| DeepSeek-V3 | DeepSeek | 671 billion | MoE architecture, Mathematical reasoning | 2025 |\n",
    "| Grok-3 | xAI | 1.4 trillion | Real-time web access, Multimodal understanding | 2025 |\n",
    "| Qwen2.5-Max | Alibaba | 720 billion | Multilingual, Mathematical reasoning, Code generation | 2025 |\n",
    "\n",
    "### Specialized Models\n",
    "\n",
    "| Model | Company | Size | Specialization | Notable Features |\n",
    "|-------|---------|------|----------------|------------------|\n",
    "| Sora Turbo | OpenAI | 3 billion | Video generation | 60s videos, 1080p, Camera controls |\n",
    "| DALL-E 3 HD | OpenAI | 20 billion | Image generation | Photorealistic images, Text integration |\n",
    "| Runway Gen-3 | Runway | 15 billion | Video generation | 10s clips, Motion control |\n",
    "| MidJourney v7 | MidJourney | 8 billion | Artistic image generation | Style consistency, 3D rendering |\n",
    "| Stable Video 3D | Stability AI | 12 billion | 3D content generation | Text-to-3D, Video-to-3D |\n",
    "| AlphaFold 3 | DeepMind | 200 million | Protein structure prediction | All biological molecules |\n",
    "| Codestral Mamba | Mistral | 7 billion | Code generation | State-space models, Long sequences |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.6 GPU and AI Chips (Updated September 2025)\n",
    "\n",
    "### Training & Inference Hardware\n",
    "\n",
    "| Chip | Manufacturer | Key Features | Memory | Release |\n",
    "|------|--------------|---------------|---------|---------|\n",
    "| H200 | NVIDIA | 4.8x HBM3e, 141GB memory, FP8 precision | 141GB HBM3e | 2024 |\n",
    "| B200 Blackwell | NVIDIA | 20 petaFLOPS FP4, NVLink 5th gen | 192GB HBM3e | 2025 |\n",
    "| GH200 Grace Hopper | NVIDIA | CPU+GPU superchip, 900GB/s bandwidth | 96GB HBM3 | 2024 |\n",
    "| MI300X | AMD | CDNA 3, 192GB HBM3, open ecosystem | 192GB HBM3 | 2024 |\n",
    "| TPU v5e | Google | Cost-optimized, 2x performance vs v4 | 32GB HBM | 2024 |\n",
    "| TPU v5p | Google | 4x training performance vs v4 | 95GB HBM | 2024 |\n",
    "| Gaudi3 | Intel | 2x training performance vs Gaudi2 | 128GB HBM2e | 2024 |\n",
    "| M4 Max | Apple | 40-core GPU, unified memory, 400GB/s | Up to 128GB | 2024 |\n",
    "| M4 Ultra | Apple | 80-core GPU, dual-chip design | Up to 256GB | 2025 |\n",
    "| CS-3 | Cerebras | Wafer-scale, 44GB on-chip memory | 44GB on-die | 2025 |\n",
    "| Groq LPU | Groq | Ultra-low latency inference | 230MB SRAM | 2024 |\n",
    "\n",
    "### Emerging AI Hardware\n",
    "\n",
    "| Technology | Company | Innovation | Target Use Case |\n",
    "|------------|---------|------------|-----------------|\n",
    "| Neuromorphic Chips | Intel Loihi 2 | Brain-inspired computing | Edge AI, low power |\n",
    "| Photonic AI | Lightmatter | Light-based computing | Datacenter interconnects |\n",
    "| Quantum-Classical | IBM | Hybrid quantum processing | Scientific computing |\n",
    "| RRAM Chips | Samsung | In-memory computing | Edge inference |\n",
    "| DNA Storage | Microsoft | Biological data storage | Long-term archival |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.7 AI Developer Tools (Updated 2025)\n",
    "\n",
    "### 1.7.1 Modern Frameworks and Libraries\n",
    "- **PyTorch 2.4+**: Native compilation, improved performance, mobile deployment\n",
    "- **TensorFlow 2.17+**: Enhanced Keras 3.0, JAX backend support\n",
    "- **JAX**: High-performance ML research, automatic differentiation\n",
    "- **Hugging Face Transformers**: 400k+ models, multimodal support\n",
    "- **LangChain/LlamaIndex**: LLM application frameworks\n",
    "- **vLLM**: High-throughput LLM inference serving\n",
    "- **Ollama**: Local LLM deployment and management\n",
    "- **MLX**: Apple Silicon native ML framework\n",
    "\n",
    "### 1.7.2 Development Environments\n",
    "- **VS Code + GitHub Copilot**: AI-powered coding assistance\n",
    "- **Cursor**: AI-first code editor with context awareness\n",
    "- **Google Colab**: Free GPU/TPU access, collaborative notebooks\n",
    "- **Jupyter Lab**: Enhanced notebook experience\n",
    "- **Deepnote**: Collaborative data science platform\n",
    "- **Gradient**: Paperspace's ML development environment\n",
    "- **Codespaces**: Cloud development environments\n",
    "\n",
    "### 1.7.3 Model Deployment and Serving\n",
    "- **Hugging Face Inference Endpoints**: Managed model hosting\n",
    "- **Modal**: Serverless compute for ML workloads\n",
    "- **Replicate**: Simple model deployment and scaling\n",
    "- **BentoML**: Model serving framework with MLOps\n",
    "- **Ray Serve**: Distributed model serving\n",
    "- **TensorRT-LLM**: NVIDIA optimized LLM inference\n",
    "- **ONNX Runtime**: Cross-platform model optimization\n",
    "\n",
    "### 1.7.4 Experiment Tracking and MLOps\n",
    "- **Weights & Biases**: Comprehensive experiment tracking\n",
    "- **MLflow**: Open-source ML lifecycle management\n",
    "- **Neptune.ai**: Metadata store for ML experiments\n",
    "- **ClearML**: End-to-end MLOps platform\n",
    "- **DVC**: Data version control and ML pipelines\n",
    "- **Kubeflow**: Kubernetes-native ML workflows\n",
    "- **ZenML**: Production-ready ML pipelines\n",
    "\n",
    "### 1.7.5 AI Agent and Application Frameworks\n",
    "- **LangGraph**: Multi-agent workflows and state management\n",
    "- **AutoGen**: Microsoft's multi-agent conversation framework\n",
    "- **CrewAI**: Collaborative AI agent orchestration\n",
    "- **Semantic Kernel**: Microsoft's AI orchestration SDK\n",
    "- **LlamaIndex**: Data framework for LLM applications\n",
    "- **Haystack**: End-to-end NLP framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.8 Quantum Computing (Updated September 2025)\n",
    "\n",
    "### Current Quantum Systems\n",
    "\n",
    "| Quantum System | Manufacturer | Qubits | Technology | Key Achievement |\n",
    "|-----------------|--------------|--------|------------|-----------------|\n",
    "| Condor | IBM | 1,121 | Superconducting | Largest qubit count (2023) |\n",
    "| Heron | IBM | 133 | Superconducting | Utility-scale quantum advantage |\n",
    "| Forte | IonQ | 32 | Trapped ion | 99.8% fidelity, cloud access |\n",
    "| H-Series H2 | Quantinuum | 56 | Trapped ion | Logical qubit operations |\n",
    "| Advantage2 | D-Wave | 5,760 | Quantum annealing | Optimization problems |\n",
    "| Willow | Google | 105 | Superconducting | Quantum error correction breakthrough |\n",
    "| Atom Computing | Atom Computing | 1,180 | Neutral atom | Record neutral atom system |\n",
    "| PsiQuantum | PsiQuantum | 1M+ | Photonic | Million-qubit roadmap |\n",
    "\n",
    "### Quantum-AI Integration\n",
    "\n",
    "| Application | Technology | Progress | Timeline |\n",
    "|-------------|------------|----------|----------|\n",
    "| Quantum ML | Variational quantum algorithms | Research phase | 2025-2027 |\n",
    "| Quantum NLP | Quantum natural language processing | Early experiments | 2026-2028 |\n",
    "| Drug Discovery | Quantum molecular simulation | Proof-of-concept | 2025-2030 |\n",
    "| Optimization | Quantum approximate optimization | Commercial trials | 2024-2026 |\n",
    "| Cryptography | Post-quantum cryptography | Deployment phase | 2024-2025 |\n",
    "\n",
    "### Quantum Software Ecosystem\n",
    "\n",
    "| Platform | Company | Purpose | Programming Language |\n",
    "|----------|---------|---------|---------------------|\n",
    "| Qiskit | IBM | Quantum development | Python |\n",
    "| Cirq | Google | Quantum circuits | Python |\n",
    "| Q# | Microsoft | Quantum programming | Q# |\n",
    "| PennyLane | Xanadu | Quantum ML | Python |\n",
    "| Amazon Braket | AWS | Cloud quantum access | Python |\n",
    "| Ocean SDK | D-Wave | Quantum annealing | Python |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "### Summary:\n",
    "- This chapter introduced fundamental deep learning concepts and the latest AI developments through September 2025\n",
    "- We explored modern applications spanning from creative AI to autonomous systems\n",
    "- Covered breakthrough architectures including advanced transformers, diffusion models, and reasoning systems\n",
    "- Examined current hardware landscape from next-gen GPUs to quantum computing\n",
    "- Reviewed the modern AI development ecosystem and tools\n",
    "\n",
    "### Key 2025 Developments:\n",
    "- **Real-time Multimodal AI**: Live audio/video processing with instant responses\n",
    "- **Agent Systems**: Autonomous AI that can plan and execute complex multi-step tasks\n",
    "- **Deep Reasoning Models**: o1-style systems that can think step-by-step through complex problems\n",
    "- **Video Generation**: High-quality, controllable video creation from text prompts\n",
    "- **Quantum-AI Integration**: Early experiments in quantum-enhanced machine learning\n",
    "\n",
    "### Discussion Questions:\n",
    "\n",
    "1. **Architecture Evolution**: How do modern reasoning models like o1 differ from traditional transformer architectures, and what implications does this have for AI capabilities?\n",
    "\n",
    "2. **Multimodal Integration**: What are the advantages and challenges of real-time multimodal AI systems that can process audio, video, and text simultaneously?\n",
    "\n",
    "3. **Agent AI vs. Traditional AI**: How do autonomous AI agents change the paradigm from Q&A systems to task-executing systems?\n",
    "\n",
    "4. **Hardware Scaling**: With quantum computers reaching 1000+ qubits, what applications might benefit first from quantum-classical hybrid AI systems?\n",
    "\n",
    "5. **Ethical Considerations**: As AI systems become more capable of autonomous action and deep reasoning, what new ethical frameworks do we need?\n",
    "\n",
    "6. **Future Trends**: Based on the 2024-2025 developments, what do you predict will be the next major breakthrough in AI?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting UV Environment Issues\n",
    "\n",
    "### Common Problems and Solutions\n",
    "\n",
    "#### 1. \"No module named pip\" Error\n",
    "This is normal for uv environments. Use these alternatives:\n",
    "```bash\n",
    "# Instead of %pip install, use:\n",
    "uv add package_name\n",
    "\n",
    "# Or in terminal:\n",
    "uv pip install package_name\n",
    "```\n",
    "\n",
    "#### 2. Package Installation Issues\n",
    "```bash\n",
    "# Update uv itself\n",
    "pip install --upgrade uv\n",
    "\n",
    "# Sync environment\n",
    "uv sync\n",
    "\n",
    "# Force reinstall problematic packages\n",
    "uv pip install --force-reinstall transformers\n",
    "```\n",
    "\n",
    "#### 3. CUDA/GPU Issues\n",
    "- The code automatically detects GPU availability\n",
    "- If CUDA is not detected, models will run on CPU (slower but functional)\n",
    "- For GPU support, ensure PyTorch CUDA version matches your CUDA installation\n",
    "\n",
    "#### 4. Model Loading Errors\n",
    "- Some large models may not load due to memory constraints\n",
    "- The updated code uses lighter alternatives and includes fallbacks\n",
    "- Internet connection required for first-time model downloads\n",
    "\n",
    "#### 5. Tokenizer Conversion Errors\n",
    "- This often happens with newer models and older transformers versions\n",
    "- The updated code uses more stable, well-tested models\n",
    "- Consider using `transformers>=4.35.0` for better compatibility\n",
    "\n",
    "#### 6. Jupyter Kernel Issues\n",
    "If you encounter persistent issues:\n",
    "```bash\n",
    "# Restart kernel and clear output\n",
    "# Or in terminal:\n",
    "uv run jupyter lab --allow-root\n",
    "```\n",
    "\n",
    "### Best Practices for UV + AI Development\n",
    "1. **Use uv add for package management**: More reliable than pip in uv environments\n",
    "2. **Keep transformers updated**: `uv add transformers>=4.35.0`\n",
    "3. **Use fallback models**: The code includes backup options for compatibility\n",
    "4. **Monitor memory usage**: Large models require significant RAM/VRAM\n",
    "5. **Test incrementally**: Run code cells one at a time to identify issues\n",
    "\n",
    "### Performance Optimization\n",
    "- **CPU**: Most examples will work but may be slower\n",
    "- **GPU**: Ensure CUDA toolkit is properly installed\n",
    "- **Memory**: Close other applications to free up RAM for large models\n",
    "- **Network**: Stable internet connection for model downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immediate Fix for Current Permission Issue\n",
    "\n",
    "### Problem: Access Denied Error\n",
    "You're seeing permission errors because some packages are locked by the running Jupyter kernel.\n",
    "\n",
    "### Solution Steps:\n",
    "\n",
    "1. **Restart Jupyter Kernel**: \n",
    "   - In VS Code: `Ctrl+Shift+P` → \"Restart Kernel\"\n",
    "   - This will free up locked files\n",
    "\n",
    "2. **Install packages in terminal** (after kernel restart):\n",
    "   ```bash\n",
    "   cd w:\\AIBookGitHub\n",
    "   uv add transformers>=4.35.0 torch>=2.0.0 torchvision accelerate\n",
    "   uv add \"huggingface_hub[hf_xet]\" ipykernel jupyter\n",
    "   ```\n",
    "\n",
    "3. **Alternative if still having issues**:\n",
    "   ```bash\n",
    "   # Delete and recreate the environment\n",
    "   rm -rf .venv\n",
    "   uv venv\n",
    "   uv add transformers torch torchvision accelerate ipykernel\n",
    "   ```\n",
    "\n",
    "4. **Verify installation**:\n",
    "   ```bash\n",
    "   uv pip list | grep -E \"(torch|transformers)\"\n",
    "   ```\n",
    "\n",
    "### Quick Test Cell\n",
    "Run this after fixing the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Environment Test - Run this after fixing the uv environment\n",
    "import sys\n",
    "import importlib.util\n",
    "\n",
    "def check_package(package_name):\n",
    "    \"\"\"Check if a package is available\"\"\"\n",
    "    spec = importlib.util.find_spec(package_name)\n",
    "    return spec is not None\n",
    "\n",
    "print(\"=== Environment Check ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"UV environment: {'uv' in sys.executable or '.venv' in sys.executable}\")\n",
    "\n",
    "# Check essential packages\n",
    "packages = ['torch', 'transformers', 'torchvision', 'accelerate']\n",
    "for pkg in packages:\n",
    "    status = \"✓\" if check_package(pkg) else \"✗\"\n",
    "    print(f\"{status} {pkg}\")\n",
    "\n",
    "# Test basic functionality\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\n✓ PyTorch {torch.__version__}\")\n",
    "    print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Simple tensor operation test\n",
    "    x = torch.randn(2, 3)\n",
    "    y = torch.matmul(x, x.T)\n",
    "    print(f\"✓ Basic tensor operations working\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "\n",
    "print(\"\\n=== Next Steps ===\")\n",
    "print(\"If all packages show ✓, you can run the AI examples above.\")\n",
    "print(\"If any show ✗, restart kernel and run the installation commands.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibookgithub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "rise": {
   "autolaunch": false,
   "center": true,
   "controls": true,
   "enable_chalkboard": true,
   "enable_spotlight": true,
   "height": 800,
   "history": true,
   "overlay": "<div class='my-top-bar'>Deep Learning</div>",
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "theme": "simple",
   "transition": "slide",
   "width": 1200
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
