{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Models in Deep Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sequential models represent the simplest and most straightforward way of building neural networks. They consist of a linear stack of layers where data flows from the input layer through each subsequent layer to the output, without any branching or complex topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "A sequential model with $L$ layers can be represented mathematically as a composition of functions:\n",
    "\n",
    "$$f(x) = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1(x)$$\n",
    "\n",
    "Where each function $f_i$ represents the operation performed by the $i$-th layer. If we denote the input as $x^{(0)}$ and the output of each layer $i$ as $x^{(i)}$, then:\n",
    "\n",
    "$$x^{(i)} = f_i(x^{(i-1)})$$\n",
    "\n",
    "For a typical fully connected layer with weights $W_i$ and biases $b_i$, followed by an activation function $\\sigma_i$, the operation is:\n",
    "\n",
    "$$x^{(i)} = \\sigma_i(W_i x^{(i-1)} + b_i)$$\n",
    "\n",
    "The sequential architecture simply chains these operations together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Representation\n",
    "\n",
    "A sequential model can be visualized as a linear chain of operations:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Input --> Layer1[Layer 1] --> Layer2[Layer 2] --> DotDot[...] --> LayerL[Layer L] --> Output\n",
    "    style Input fill:#f9f9f9,stroke:#333,stroke-width:1px,color:black\n",
    "    style Output fill:#f9f9f9,stroke:#333,stroke-width:1px,color:black\n",
    "    style Layer1 fill:#bbdefb,stroke:#333,stroke-width:1px,color:black\n",
    "    style Layer2 fill:#bbdefb,stroke:#333,stroke-width:1px,color:black\n",
    "    style DotDot fill:none,stroke:none,color:black\n",
    "    style LayerL fill:#bbdefb,stroke:#333,stroke-width:1px,color:black\n",
    "```\n",
    "\n",
    "More detailed diagram showing the mathematical operations in each layer:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Input((\"Input x\")) --> L1[\"Layer 1\\nW₁x + b₁\\n + σ₁\"] --> L2[\"Layer 2\\nW₂x + b₂\\n + σ₂\"] --> Dots[\"...\"] --> LL[\"Layer L\\nWₗx + bₗ\\n + σₗ\"] --> Output((\"Output\"))\n",
    "    style Input fill:#f5f5f5,stroke:#333,stroke-width:1px,color:black\n",
    "    style Output fill:#f5f5f5,stroke:#333,stroke-width:1px,color:black\n",
    "    style L1 fill:#bbdefb,stroke:#333,stroke-width:1px,color:black\n",
    "    style L2 fill:#bbdefb,stroke:#333,stroke-width:1px,color:black\n",
    "    style Dots fill:none,stroke:none,color:black\n",
    "    style LL fill:#bbdefb,stroke:#333,stroke-width:1px,color:black\n",
    "```\n",
    "\n",
    "Where $\\sigma_i$ represents the activation function at layer $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with PyTorch\n",
    "\n",
    "PyTorch provides the `nn.Sequential` container to create sequential models easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a simple sequential model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),  # Input layer: 784 features (e.g., MNIST images) -> 128 hidden units\n",
    "    nn.ReLU(),            # Activation function\n",
    "    nn.Dropout(0.2),      # Regularization: 20% dropout\n",
    "    nn.Linear(128, 64),   # Hidden layer: 128 -> 64 units\n",
    "    nn.ReLU(),            # Activation function\n",
    "    nn.Linear(64, 10)     # Output layer: 64 -> 10 classes\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass Calculation\n",
    "\n",
    "Let's trace how data flows through this sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 784])\n",
      "After first linear layer: torch.Size([1, 128])\n",
      "After first activation: torch.Size([1, 128])\n",
      "After second linear layer: torch.Size([1, 64])\n",
      "After second activation: torch.Size([1, 64])\n",
      "Final output shape: torch.Size([1, 10])\n",
      "\n",
      "Direct model output matches manual calculation: False\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor (batch_size=1, features=784)\n",
    "x = torch.randn(1, 784)\n",
    "\n",
    "# Manual forward pass through each layer to show the sequential nature\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Layer 1: Linear + ReLU\n",
    "z1 = model[0](x)  # Linear\n",
    "print(f\"After first linear layer: {z1.shape}\")\n",
    "a1 = model[1](z1)  # ReLU\n",
    "print(f\"After first activation: {a1.shape}\")\n",
    "\n",
    "# Layer 2: Dropout + Linear + ReLU\n",
    "d1 = model[2](a1)  # Dropout\n",
    "z2 = model[3](d1)  # Linear\n",
    "print(f\"After second linear layer: {z2.shape}\")\n",
    "a2 = model[4](z2)  # ReLU\n",
    "print(f\"After second activation: {a2.shape}\")\n",
    "\n",
    "# Output layer: Linear\n",
    "output = model[5](a2)  # Linear\n",
    "print(f\"Final output shape: {output.shape}\")\n",
    "\n",
    "# Verify that this matches the direct forward pass\n",
    "direct_output = model(x)\n",
    "print(\"\\nDirect model output matches manual calculation:\", \n",
    "      torch.allclose(output, direct_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Ways to Define Sequential Models in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with OrderedDict:\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Model as a class:\n",
      "SimpleSequentialNet(\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using nn.Sequential with ordered dictionary\n",
    "from collections import OrderedDict\n",
    "\n",
    "model_ordered = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(784, 128)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('dropout', nn.Dropout(0.2)),\n",
    "    ('fc2', nn.Linear(128, 64)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('fc3', nn.Linear(64, 10))\n",
    "]))\n",
    "\n",
    "print(\"Model with OrderedDict:\")\n",
    "print(model_ordered)\n",
    "\n",
    "# Method 2: Using a class definition\n",
    "class SimpleSequentialNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleSequentialNet, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "model_class = SimpleSequentialNet()\n",
    "print(\"\\nModel as a class:\")\n",
    "print(model_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations of Sequential Models\n",
    "\n",
    "### Advantages\n",
    "- **Simplicity**: Easy to define and understand\n",
    "- **Maintainability**: Clear structure makes code maintenance easier\n",
    "- **Efficiency**: Straightforward optimization for simple architectures\n",
    "\n",
    "### Limitations\n",
    "- **Restricted Topology**: Only supports linear layer stacking\n",
    "- **No Branching**: Cannot implement skip connections or parallel paths\n",
    "- **Limited Layer Reuse**: Difficult to reuse layers or weights\n",
    "- **No Conditional Logic**: Cannot incorporate dynamic behavior based on inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Sequential Model\n",
    "\n",
    "Below is a complete example of training a sequential model on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and data ready for training. Uncomment the train() and test() calls to run training.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations for MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset (if available)\n",
    "try:\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000)\n",
    "    \n",
    "    # Create sequential model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),  # Flatten 28x28 images to 784 features\n",
    "        nn.Linear(784, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    )\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    def train(model, train_loader, criterion, optimizer, epochs=3):\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Print statistics\n",
    "                running_loss += loss.item()\n",
    "                if batch_idx % 100 == 99:    # Print every 100 mini-batches\n",
    "                    print(f'Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {running_loss/100:.4f}')\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "    def test(model, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()  # Sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "        \n",
    "    # Train the model (uncomment to run)\n",
    "    # train(model, train_loader, criterion, optimizer, epochs=3)\n",
    "    # test(model, test_loader)\n",
    "    \n",
    "    print(\"Model and data ready for training. Uncomment the train() and test() calls to run training.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Couldn't load MNIST dataset: {e}\\nThis is expected if running offline without dataset.\")\n",
    "    print(\"The code structure is preserved for reference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Analysis: Gradients in Sequential Models\n",
    "\n",
    "The backpropagation algorithm in a sequential model follows a clear path from the output layer back to the input layer. For a loss function $L$ and a sequential model with $n$ layers, we can derive the gradient with respect to the weights $W_i$ of layer $i$ using the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_i} = \\frac{\\partial L}{\\partial x^{(n)}} \\cdot \\frac{\\partial x^{(n)}}{\\partial x^{(n-1)}} \\cdot ... \\cdot \\frac{\\partial x^{(i+1)}}{\\partial x^{(i)}} \\cdot \\frac{\\partial x^{(i)}}{\\partial W_i}$$\n",
    "\n",
    "Where $x^{(i)}$ is the output of layer $i$. This demonstrates the sequential nature of gradient computation in these models.\n",
    "\n",
    "When using activation functions like ReLU:\n",
    "\n",
    "$$\\frac{\\partial \\text{ReLU}(z)}{\\partial z} = \\begin{cases}\n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "This introduces non-linearity that allows the network to learn complex patterns while maintaining the simple sequential structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Sequential models provide an intuitive and straightforward approach to building neural networks. They are ideal for problems where the data can be processed in a linear fashion, from input to output. Their mathematical simplicity makes them easier to understand, implement, and debug. However, their linear structure can be limiting for complex architectures requiring skip connections or branches.\n",
    "\n",
    "For more complex architectures, we'll need to move beyond the sequential model to functional and subclassing approaches, which we'll explore in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Basic**: Explain how data flows through a sequential model both mathematically and conceptually.\n",
    "\n",
    "2. **Intermediate**: Implement a sequential model for digit classification on MNIST that achieves at least 97% accuracy.\n",
    "\n",
    "3. **Advanced**: Compare and contrast the performance of different sequential architectures with varying depths and widths. Analyze how changing layer sizes affects model performance.\n",
    "\n",
    "4. **Practical**: Design a sequential model for a regression task on a dataset of your choice. Implement early stopping to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
