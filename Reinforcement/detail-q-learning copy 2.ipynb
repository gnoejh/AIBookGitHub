{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0419a2a1",
   "metadata": {},
   "source": [
    "# Detailed Q-Learning Implementation with Agent Movement Visualization\n",
    "\n",
    "This notebook provides a detailed implementation of Q-learning with visualizations to observe agent movement step-by-step in a grid world environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffd833",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.patches import Rectangle, Arrow\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40195ad",
   "metadata": {},
   "source": [
    "## 2. Create Grid World Environment\n",
    "\n",
    "We'll create a simple grid world where:\n",
    "- The agent starts at a specified position\n",
    "- There are obstacles that the agent must avoid\n",
    "- There is a goal state that the agent must reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv:\n",
    "    def __init__(self, height=5, width=5):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.n_states = height * width\n",
    "        self.n_actions = 4  # up, right, down, left\n",
    "        \n",
    "        # Define obstacles (represented as -1) and goal state (10)\n",
    "        self.grid = np.zeros((height, width))\n",
    "        self.obstacles = [(1, 1), (1, 2), (2, 1), (3, 3)]\n",
    "        self.goal_state = (4, 4)\n",
    "        \n",
    "        # Set obstacles and goal in the grid\n",
    "        for obs in self.obstacles:\n",
    "            self.grid[obs] = -1\n",
    "        self.grid[self.goal_state] = 10\n",
    "        \n",
    "        # Starting position\n",
    "        self.start_state = (0, 0)\n",
    "        self.current_state = self.start_state\n",
    "        \n",
    "        # Define action mappings\n",
    "        self.actions = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (0, 1),   # right\n",
    "            2: (1, 0),   # down\n",
    "            3: (0, -1)   # left\n",
    "        }\n",
    "        \n",
    "        # History to store all states visited\n",
    "        self.history = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        self.history = [self.current_state]\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Get the direction from the action\n",
    "        direction = self.actions[action]\n",
    "        \n",
    "        # Calculate new state\n",
    "        new_row = self.current_state[0] + direction[0]\n",
    "        new_col = self.current_state[1] + direction[1]\n",
    "        new_state = (new_row, new_col)\n",
    "        \n",
    "        # Check if new state is valid\n",
    "        if self._is_valid_state(new_state):\n",
    "            self.current_state = new_state\n",
    "        \n",
    "        # Add to history\n",
    "        self.history.append(self.current_state)\n",
    "        \n",
    "        # Calculate reward and check if done\n",
    "        if self.current_state == self.goal_state:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif self.current_state in self.obstacles:\n",
    "            reward = -10\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1  # Small negative reward to encourage finding goal faster\n",
    "            done = False\n",
    "        \n",
    "        return self.current_state, reward, done\n",
    "    \n",
    "    def _is_valid_state(self, state):\n",
    "        row, col = state\n",
    "        # Check if state is within grid boundaries\n",
    "        if 0 <= row < self.height and 0 <= col < self.width:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def render(self, q_table=None, show_history=False):\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # Create a colormap for the grid\n",
    "        cmap = mcolors.ListedColormap(['white', 'red', 'green'])\n",
    "        bounds = [-2, -0.5, 0.5, 11]\n",
    "        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "        \n",
    "        # Draw the grid\n",
    "        ax.imshow(self.grid, cmap=cmap, norm=norm)\n",
    "        \n",
    "        # Draw grid lines\n",
    "        for i in range(self.width + 1):\n",
    "            ax.axvline(i - 0.5, color='black', linewidth=1)\n",
    "        for i in range(self.height + 1):\n",
    "            ax.axhline(i - 0.5, color='black', linewidth=1)\n",
    "        \n",
    "        # Draw diagonal lines to divide cells into 4 triangular regions\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                # Draw lines from corners to center of cell\n",
    "                # Top left to center\n",
    "                ax.plot([j-0.5, j], [i-0.5, i], color='black', linewidth=0.5)\n",
    "                # Top right to center\n",
    "                ax.plot([j+0.5, j], [i-0.5, i], color='black', linewidth=0.5)\n",
    "                # Bottom left to center\n",
    "                ax.plot([j-0.5, j], [i+0.5, i], color='black', linewidth=0.5)\n",
    "                # Bottom right to center\n",
    "                ax.plot([j+0.5, j], [i+0.5, i], color='black', linewidth=0.5)\n",
    "        \n",
    "        # Mark obstacles and goal\n",
    "        for obs in self.obstacles:\n",
    "            ax.add_patch(Rectangle((obs[1] - 0.5, obs[0] - 0.5), 1, 1, fill=True, color='red', alpha=0.5))\n",
    "        ax.add_patch(Rectangle((self.goal_state[1] - 0.5, self.goal_state[0] - 0.5), 1, 1, fill=True, color='green', alpha=0.5))\n",
    "        \n",
    "        # Highlight current position\n",
    "        ax.add_patch(Rectangle((self.current_state[1] - 0.5, self.current_state[0] - 0.5), 1, 1, fill=True, color='blue', alpha=0.3))\n",
    "        \n",
    "        # Show history of positions if requested\n",
    "        if show_history and len(self.history) > 1:\n",
    "            history_y = [state[0] for state in self.history]\n",
    "            history_x = [state[1] for state in self.history]\n",
    "            ax.plot(history_x, history_y, 'o-', color='purple', markersize=8, alpha=0.6)\n",
    "        \n",
    "        # Show Q-values as arrows if provided\n",
    "        if q_table is not None:\n",
    "            self._draw_policy_arrows(ax, q_table)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xticks(np.arange(self.width))\n",
    "        ax.set_yticks(np.arange(self.height))\n",
    "        ax.set_xticklabels(np.arange(self.width))\n",
    "        ax.set_yticklabels(np.arange(self.height))\n",
    "        ax.set_title('Grid World')\n",
    "        plt.show()\n",
    "    \n",
    "    def _draw_policy_arrows(self, ax, q_table):\n",
    "        # Draw arrows for all four actions in each state with corresponding q-values\n",
    "        action_directions = {\n",
    "            0: (0, -0.25),    # Up arrow position in cell\n",
    "            1: (0.25, 0),     # Right arrow position in cell\n",
    "            2: (0, 0.25),     # Down arrow position in cell\n",
    "            3: (-0.25, 0)     # Left arrow position in cell\n",
    "        }\n",
    "        \n",
    "        arrow_offsets = {\n",
    "            0: (0, -0.15),    # Text offset for Up q-value\n",
    "            1: (0.18, 0),     # Text offset for Right q-value\n",
    "            2: (0, 0.18),     # Text offset for Down q-value\n",
    "            3: (-0.18, 0)     # Text offset for Left q-value\n",
    "        }\n",
    "        \n",
    "        # Define the direction of each arrow\n",
    "        arrow_directions = {\n",
    "            0: (0, -0.1),     # Up\n",
    "            1: (0.1, 0),      # Right\n",
    "            2: (0, 0.1),      # Down\n",
    "            3: (-0.1, 0)      # Left\n",
    "        }\n",
    "        \n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if (i, j) not in self.obstacles and (i, j) != self.goal_state:\n",
    "                    state_idx = i * self.width + j\n",
    "                    \n",
    "                    # Find the best action for this state\n",
    "                    best_action = np.argmax(q_table[state_idx]) if np.any(q_table[state_idx] != 0) else None\n",
    "                    \n",
    "                    # Draw arrows and q-values for all four actions\n",
    "                    for action in range(self.n_actions):\n",
    "                        q_value = q_table[state_idx, action]\n",
    "                        \n",
    "                        # Skip if q-value is zero (unexplored)\n",
    "                        if q_value == 0 and not np.any(q_table[state_idx] != 0):\n",
    "                            continue\n",
    "                            \n",
    "                        # Get arrow start position and direction\n",
    "                        pos_offset = action_directions[action]\n",
    "                        start_x, start_y = j + pos_offset[0], i + pos_offset[1]\n",
    "                        dx, dy = arrow_directions[action]\n",
    "                        \n",
    "                        # Set arrow color (green for best action, black for others)\n",
    "                        arrow_color = 'green' if action == best_action else 'black'\n",
    "                        arrow_width = 2 if action == best_action else 1\n",
    "                        \n",
    "                        # Draw the arrow\n",
    "                        ax.arrow(start_x, start_y, dx, dy, head_width=0.07, head_length=0.07, \n",
    "                                fc=arrow_color, ec=arrow_color, linewidth=arrow_width)\n",
    "                        \n",
    "                        # Add q-value text\n",
    "                        text_offset = arrow_offsets[action]\n",
    "                        text_x, text_y = j + text_offset[0], i + text_offset[1]\n",
    "                        ax.text(text_x, text_y, f'{q_value:.1f}', ha='center', va='center', \n",
    "                                fontsize=7, color=arrow_color, fontweight='bold' if action == best_action else 'normal')\n",
    "        \n",
    "    def animate_episode(self, history):\n",
    "        # For animation of an entire episode\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        def update(frame):\n",
    "            ax.clear()\n",
    "            \n",
    "            # Draw the grid\n",
    "            cmap = mcolors.ListedColormap(['white', 'red', 'green'])\n",
    "            bounds = [-2, -0.5, 0.5, 11]\n",
    "            norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "            ax.imshow(self.grid, cmap=cmap, norm=norm)\n",
    "            \n",
    "            # Draw grid lines\n",
    "            for i in range(self.width + 1):\n",
    "                ax.axvline(i - 0.5, color='black', linewidth=1)\n",
    "            for i in range(self.height + 1):\n",
    "                ax.axhline(i - 0.5, color='black', linewidth=1)\n",
    "            \n",
    "            # Draw diagonal lines to divide cells into 4 triangular regions\n",
    "            for i in range(self.height):\n",
    "                for j in range(self.width):\n",
    "                    # Draw lines from corners to center of cell\n",
    "                    # Top left to center\n",
    "                    ax.plot([j-0.5, j], [i-0.5, i], color='black', linewidth=0.5)\n",
    "                    # Top right to center\n",
    "                    ax.plot([j+0.5, j], [i-0.5, i], color='black', linewidth=0.5)\n",
    "                    # Bottom left to center\n",
    "                    ax.plot([j-0.5, j], [i+0.5, i], color='black', linewidth=0.5)\n",
    "                    # Bottom right to center\n",
    "                    ax.plot([j+0.5, j], [i+0.5, i], color='black', linewidth=0.5)\n",
    "            \n",
    "            # Mark obstacles and goal\n",
    "            for obs in self.obstacles:\n",
    "                ax.add_patch(Rectangle((obs[1] - 0.5, obs[0] - 0.5), 1, 1, fill=True, color='red', alpha=0.5))\n",
    "            ax.add_patch(Rectangle((self.goal_state[1] - 0.5, self.goal_state[0] - 0.5), 1, 1, fill=True, color='green', alpha=0.5))\n",
    "            \n",
    "            # Highlight current position for this frame\n",
    "            if frame < len(history):\n",
    "                current_pos = history[frame]\n",
    "                ax.add_patch(Rectangle((current_pos[1] - 0.5, current_pos[0] - 0.5), 1, 1, fill=True, color='blue', alpha=0.6))\n",
    "                \n",
    "                # Show path up to this point\n",
    "                if frame > 0:\n",
    "                    path_y = [state[0] for state in history[:frame+1]]\n",
    "                    path_x = [state[1] for state in history[:frame+1]]\n",
    "                    ax.plot(path_x, path_y, 'o-', color='purple', markersize=5, alpha=0.6)\n",
    "            \n",
    "            # Set labels and title\n",
    "            ax.set_xticks(np.arange(self.width))\n",
    "            ax.set_yticks(np.arange(self.height))\n",
    "            ax.set_xticklabels(np.arange(self.width))\n",
    "            ax.set_yticklabels(np.arange(self.height))\n",
    "            ax.set_title(f'Agent Movement - Step {frame}')\n",
    "        \n",
    "        anim = animation.FuncAnimation(fig, update, frames=len(history), interval=500)\n",
    "        return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the grid world environment\n",
    "env = GridWorldEnv(height=5, width=5)\n",
    "\n",
    "# Render the initial grid world\n",
    "print(\"Initial Grid World:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0dcaab",
   "metadata": {},
   "source": [
    "## 3. Implement Q-Learning Algorithm\n",
    "\n",
    "Now we'll implement the Q-learning algorithm that will allow our agent to learn optimal policies for navigating the grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay=0.995):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_rate\n",
    "        self.min_epsilon = min_exploration_rate\n",
    "        self.epsilon_decay = exploration_decay\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        # Store episode data for visualization\n",
    "        self.episode_data = []\n",
    "    \n",
    "    def choose_action(self, state, row_col=True):\n",
    "        # Convert row, col state to flat index if needed\n",
    "        if row_col:\n",
    "            state_idx = state[0] * 5 + state[1]  # Assuming 5x5 grid\n",
    "        else:\n",
    "            state_idx = state\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            action = random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            # Exploit: choose the best action based on Q-values\n",
    "            action = np.argmax(self.q_table[state_idx])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done, row_col=True):\n",
    "        # Convert states to flat indices if needed\n",
    "        if row_col:\n",
    "            state_idx = state[0] * 5 + state[1]  # Assuming 5x5 grid\n",
    "            next_state_idx = next_state[0] * 5 + next_state[1]\n",
    "        else:\n",
    "            state_idx = state\n",
    "            next_state_idx = next_state\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state_idx, action]\n",
    "        \n",
    "        # Q-learning update rule\n",
    "        if done:\n",
    "            # If terminal state, there is no future reward\n",
    "            target_q = reward\n",
    "        else:\n",
    "            # Calculate target using Bellman equation\n",
    "            max_next_q = np.max(self.q_table[next_state_idx])\n",
    "            target_q = reward + self.gamma * max_next_q\n",
    "        \n",
    "        # Update Q-value with learning rate\n",
    "        self.q_table[state_idx, action] += self.lr * (target_q - current_q)\n",
    "        \n",
    "        # Store update data for visualization\n",
    "        update_data = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state,\n",
    "            'current_q': current_q,\n",
    "            'target_q': target_q,\n",
    "            'new_q': self.q_table[state_idx, action]\n",
    "        }\n",
    "        self.episode_data.append(update_data)\n",
    "    \n",
    "    def decay_exploration(self):\n",
    "        # Decay exploration rate after each episode\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def reset_episode_data(self):\n",
    "        self.episode_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc79738",
   "metadata": {},
   "source": [
    "## 4. Train the Agent and Visualize Movement\n",
    "\n",
    "Now we'll train our Q-learning agent and observe its movement in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ffdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and agent\n",
    "env = GridWorldEnv(height=5, width=5)\n",
    "agent = QLearningAgent(n_states=env.n_states, n_actions=env.n_actions)\n",
    "\n",
    "# Training parameters\n",
    "n_episodes = 500\n",
    "max_steps = 100\n",
    "\n",
    "# Lists to store metrics\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "all_episode_histories = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    "    # Reset environment and agent episode data\n",
    "    state = env.reset()\n",
    "    agent.reset_episode_data()\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    # Episode loop\n",
    "    while not done and steps < max_steps:\n",
    "        # Choose an action\n",
    "        action = agent.choose_action(state)\n",
    "        \n",
    "        # Take the action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # Update Q-values\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update state and counters\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    # Decay exploration rate\n",
    "    agent.decay_exploration()\n",
    "    \n",
    "    # Store episode metrics\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_steps.append(steps)\n",
    "    all_episode_histories.append(env.history.copy())\n",
    "    \n",
    "    # Print progress occasionally\n",
    "    if episode % 50 == 0 or episode == n_episodes - 1:\n",
    "        print(f\"Episode {episode}: Reward = {total_reward}, Steps = {steps}, Epsilon = {agent.epsilon:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfa53f",
   "metadata": {},
   "source": [
    "## 5. Visualize Learning Progress\n",
    "\n",
    "Let's visualize how the agent's performance improved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35472294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward and steps per episode\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(episode_rewards, 'b-')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Rewards per Episode')\n",
    "# Add smoothed line for trend\n",
    "window_size = 50\n",
    "if n_episodes >= window_size:\n",
    "    smoothed_rewards = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    ax1.plot(range(window_size-1, len(episode_rewards)), smoothed_rewards, 'r-', linewidth=2)\n",
    "    ax1.legend(['Rewards', 'Moving Average'])\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(episode_steps, 'g-')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Steps')\n",
    "ax2.set_title('Steps per Episode')\n",
    "# Add smoothed line for trend\n",
    "if n_episodes >= window_size:\n",
    "    smoothed_steps = np.convolve(episode_steps, np.ones(window_size)/window_size, mode='valid')\n",
    "    ax2.plot(range(window_size-1, len(episode_steps)), smoothed_steps, 'r-', linewidth=2)\n",
    "    ax2.legend(['Steps', 'Moving Average'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc4db5",
   "metadata": {},
   "source": [
    "## 6. Visualize the Final Policy\n",
    "\n",
    "Let's visualize the learned policy using arrows to indicate the best action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a225178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and render with the learned Q-table\n",
    "env.reset()\n",
    "print(\"Final Policy with Q-values for All Actions:\")\n",
    "env.render(q_table=agent.q_table)\n",
    "\n",
    "# Print the Q-table in a readable format\n",
    "print(\"\\nQ-Table:\")\n",
    "for i in range(env.height):\n",
    "    for j in range(env.width):\n",
    "        state_idx = i * env.width + j\n",
    "        print(f\"State ({i},{j}): {agent.q_table[state_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e87315",
   "metadata": {},
   "source": [
    "## 7. Detailed Movement Visualization\n",
    "\n",
    "Now let's observe the agent's movement in detail by running a test episode and visualizing each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ef769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test episode with no exploration (fully exploit the learned policy)\n",
    "original_epsilon = agent.epsilon\n",
    "agent.epsilon = 0  # No exploration, just follow the learned policy\n",
    "\n",
    "# Reset environment\n",
    "state = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Starting a test episode...\")\n",
    "\n",
    "# Run episode\n",
    "while not done and steps < max_steps:\n",
    "    # Choose action based on learned policy\n",
    "    action = agent.choose_action(state)\n",
    "    \n",
    "    # Take action\n",
    "    next_state, reward, done = env.step(action)\n",
    "    \n",
    "    # Display the action and current state\n",
    "    action_names = [\"Up\", \"Right\", \"Down\", \"Left\"]\n",
    "    print(f\"Step {steps+1}: At state {state}, taking action {action_names[action]}, moved to {next_state}, reward: {reward}\")\n",
    "    \n",
    "    # Visualize\n",
    "    env.render(q_table=agent.q_table, show_history=True)\n",
    "    \n",
    "    # Update state and counters\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "print(f\"\\nTest episode complete! Total steps: {steps}, Total reward: {total_reward}\")\n",
    "\n",
    "# Restore original epsilon\n",
    "agent.epsilon = original_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213326b",
   "metadata": {},
   "source": [
    "## 8. Agent Movement Animation\n",
    "\n",
    "Let's create an animation of the agent's movement to better visualize its journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll animate the most recent test episode\n",
    "animation_result = env.animate_episode(env.history)\n",
    "animation_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352ae52",
   "metadata": {},
   "source": [
    "## 9. Compare Different Learning Phases\n",
    "\n",
    "Let's compare the agent's movement and decision-making during different phases of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca995815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare early, mid, and late learning episodes\n",
    "episodes_to_compare = [0, n_episodes // 2, n_episodes - 1]\n",
    "labels = ['Early Learning', 'Mid Learning', 'Final Policy']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, (ep_idx, label) in enumerate(zip(episodes_to_compare, labels)):\n",
    "    ax = axes[i]\n",
    "    history = all_episode_histories[ep_idx]\n",
    "    \n",
    "    # Draw the grid\n",
    "    cmap = mcolors.ListedColormap(['white', 'red', 'green'])\n",
    "    bounds = [-2, -0.5, 0.5, 11]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    ax.imshow(env.grid, cmap=cmap, norm=norm)\n",
    "    \n",
    "    # Draw grid lines\n",
    "    for j in range(env.width + 1):\n",
    "        ax.axvline(j - 0.5, color='black', linewidth=1)\n",
    "    for j in range(env.height + 1):\n",
    "        ax.axhline(j - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Mark obstacles and goal\n",
    "    for obs in env.obstacles:\n",
    "        ax.add_patch(Rectangle((obs[1] - 0.5, obs[0] - 0.5), 1, 1, fill=True, color='red', alpha=0.5))\n",
    "    ax.add_patch(Rectangle((env.goal_state[1] - 0.5, env.goal_state[0] - 0.5), 1, 1, fill=True, color='green', alpha=0.5))\n",
    "    \n",
    "    # Show path\n",
    "    if len(history) > 1:\n",
    "        path_y = [state[0] for state in history]\n",
    "        path_x = [state[1] for state in history]\n",
    "        ax.plot(path_x, path_y, 'o-', color='purple', markersize=5, alpha=0.6)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xticks(np.arange(env.width))\n",
    "    ax.set_yticks(np.arange(env.height))\n",
    "    ax.set_xticklabels(np.arange(env.width))\n",
    "    ax.set_yticklabels(np.arange(env.height))\n",
    "    ax.set_title(f\"{label}\\nEpisode {ep_idx} - {len(history)} steps\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a97d19",
   "metadata": {},
   "source": [
    "## 10. Analyze Q-Values in Detail\n",
    "\n",
    "Let's examine the Q-values for specific states to understand the agent's decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18241dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_state_qvalues(state, q_table, env):\n",
    "    # Convert state to index\n",
    "    row, col = state\n",
    "    state_idx = row * env.width + col\n",
    "    q_values = q_table[state_idx]\n",
    "    \n",
    "    # Action names\n",
    "    action_names = [\"Up\", \"Right\", \"Down\", \"Left\"]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Bar chart of Q-values\n",
    "    bars = ax1.bar(action_names, q_values, color='skyblue')\n",
    "    ax1.set_ylabel('Q-Value')\n",
    "    ax1.set_title(f'Q-Values for State ({row}, {col})')\n",
    "    \n",
    "    # Highlight the best action\n",
    "    best_action = np.argmax(q_values)\n",
    "    bars[best_action].set_color('green')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Visualization of state in grid\n",
    "    cmap = mcolors.ListedColormap(['white', 'red', 'green'])\n",
    "    bounds = [-2, -0.5, 0.5, 11]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    ax2.imshow(env.grid, cmap=cmap, norm=norm)\n",
    "    \n",
    "    # Draw grid lines\n",
    "    for i in range(env.width + 1):\n",
    "        ax2.axvline(i - 0.5, color='black', linewidth=1)\n",
    "    for i in range(env.height + 1):\n",
    "        ax2.axhline(i - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Mark obstacles and goal\n",
    "    for obs in env.obstacles:\n",
    "        ax2.add_patch(Rectangle((obs[1] - 0.5, obs[0] - 0.5), 1, 1, fill=True, color='red', alpha=0.5))\n",
    "    ax2.add_patch(Rectangle((env.goal_state[1] - 0.5, env.goal_state[0] - 0.5), 1, 1, fill=True, color='green', alpha=0.5))\n",
    "    \n",
    "    # Highlight current state\n",
    "    ax2.add_patch(Rectangle((col - 0.5, row - 0.5), 1, 1, fill=True, color='blue', alpha=0.3))\n",
    "    \n",
    "    # Draw arrow for best action\n",
    "    dx, dy = env.actions[best_action]\n",
    "    # Flip for plotting\n",
    "    dx, dy = dy, -dx\n",
    "    ax2.arrow(col, row, dx * 0.5, dy * 0.5, head_width=0.2, head_length=0.2, fc='black', ec='black', linewidth=2)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax2.set_xticks(np.arange(env.width))\n",
    "    ax2.set_yticks(np.arange(env.height))\n",
    "    ax2.set_xticklabels(np.arange(env.width))\n",
    "    ax2.set_yticklabels(np.arange(env.height))\n",
    "    ax2.set_title(f'Best Action for State ({row}, {col}): {action_names[best_action]}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze Q-values for specific states\n",
    "key_states = [(0, 0), (2, 2), (3, 4), (4, 3)]\n",
    "for state in key_states:\n",
    "    visualize_state_qvalues(state, agent.q_table, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee3307",
   "metadata": {},
   "source": [
    "## 11. Decision-Making Breakdown\n",
    "\n",
    "Let's analyze a single step of the agent's decision-making process in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_decision(state, q_table, env):\n",
    "    row, col = state\n",
    "    state_idx = row * env.width + col\n",
    "    q_values = q_table[state_idx]\n",
    "    action_names = [\"Up\", \"Right\", \"Down\", \"Left\"]\n",
    "    \n",
    "    print(f\"Decision Analysis for State ({row}, {col}):\\n\")\n",
    "    print(\"Q-Values by Action:\")\n",
    "    for i, (action, q_val) in enumerate(zip(action_names, q_values)):\n",
    "        # Calculate where this action would lead\n",
    "        dx, dy = env.actions[i]\n",
    "        new_row, new_col = row + dx, col + dy\n",
    "        \n",
    "        # Check if the action leads to a valid state\n",
    "        if 0 <= new_row < env.height and 0 <= new_col < env.width:\n",
    "            new_state = (new_row, new_col)\n",
    "            # Determine what's in that state\n",
    "            if new_state in env.obstacles:\n",
    "                result = \"obstacle (invalid move)\"\n",
    "            elif new_state == env.goal_state:\n",
    "                result = \"goal state\"\n",
    "            else:\n",
    "                result = \"valid space\"\n",
    "        else:\n",
    "            result = \"out of bounds (invalid move)\"\n",
    "            \n",
    "        print(f\"  - {action}: {q_val:.4f} → leads to ({new_row}, {new_col}): {result}\")\n",
    "    \n",
    "    # Best action\n",
    "    best_action_idx = np.argmax(q_values)\n",
    "    best_action = action_names[best_action_idx]\n",
    "    print(f\"\\nBest Action: {best_action} with Q-value of {q_values[best_action_idx]:.4f}\")\n",
    "    \n",
    "    # Expected future rewards\n",
    "    dx, dy = env.actions[best_action_idx]\n",
    "    new_row, new_col = row + dx, col + dy\n",
    "    if 0 <= new_row < env.height and 0 <= new_col < env.width:\n",
    "        new_state_idx = new_row * env.width + new_col\n",
    "        future_reward = np.max(q_table[new_state_idx])\n",
    "        print(f\"Expected best future reward from next state: {future_reward:.4f}\")\n",
    "    \n",
    "    # Decision quality\n",
    "    q_range = np.max(q_values) - np.min(q_values)\n",
    "    if q_range > 5:\n",
    "        confidence = \"Very confident (large difference between best and worst actions)\"\n",
    "    elif q_range > 2:\n",
    "        confidence = \"Moderately confident\"\n",
    "    elif q_range > 0.5:\n",
    "        confidence = \"Somewhat confident\"\n",
    "    else:\n",
    "        confidence = \"Not confident (small difference between actions)\"\n",
    "    \n",
    "    print(f\"Decision Confidence: {confidence}\")\n",
    "\n",
    "# Analyze a few interesting states\n",
    "states_to_explain = [(0, 0), (2, 2), (4, 3)]\n",
    "    explain_decision(state, agent.q_table, env)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19050157",
   "metadata": {},
   "source": [
    "## 12. Experiment with Different Parameters\n",
    "\n",
    "Let's examine how different parameters affect the agent's learning and movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ce7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_learning_parameters(learning_rates, discount_factors):\n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for gamma in discount_factors:\n",
    "            print(f\"Training with learning_rate={lr}, discount_factor={gamma}\")\n",
    "            \n",
    "            # Create environment and agent with these parameters\n",
    "            test_env = GridWorldEnv(height=5, width=5)\n",
    "            test_agent = QLearningAgent(n_states=test_env.n_states, n_actions=test_env.n_actions,\n",
    "                                        learning_rate=lr, discount_factor=gamma)\n",
    "            \n",
    "            # Training parameters\n",
    "            num_episodes = 300\n",
    "            max_episode_steps = 100\n",
    "            \n",
    "            # Training loop\n",
    "            episode_rewards = []\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                state = test_env.reset()\n",
    "                total_reward = 0\n",
    "                done = False\n",
    "                steps = 0\n",
    "                \n",
    "                while not done and steps < max_episode_steps:\n",
    "                    action = test_agent.choose_action(state)\n",
    "                    next_state, reward, done = test_env.step(action)\n",
    "                    test_agent.update(state, action, reward, next_state, done)\n",
    "                    \n",
    "                    state = next_state\n",
    "                    total_reward += reward\n",
    "                    steps += 1\n",
    "                \n",
    "                test_agent.decay_exploration()\n",
    "                episode_rewards.append(total_reward)\n",
    "            \n",
    "            # Store results\n",
    "            key = f\"lr={lr}, gamma={gamma}\"\n",
    "            results[key] = {\n",
    "                'rewards': episode_rewards,\n",
    "                'avg_last_100': np.mean(episode_rewards[-100:]),\n",
    "                'q_table': test_agent.q_table.copy()\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define parameter values to test\n",
    "learning_rates = [0.1, 0.5]\n",
    "discount_factors = [0.9, 0.99]\n",
    "\n",
    "# Run comparison\n",
    "parameter_results = compare_learning_parameters(learning_rates, discount_factors)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for key, data in parameter_results.items():\n",
    "    plt.plot(data['rewards'], label=key)\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Reward Comparison for Different Parameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Average reward over last 100 episodes:\")\n",
    "for key, data in parameter_results.items():\n",
    "    print(f\"{key}: {data['avg_last_100']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6530f",
   "metadata": {},
   "source": [
    "## 13. Conclusion\n",
    "\n",
    "In this notebook, we've implemented Q-learning and visualized the agent's movement in detail. Key observations include:\n",
    "\n",
    "1. The agent initially explores randomly but gradually learns the optimal policy\n",
    "2. The agent learns to avoid obstacles and find the shortest path to the goal\n",
    "3. Different parameters can significantly affect learning performance\n",
    "4. The Q-table provides insights into the agent's decision-making process\n",
    "\n",
    "By visualizing each step, we've gained a better understanding of how Q-learning works in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
