{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24fbfb72",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Reinforcement/theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7bd16",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "This notebook explores the fundamental concepts of Reinforcement Learning (RL), its theoretical foundations, mathematical principles, and real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a61df",
   "metadata": {},
   "source": [
    "## 1. Introduction to Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward.\n",
    "\n",
    "Unlike supervised learning, the agent is not explicitly told which actions to take but must discover which actions yield the highest reward through trial and error. This approach is inspired by behavioral psychology, where learning happens through interaction with the environment.\n",
    "\n",
    "Mathematically, RL is often formalized using Markov Decision Processes (MDPs), which provide a framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111fec4",
   "metadata": {},
   "source": [
    "### 1.1 Markov Decision Process (MDP)\n",
    "\n",
    "The mathematical foundation of reinforcement learning is the Markov Decision Process (MDP), defined by a 5-tuple (S, A, P, R, γ):\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph MDP\n",
    "    S[States S] --> P[Transition Probabilities P]\n",
    "    A[Actions A] --> P\n",
    "    P --> S'[Next States S']\n",
    "    P --> R[Rewards R]\n",
    "    R --> γ[Discount Factor γ]\n",
    "    end\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **S**: Set of possible states\n",
    "- **A**: Set of possible actions\n",
    "- **P**: State transition probability function P(s'|s,a)\n",
    "- **R**: Reward function R(s,a,s')\n",
    "- **γ**: Discount factor determining the importance of future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25081f3a",
   "metadata": {},
   "source": [
    "## 2. Key Components of Reinforcement Learning\n",
    "\n",
    "| Component | Description |\n",
    "| --- | --- |\n",
    "| Agent | The learner or decision maker |\n",
    "| Environment | Everything the agent interacts with |\n",
    "| State (s) | Current situation of the agent |\n",
    "| Action (a) | Moves the agent can make |\n",
    "| Reward (r) | Feedback from the environment |\n",
    "| Policy (π) | Strategy that the agent employs to determine next action |\n",
    "| Value Function (V(s) or Q(s,a)) | Prediction of future rewards |\n",
    "| Model | Agent's representation of the environment |\n",
    "| State Transition Probability | P(s'|s,a) - probability of moving to state s' from state s taking action a |\n",
    "| Discount Factor (γ) | Parameter between 0 and 1 that determines the importance of future rewards |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0c4b4",
   "metadata": {},
   "source": [
    "## 3. The RL Process\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Agent] --Action--> B[Environment]\n",
    "    B --State--> A\n",
    "    B --Reward--> A\n",
    "```\n",
    "\n",
    "The cycle of agent-environment interaction forms the foundation of reinforcement learning. At each time step t:\n",
    "\n",
    "1. The agent receives state s<sub>t</sub> from the environment\n",
    "2. Based on s<sub>t</sub>, the agent selects an action a<sub>t</sub>\n",
    "3. The environment transitions to a new state s<sub>t+1</sub>\n",
    "4. The environment provides a reward r<sub>t+1</sub>\n",
    "5. The agent uses this experience tuple (s<sub>t</sub>, a<sub>t</sub>, r<sub>t+1</sub>, s<sub>t+1</sub>) to improve its policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35725e4f",
   "metadata": {},
   "source": [
    "## 4. Types of Reinforcement Learning Algorithms\n",
    "\n",
    "| Algorithm Type | Description | Examples | Key Equations |\n",
    "| --- | --- | --- | --- |\n",
    "| Value-Based | Learn the value of being in a given state or state-action pair | Q-Learning, SARSA | Q(s,a) ← Q(s,a) + α[r + γ·max<sub>a'</sub>Q(s',a') - Q(s,a)] |\n",
    "| Policy-Based | Directly learn the policy mapping from states to actions | REINFORCE, Policy Gradients | ∇J(θ) = E<sub>π</sub>[∇log π<sub>θ</sub>(a\\|s)·Q<sub>π</sub>(s,a)] |\n",
    "| Model-Based | Build a model of the environment and plan using it | Dyna-Q, AlphaZero | Uses P(s'\\|s,a) and R(s,a,s') to plan |\n",
    "| Actor-Critic | Combine value-based and policy-based approaches | A2C, A3C, PPO | Uses both policy (actor) and value function (critic) |\n",
    "| Deep RL | Use deep neural networks as function approximators | DQN, DDPG, TD3 | Applies deep learning to approximate Q-values or policies |\n",
    "| Multi-Agent RL | Multiple agents learning simultaneously | MADDPG, QMIX | Considers interactions between agents |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea158d62",
   "metadata": {},
   "source": [
    "## 5. Q-Learning: Theoretical Foundation\n",
    "\n",
    "Q-Learning is a value-based reinforcement learning algorithm that learns the value of an action in a particular state. It's an off-policy temporal difference learning algorithm.\n",
    "\n",
    "### The Q-Learning Algorithm:\n",
    "\n",
    "1. Initialize Q(s,a) arbitrarily for all state-action pairs\n",
    "2. For each episode:\n",
    "   - Initialize state s\n",
    "   - For each step of the episode:\n",
    "     - Choose action a from s using policy derived from Q (e.g., ε-greedy)\n",
    "     - Take action a, observe reward r and next state s'\n",
    "     - Update Q-value: Q(s,a) ← Q(s,a) + α[r + γ·max<sub>a'</sub>Q(s',a') - Q(s,a)]\n",
    "     - s ← s'\n",
    "   - Until s is terminal\n",
    "\n",
    "### Convergence Properties:\n",
    "\n",
    "Q-Learning converges to the optimal action-value function Q* as long as:\n",
    "- All state-action pairs are visited infinitely often\n",
    "- The learning rate α decays appropriately\n",
    "- The Markov property holds for the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae532108",
   "metadata": {},
   "source": [
    "### 5.1 Q-Learning in Action\n",
    "\n",
    "The following diagram illustrates how Q-learning updates its Q-values in a grid world environment:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Start: Agent in state s] --> B[Choose action a using ε-greedy]\n",
    "    B --> C[Take action a, observe reward r and new state s']\n",
    "    C --> D[\"Calculate TD Error: r + γ·max Q[s',a'] - Q[s,a]\"]\n",
    "    D --> E[\"Update Q-value: Q[s,a] += α × TD Error\"]\n",
    "    E --> F[s = s']\n",
    "    F --> G{Terminal state?}\n",
    "    G -->|No| B\n",
    "    G -->|Yes| H[End episode]\n",
    "```\n",
    "\n",
    "#### Q-Table Visualization Example:\n",
    "\n",
    "As learning progresses, the Q-table evolves from random values to meaningful action values:\n",
    "\n",
    "|  | Left | Right | Up | Down |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| **State 1** | 0.0 | 0.2 | 0.0 | 0.0 |\n",
    "| **State 2** | 0.1 | 0.0 | 0.5 | 0.0 |\n",
    "| **State 3** | 0.0 | 0.0 | 0.1 | 0.8 |\n",
    "| **State 4** | 0.3 | 0.6 | 0.0 | 0.0 |\n",
    "\n",
    "Over time, these values guide the agent toward optimal behavior as it learns which actions maximize long-term rewards in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525c845",
   "metadata": {},
   "source": [
    "## 6. Policy Representation and Evaluation\n",
    "\n",
    "A policy π defines the agent's behavior by mapping states to a probability distribution over actions. There are two main ways to represent policies:\n",
    "\n",
    "### Deterministic Policies\n",
    "- Maps each state to a specific action: π(s) = a\n",
    "- Example: Always move right in state A\n",
    "\n",
    "### Stochastic Policies\n",
    "- Maps each state to a probability distribution over actions: π(a|s) = P(A<sub>t</sub>=a|S<sub>t</sub>=s)\n",
    "- Example: In state A, move right with 80% probability and move left with 20% probability\n",
    "\n",
    "### Policy Evaluation Methods:\n",
    "\n",
    "1. **Monte Carlo evaluation**: Estimate value functions by averaging returns from complete episodes\n",
    "2. **Temporal Difference evaluation**: Update value estimates based on other value estimates (bootstrapping)\n",
    "3. **Dynamic Programming**: Iteratively apply the Bellman equation to compute value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af9702",
   "metadata": {},
   "source": [
    "## 7. Challenges in Reinforcement Learning\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Reinforcement Learning Challenges] --> B[Exploration vs. Exploitation]\n",
    "    A --> C[Credit Assignment Problem]\n",
    "    A --> D[Sample Efficiency]\n",
    "    A --> E[Generalization]\n",
    "    A --> F[Stability and Convergence]\n",
    "    A --> G[Partial Observability]\n",
    "    A --> H[Reward Design]\n",
    "    \n",
    "    B --> B1[Balancing new actions vs. exploiting known rewards]\n",
    "    C --> C1[Determining which actions led to rewards/penalties]\n",
    "    D --> D1[Learning from limited environment interactions]\n",
    "    E --> E1[Transferring knowledge to new situations]\n",
    "    F --> F1[Ensuring consistent improvement during learning]\n",
    "    G --> G1[Dealing with incomplete state information]\n",
    "    H --> H1[Creating reward functions that induce desired behavior]\n",
    "```\n",
    "\n",
    "### Mathematical Formulations of Key Challenges:\n",
    "\n",
    "- **Exploration vs. Exploitation**: Formalized through multi-armed bandit problems and ε-greedy, UCB, or Thompson sampling approaches\n",
    "- **Credit Assignment**: Addressed through eligibility traces and TD(λ) methods\n",
    "- **Partial Observability**: Modeled with POMDPs (Partially Observable Markov Decision Processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21bd682",
   "metadata": {},
   "source": [
    "## 7.2 Dynamic Programming in Reinforcement Learning\n",
    "\n",
    "Dynamic Programming (DP) methods are foundational algorithms in RL that solve MDPs when the model of the environment is completely known.\n",
    "\n",
    "### Key Characteristics of DP:\n",
    "\n",
    "- Requires complete knowledge of the environment (transition probabilities P(s'|s,a) and reward function R(s,a,s'))\n",
    "- Uses the Bellman equations to iteratively update value functions\n",
    "- Guarantees convergence to optimal policies\n",
    "- Computationally intensive for large state spaces\n",
    "\n",
    "### Core DP Algorithms:\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Dynamic Programming Algorithms\"\n",
    "    A[Policy Evaluation] --> B[Policy Improvement]\n",
    "    B --> C[Policy Iteration]\n",
    "    D[Value Iteration] --> E[Optimality]\n",
    "    C --> E\n",
    "    end\n",
    "```\n",
    "\n",
    "#### 1. Policy Evaluation\n",
    "\n",
    "Computes the state-value function V<sub>π</sub> for a given policy π:\n",
    "\n",
    "For each state s:\n",
    "- Initialize V(s) arbitrarily\n",
    "- Repeat until convergence:\n",
    "  - For each state s:\n",
    "    - V<sub>π</sub>(s) ← ∑<sub>a</sub> π(a|s) ∑<sub>s',r</sub> p(s',r|s,a)[r + γV<sub>π</sub>(s')]\n",
    "\n",
    "#### 2. Policy Improvement\n",
    "\n",
    "Generates a better policy π' from the current policy π using the value function V<sub>π</sub>:\n",
    "\n",
    "For each state s:\n",
    "- π'(s) ← argmax<sub>a</sub> ∑<sub>s',r</sub> p(s',r|s,a)[r + γV<sub>π</sub>(s')]\n",
    "\n",
    "#### 3. Policy Iteration\n",
    "\n",
    "Alternates between policy evaluation and improvement until convergence:\n",
    "1. Initialize policy π arbitrarily\n",
    "2. **Policy Evaluation**: Compute V<sub>π</sub>\n",
    "3. **Policy Improvement**: Generate improved policy π'\n",
    "4. If π' = π, stop; else π ← π' and go to step 2\n",
    "\n",
    "#### 4. Value Iteration\n",
    "\n",
    "Combines policy evaluation and improvement in a single update:\n",
    "\n",
    "For each state s:\n",
    "- Initialize V(s) arbitrarily\n",
    "- Repeat until convergence:\n",
    "  - For each state s:\n",
    "    - V(s) ← max<sub>a</sub> ∑<sub>s',r</sub> p(s',r|s,a)[r + γV(s')]\n",
    "\n",
    "### Limitations of DP:\n",
    "\n",
    "- Requires complete model of the environment (rarely available in practice)\n",
    "- Suffers from the \"curse of dimensionality\" - computational complexity grows exponentially with state space size\n",
    "- Not suitable for continuous state or action spaces without approximation\n",
    "- Cannot handle stochastic environments with unknown dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ee5aa",
   "metadata": {},
   "source": [
    "## 7.3 Monte Carlo Methods in Reinforcement Learning\n",
    "\n",
    "Monte Carlo (MC) methods learn directly from complete episodes of experience without requiring prior knowledge of environment dynamics.\n",
    "\n",
    "### Key Characteristics of MC:\n",
    "\n",
    "- Model-free: No need for transition probabilities or reward function\n",
    "- Learn from complete episodes (requires episodes to terminate)\n",
    "- Update estimates only after complete episodes\n",
    "- Can handle unknown environments\n",
    "- High variance but unbiased estimates\n",
    "\n",
    "### Monte Carlo Prediction (Policy Evaluation)\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Generate episode following policy π] --> B[For each state s in episode]\n",
    "    B --> C[Calculate return G following first visit to s]\n",
    "    C --> D[\"Update value estimate V(s) by averaging returns\"]\n",
    "    D --> E[More episodes?]\n",
    "    E -->|Yes| A\n",
    "    E -->|No| F[Final value function V]\n",
    "```\n",
    "\n",
    "#### First-Visit MC:\n",
    "- For each state s, V(s) is the average of returns following the **first** visit to s in each episode\n",
    "\n",
    "#### Every-Visit MC:\n",
    "- For each state s, V(s) is the average of returns following **all** visits to s in each episode\n",
    "\n",
    "### Monte Carlo Control\n",
    "\n",
    "MC methods can be used for control (finding optimal policies) using the following approaches:\n",
    "\n",
    "#### Monte Carlo Exploring Starts (ES):\n",
    "1. Initialize policy π and action-value function Q arbitrarily\n",
    "2. Generate episode with exploring starts (all state-action pairs have non-zero probability)\n",
    "3. For each state-action pair (s,a) in the episode:\n",
    "   - Estimate Q(s,a) by averaging returns\n",
    "4. Improve policy: π(s) ← argmax<sub>a</sub> Q(s,a)\n",
    "5. Repeat steps 2-4\n",
    "\n",
    "#### Monte Carlo with ε-greedy exploration:\n",
    "- Similar to ES, but uses ε-greedy policy to ensure exploration\n",
    "- With probability ε, choose a random action\n",
    "- With probability 1-ε, choose the greedy action a = argmax<sub>a</sub> Q(s,a)\n",
    "\n",
    "### Advantages of Monte Carlo Methods:\n",
    "\n",
    "- Can learn directly from experience without a model\n",
    "- Can focus learning on relevant states actually visited\n",
    "- Less affected by Markov property violations\n",
    "- Can be more efficient in large state spaces where approximation is necessary\n",
    "\n",
    "### Limitations of Monte Carlo Methods:\n",
    "\n",
    "- Requires complete episodes (not suitable for continuing tasks)\n",
    "- Updates only occur after complete episodes (slower learning)\n",
    "- High variance in returns leads to slower convergence\n",
    "- Struggles with exploration in deterministic environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a816560",
   "metadata": {},
   "source": [
    "## 7.5 Temporal Difference Learning Methods\n",
    "\n",
    "Temporal Difference (TD) learning combines ideas from both Dynamic Programming and Monte Carlo methods, learning from incomplete episodes by bootstrapping.\n",
    "\n",
    "### Key Characteristics of TD Learning:\n",
    "\n",
    "- Model-free: No need for transition probabilities or reward function\n",
    "- Online learning: Updates estimates at each time step (no need to wait for episode completion)\n",
    "- Bootstrapping: Updates are based on existing estimates rather than complete returns\n",
    "- Balance of bias and variance: Less variance than MC but introduces some bias\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Observe state s\"] --> B[\"Select action a (e.g., ε-greedy)\"]\n",
    "    B --> C[\"Execute action a, observe reward r and next state s'\"]\n",
    "    C --> D[\"Update value estimate using TD error\"]\n",
    "    D --> E[\"s ← s'\"]\n",
    "    E --> F{\"Terminal state?\"}\n",
    "    F -->|No| B\n",
    "    F -->|Yes| G[\"Start new episode\"]\n",
    "    G --> A\n",
    "```\n",
    "\n",
    "### TD Learning Variants:\n",
    "\n",
    "#### SARSA (On-policy TD):\n",
    "- Update: Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]\n",
    "- Where a' is selected according to policy derived from Q (typically ε-greedy)\n",
    "- On-policy: Learns value of policy being followed (including exploration)\n",
    "\n",
    "#### Q-Learning (Off-policy TD):\n",
    "- Update: Q(s,a) ← Q(s,a) + α[r + γmax<sub>a'</sub>Q(s',a') - Q(s,a)]\n",
    "- Off-policy: Learns optimal policy regardless of actions actually taken\n",
    "- More sample efficient but can be less stable\n",
    "\n",
    "#### Expected SARSA:\n",
    "- Update: Q(s,a) ← Q(s,a) + α[r + γ∑<sub>a'</sub>π(a'|s')Q(s',a') - Q(s,a)]\n",
    "- Reduces variance compared to SARSA\n",
    "- Computationally more expensive\n",
    "\n",
    "#### Double Q-Learning:\n",
    "- Maintains two value functions Q<sub>1</sub> and Q<sub>2</sub>\n",
    "- Updates: Q<sub>1</sub>(s,a) ← Q<sub>1</sub>(s,a) + α[r + γQ<sub>2</sub>(s',argmax<sub>a'</sub>Q<sub>1</sub>(s',a')) - Q<sub>1</sub>(s,a)]\n",
    "- Reduces maximization bias in Q-learning\n",
    "\n",
    "### TD(λ) - Eligibility Traces:\n",
    "\n",
    "TD(λ) bridges the gap between TD and Monte Carlo methods using eligibility traces:\n",
    "\n",
    "- Eligibility trace e(s) tracks how recently and frequently states were visited\n",
    "- Updates all state values proportional to their eligibility: V(s) ← V(s) + αδe(s)\n",
    "- Where δ is the TD error: r + γV(s') - V(s)\n",
    "- λ=0 corresponds to TD(0), λ=1 approximates Monte Carlo\n",
    "\n",
    "### Advantages of TD Learning:\n",
    "\n",
    "- Can learn online after every step (without waiting for episode end)\n",
    "- Typically converges faster than Monte Carlo methods\n",
    "- Works in continuing (non-episodic) tasks\n",
    "- Lower variance than Monte Carlo methods\n",
    "\n",
    "### Limitations of TD Learning:\n",
    "\n",
    "- Introduces bias due to bootstrapping\n",
    "- More sensitive to initial value functions\n",
    "- Function approximation can lead to instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747d1b9",
   "metadata": {},
   "source": [
    "## 7.4 Comparison: DP vs. MC vs. TD Learning\n",
    "\n",
    "| Method | Knowledge Required | Update Timing | Bias/Variance | Bootstrapping | Sample Efficiency |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| **Dynamic Programming** | Complete model (P, R) | Sweep through states | No bias, low variance | Yes (bootstraps) | N/A (no samples) |\n",
    "| **Monte Carlo** | No model needed | End of episode | No bias, high variance | No bootstrapping | Low efficiency |\n",
    "| **Temporal Difference** | No model needed | Each time step | Some bias, lower variance | Yes (bootstraps) | Medium efficiency |\n",
    "\n",
    "### Visual Spectrum of Methods\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph \"Learning Methods Spectrum\"\n",
    "    A[\"Dynamic Programming\"] --> B[\"TD Learning\"] --> C[\"Monte Carlo\"]\n",
    "    end\n",
    "```\n",
    "\n",
    "### Mathematical Formulation Comparison:\n",
    "\n",
    "**DP Update:**\n",
    "V(s) ← ∑<sub>a</sub> π(a|s) ∑<sub>s',r</sub> p(s',r|s,a)[r + γV(s')]\n",
    "\n",
    "**MC Update:**\n",
    "V(s) ← V(s) + α[G<sub>t</sub> - V(s)]\n",
    "   where G<sub>t</sub> is the actual return from time t\n",
    "\n",
    "**TD Update (TD(0)):**\n",
    "V(s) ← V(s) + α[r + γV(s') - V(s)]\n",
    "\n",
    "### Unified View: TD(λ)\n",
    "\n",
    "TD(λ) provides a unified view of these methods through eligibility traces:\n",
    "- TD(0): Pure temporal difference learning (λ = 0)\n",
    "- TD(1): Equivalent to Monte Carlo (λ = 1)\n",
    "- DP: Special case when model is known and λ = 0\n",
    "\n",
    "**TD(λ) Update:**\n",
    "V(s) ← V(s) + α[G<sub>t</sub><sup>λ</sup> - V(s)]\n",
    "\n",
    "where G<sub>t</sub><sup>λ</sup> is the λ-return, a weighted average of returns of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ca800",
   "metadata": {},
   "source": [
    "### 7.1 Visualization of Exploration vs. Exploitation Tradeoff\n",
    "\n",
    "The exploration-exploitation dilemma is one of the fundamental challenges in reinforcement learning.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph \"Exploration-Exploitation Spectrum\"\n",
    "    A[Pure Exploration] --> B[Balanced Approach] --> C[Pure Exploitation]\n",
    "    end\n",
    "```\n",
    "\n",
    "Common exploration strategies include:\n",
    "- **ε-greedy**: Choose random action with probability ε, best known action with probability 1-ε\n",
    "- **Softmax**: Choose actions with probability proportional to their estimated value\n",
    "- **UCB (Upper Confidence Bound)**: Balance exploitation with exploration bonus based on uncertainty\n",
    "- **Thompson Sampling**: Sample from posterior distribution of action values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54008b",
   "metadata": {},
   "source": [
    "## 8. Real-World Applications of Reinforcement Learning\n",
    "\n",
    "| Domain | Applications | Notable Examples | Key Challenges |\n",
    "| --- | --- | --- | --- |\n",
    "| Gaming | Game AI, strategy games | AlphaGo, OpenAI Five, MuZero | Large state spaces, complex rules |\n",
    "| Robotics | Robot navigation, manipulation, locomotion | Boston Dynamics robots, Fetch robots | Physical constraints, safety concerns |\n",
    "| Healthcare | Treatment recommendations, drug discovery | DeepMind's protein folding, adaptive clinical trials | Data limitations, ethical considerations |\n",
    "| Finance | Trading strategies, portfolio optimization | JP Morgan's LOXM, algorithmic trading systems | Market uncertainty, risk management |\n",
    "| Autonomous Vehicles | Self-driving cars, drones | Waymo, Tesla Autopilot, Skydio drones | Safety, handling rare events |\n",
    "| Resource Management | Power systems, data centers | DeepMind's data center cooling, smart grid optimization | System complexity, multiple objectives |\n",
    "| Recommendation Systems | Content recommendation, advertising | Netflix, YouTube algorithms, targeted ads | User preference shifts, feedback loops |\n",
    "| Natural Language Processing | Dialogue systems, text generation | Reinforcement Learning from Human Feedback (RLHF) | Reward design, alignment with human values |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b40bae",
   "metadata": {},
   "source": [
    "## 9. Comparison of RL with Other ML Paradigms\n",
    "\n",
    "| Aspect | Supervised Learning | Unsupervised Learning | Reinforcement Learning |\n",
    "| --- | --- | --- | --- |\n",
    "| Data Needed | Labeled data (input-output pairs) | Unlabeled data | Environment interaction trajectories |\n",
    "| Goal | Predict output from input | Find patterns/structure in data | Maximize cumulative reward |\n",
    "| Feedback | Immediate, direct (error signal) | None | Delayed, indirect (rewards) |\n",
    "| Common Tasks | Classification, Regression | Clustering, Dimensionality Reduction | Decision making, Control, Planning |\n",
    "| Example Algorithms | SVM, Neural Networks, Decision Trees | K-means, PCA, Autoencoders | Q-Learning, Policy Gradients, Actor-Critic |\n",
    "| Mathematical Framework | Statistical learning theory | Information theory, statistics | Markov Decision Processes |\n",
    "| Evaluation Metric | Accuracy, RMSE, F1-score | Reconstruction error, silhouette score | Cumulative reward, success rate |\n",
    "| Sample Efficiency | Generally high | Moderate | Generally low |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3b458",
   "metadata": {},
   "source": [
    "## 10. Further Resources\n",
    "\n",
    "1. **Books**:\n",
    "   - \"Reinforcement Learning: An Introduction\" by Sutton and Barto\n",
    "   - \"Deep Reinforcement Learning Hands-On\" by Maxim Lapan\n",
    "   - \"Algorithms for Reinforcement Learning\" by Csaba Szepesvári\n",
    "   - \"Deep Reinforcement Learning\" by Aske Plaat\n",
    "\n",
    "2. **Online Courses**:\n",
    "   - David Silver's RL Course (DeepMind)\n",
    "   - CS285 Deep Reinforcement Learning (UC Berkeley)\n",
    "   - CS234 Reinforcement Learning (Stanford)\n",
    "   - Practical RL (Higher School of Economics)\n",
    "\n",
    "3. **Key Research Papers**:\n",
    "   - \"Playing Atari with Deep Reinforcement Learning\" (Mnih et al., 2013)\n",
    "   - \"Human-level control through deep reinforcement learning\" (Mnih et al., 2015)\n",
    "   - \"Mastering the game of Go with deep neural networks and tree search\" (Silver et al., 2016)\n",
    "   - \"Proximal Policy Optimization Algorithms\" (Schulman et al., 2017)\n",
    "\n",
    "4. **Theoretical Foundations**:\n",
    "   - Markov Decision Processes (MDPs)\n",
    "   - Dynamic Programming\n",
    "   - Bellman Equations\n",
    "   - Policy Gradient Theorems\n",
    "\n",
    "5. **Mathematical Prerequisites**:\n",
    "   - Probability and Statistics\n",
    "   - Linear Algebra\n",
    "   - Calculus (especially Gradient-based methods)\n",
    "   - Optimization Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090a1a2",
   "metadata": {},
   "source": [
    "## 11. Mathematical Foundations of RL\n",
    "\n",
    "### Bellman Equations\n",
    "\n",
    "The foundation of many RL algorithms lies in the Bellman equations:\n",
    "\n",
    "**Value Function Bellman Equation**:\n",
    "V<sub>π</sub>(s) = ∑<sub>a</sub> π(a|s) ∑<sub>s',r</sub> p(s',r|s,a)[r + γV<sub>π</sub>(s')]\n",
    "\n",
    "**Action-Value Function Bellman Equation**:\n",
    "Q<sub>π</sub>(s,a) = ∑<sub>s',r</sub> p(s',r|s,a)[r + γ∑<sub>a'</sub> π(a'|s')Q<sub>π</sub>(s',a')]\n",
    "\n",
    "**Optimal Bellman Equations**:\n",
    "V*(s) = max<sub>a</sub> ∑<sub>s',r</sub> p(s',r|s,a)[r + γV*(s')]\n",
    "Q*(s,a) = ∑<sub>s',r</sub> p(s',r|s,a)[r + γmax<sub>a'</sub> Q*(s',a')]\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "\n",
    "For any differentiable policy π<sub>θ</sub>:\n",
    "∇<sub>θ</sub>J(θ) ∝ ∑<sub>s</sub> d<sub>π</sub>(s) ∑<sub>a</sub> ∇<sub>θ</sub>π<sub>θ</sub>(a|s)Q<sub>π</sub>(s,a)\n",
    "\n",
    "where d<sub>π</sub>(s) is the stationary distribution of states under policy π."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c48cf5",
   "metadata": {},
   "source": [
    "## 12. Deep Reinforcement Learning\n",
    "\n",
    "Deep Reinforcement Learning combines deep neural networks with reinforcement learning principles to handle high-dimensional state spaces and complex environments.\n",
    "\n",
    "### 12.1 DRL Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph Environment\n",
    "    E[Current State] --> R[Reward Signal]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Deep RL Agent\"\n",
    "    E --> CNN[Convolutional Layers]\n",
    "    CNN --> FC[Fully Connected Layers]\n",
    "    FC --> V[Value Network]\n",
    "    FC --> P[Policy Network]\n",
    "    V --> D[Decision Making]\n",
    "    P --> D\n",
    "    end\n",
    "    \n",
    "    D --> A[Action]\n",
    "    A --> E\n",
    "    R --> V\n",
    "```\n",
    "\n",
    "### 12.2 Key Deep RL Algorithms\n",
    "\n",
    "| Algorithm | Description | Innovation |\n",
    "| --- | --- | --- |\n",
    "| DQN | Deep Q-Network | Experience replay and target networks |\n",
    "| A3C | Asynchronous Advantage Actor-Critic | Parallel actor-learners |\n",
    "| PPO | Proximal Policy Optimization | Clipped surrogate objective |\n",
    "| SAC | Soft Actor-Critic | Maximum entropy framework |\n",
    "| TD3 | Twin Delayed DDPG | Double Q-learning with delayed policy updates |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fac4fb",
   "metadata": {},
   "source": [
    "## 13. Practical RL Frameworks and Libraries\n",
    "\n",
    "Several libraries and frameworks have been developed to simplify the implementation of reinforcement learning algorithms:\n",
    "\n",
    "| Library | Focus | Key Features | Best For |\n",
    "| --- | --- | --- | --- |\n",
    "| **OpenAI Gym/Gymnasium** | Environments | Standardized environment interface, wide variety of domains | Getting started, testing algorithms |\n",
    "| **Stable Baselines3** | Algorithms | Reliable implementations, simple API, PyTorch-based | Production implementations |\n",
    "| **RLlib** | Distributed RL | Scalable, distributed training, multi-agent support | Large-scale training |\n",
    "| **TensorFlow-Agents** | Deep RL | TensorFlow integration, distributed training | TensorFlow users |\n",
    "| **Dopamine** | Research | Clean implementations of DQN, Rainbow, C51, IQN | Research experimentation |\n",
    "| **MushroomRL** | Comprehensive | Extensive algorithm implementations, classic to SOTA | Education, research |\n",
    "| **Tianshou** | Modular | Fast, modular design, vectorized environments | Advanced users |\n",
    "| **CleanRL** | Single-file | Single-file implementations, easy to understand | Learning algorithm details |\n",
    "\n",
    "### Example: Setting up an environment with Gymnasium\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create an environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reset the environment to get initial state\n",
    "observation, info = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    # Render the environment (in notebooks, use a different rendering approach)\n",
    "    env.render()\n",
    "    \n",
    "    # Select a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Take the action and observe the result\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Check if episode has ended\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "```\n",
    "\n",
    "### Common Workflow for RL Experimentation:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"Define Environment\"] --> B[\"Implement or Import Algorithm\"]\n",
    "    B --> C[\"Train Agent\"]\n",
    "    C --> D[\"Evaluate Performance\"]\n",
    "    D --> E[\"Tune Hyperparameters\"]\n",
    "    E --> C\n",
    "    D -->|\"Satisfactory\"| F[\"Deploy Model\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c906557",
   "metadata": {},
   "source": [
    "## 19. Implementing Policy Gradient Methods\n",
    "\n",
    "Policy gradient methods directly optimize the policy by performing gradient ascent on the expected return.\n",
    "\n",
    "### REINFORCE: The Basic Policy Gradient Algorithm\n",
    "\n",
    "The core idea of REINFORCE is to adjust the policy parameters θ in the direction that increases the probability of actions that lead to higher returns:\n",
    "\n",
    "∇<sub>θ</sub>J(θ) = E<sub>π</sub>[∇<sub>θ</sub>log π<sub>θ</sub>(a|s) · G<sub>t</sub>]\n",
    "\n",
    "Where:\n",
    "- J(θ) is the expected return starting from initial state\n",
    "- ∇<sub>θ</sub>log π<sub>θ</sub>(a|s) is the score function (gradient of log probability of taking action a in state s)\n",
    "- G<sub>t</sub> is the return (sum of discounted rewards) from time step t\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Initialize policy parameters θ\"] --> B[\"Generate episode using π_θ\"]\n",
    "    B --> C[\"Calculate returns G_t for each timestep\"]\n",
    "    C --> D[\"Update θ: θ = θ + α∇_θlog π_θ(a|s)G_t\"]\n",
    "    D --> E{\"Converged?\"}\n",
    "    E -->|No| B\n",
    "    E -->|Yes| F[\"Final policy π_θ\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa23f15",
   "metadata": {},
   "source": [
    "## 20. Exercises and Problems\n",
    "\n",
    "### Exercise 1: Bellman Equations\n",
    "Calculate the state values for the following grid world MDP: 3x3 grid with a +1 reward in the bottom-right and -1 reward in the middle. Use γ = 0.9 and assume a uniform random policy (equal probability of each action).\n",
    "\n",
    "### Exercise 2: Q-Learning Implementation\n",
    "Implement Q-learning for a simple 2x2 grid world where the top-right cell has a reward of +1, and all other transitions have a reward of -0.1. Compare the performance with different values of learning rate α and exploration parameter ε.\n",
    "\n",
    "### Exercise 3: Policy Evaluation\n",
    "Given a 4x4 grid world with obstacles at (1,1) and (2,2), a goal state at (3,3) with reward +1, and all other transitions having reward -0.1, evaluate the following deterministic policy using iterative policy evaluation:\n",
    "```\n",
    "π = [['right', 'right', 'right', 'down'],\n",
    "     ['up', 'none', 'right', 'down'],\n",
    "     ['up', 'right', 'none', 'down'],\n",
    "     ['up', 'right', 'right', 'done']]\n",
    "```\n",
    "\n",
    "### Exercise 4: Temporal Difference vs Monte Carlo\n",
    "Consider a simple 4-state Markov chain where one state gives a stochastic reward of either +1 or -1 with equal probability. Compare the estimates of the state values using TD(0) and Monte Carlo methods after 100 episodes. Which converges faster and why?\n",
    "\n",
    "### Exercise 5: Function Approximation\n",
    "For a continuous state space problem (e.g., CartPole), implement a linear function approximator for the value function. Use tile coding to create features, and compare the performance to a neural network approximator.\n",
    "\n",
    "### Challenge Problem: Multi-Agent Coordination\n",
    "Implement a simple multi-agent system where two agents need to coordinate to reach a common goal. Use independent Q-learning for each agent and observe if they can learn to coordinate without explicit communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4396291",
   "metadata": {},
   "source": [
    "## 18. Conclusion and Key Takeaways\n",
    "\n",
    "### Summary of Reinforcement Learning Concepts\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Reinforcement Learning\"] --> B[\"Foundations\"]\n",
    "    A --> C[\"Algorithms\"]\n",
    "    A --> D[\"Extensions\"]\n",
    "    A --> E[\"Applications\"]\n",
    "    A --> F[\"Challenges\"]\n",
    "    \n",
    "    B --> B1[\"MDP Framework\"]\n",
    "    B --> B2[\"Value Functions\"]\n",
    "    B --> B3[\"Policies\"]\n",
    "    \n",
    "    C --> C1[\"Value-Based: Q-Learning, SARSA\"]\n",
    "    C --> C2[\"Policy-Based: Policy Gradients\"]\n",
    "    C --> C3[\"Actor-Critic Methods\"]\n",
    "    C --> C4[\"Model-Based Methods\"]\n",
    "    \n",
    "    D --> D1[\"Deep RL\"]\n",
    "    D --> D2[\"Multi-Agent RL\"]\n",
    "    D --> D3[\"Safe RL\"]\n",
    "    D --> D4[\"Meta-RL\"]\n",
    "    \n",
    "    E --> E1[\"Games & Simulations\"]\n",
    "    E --> E2[\"Robotics & Control\"]\n",
    "    E --> E3[\"Recommendation Systems\"]\n",
    "    E --> E4[\"Healthcare & Finance\"]\n",
    "    \n",
    "    F --> F1[\"Sample Efficiency\"]\n",
    "    F --> F2[\"Generalization\"]\n",
    "    F --> F3[\"Exploration-Exploitation\"]\n",
    "    F --> F4[\"Value Alignment\"]\n",
    "```\n",
    "\n",
    "### Key Principles to Remember:\n",
    "\n",
    "1. **Reward Hypothesis**: All goals can be framed as maximization of expected cumulative reward\n",
    "2. **Exploration-Exploitation**: Balancing new knowledge vs. exploiting known strategies is essential\n",
    "3. **Credit Assignment**: Determining which actions led to rewards is a fundamental challenge\n",
    "4. **State Representation**: Good state representations capture all relevant information for decision-making\n",
    "5. **Function Approximation**: Enables generalization to unseen states in large environments\n",
    "6. **Policy vs. Value**: Both are important; value functions tell us 'how good', policies tell us 'what to do'\n",
    "7. **Model-Free vs. Model-Based**: Trading off between sample efficiency and computational complexity\n",
    "\n",
    "### The Future of RL:\n",
    "\n",
    "Reinforcement learning continues to push the boundaries of AI capabilities, with promising directions including:\n",
    "\n",
    "- More sample-efficient algorithms that can learn from limited data\n",
    "- Better integration with other AI approaches (supervised, self-supervised, etc.)\n",
    "- Improved alignment with human values and preferences\n",
    "- More robust generalization to new environments and tasks\n",
    "- Increased applications in real-world domains beyond games and simulations\n",
    "\n",
    "As algorithms improve and computational resources increase, reinforcement learning will likely play an increasingly important role in developing autonomous systems that can learn, adapt, and make decisions in complex, uncertain environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
