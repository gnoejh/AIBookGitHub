{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24fbfb72",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Reinforcement/theory-mba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7bd16",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: The Basics\n",
    "\n",
    "This notebook introduces the fundamental concepts of Reinforcement Learning (RL), how it works, and some real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023f95d",
   "metadata": {},
   "source": [
    "[Gymnasium (formerly Gym) documentation](https://www.gymlibrary.dev/content/tutorials/) - Python library for RL environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a61df",
   "metadata": {},
   "source": [
    "## 1. What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. Think of it like training a dog - when the dog does something good, you give it a treat (reward), and over time it learns what actions lead to treats.\n",
    "\n",
    "In RL, our agent (like the dog) takes actions in an environment. After each action, it receives:\n",
    "1. A new state (situation)\n",
    "2. A reward (feedback on how good/bad the action was)\n",
    "\n",
    "The goal is to learn which actions to take in different situations to get the most rewards over time.\n",
    "\n",
    "![RL Basic Concept](https://miro.medium.com/max/1400/1*Z2yMvuQ1-t5Ol1ac_W4dOQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111fec4",
   "metadata": {},
   "source": [
    "### 1.1 The RL Framework\n",
    "\n",
    "The basic structure of reinforcement learning includes:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Agent] --Takes action--> B[Environment]\n",
    "    B --Gives new state--> A\n",
    "    B --Gives reward--> A\n",
    "```\n",
    "\n",
    "Key elements:\n",
    "- **States**: Different situations our agent can be in\n",
    "- **Actions**: Things our agent can do\n",
    "- **Rewards**: Feedback on how good an action was\n",
    "- **Policy**: The strategy our agent uses to decide which action to take"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25081f3a",
   "metadata": {},
   "source": [
    "## 2. Key Components of Reinforcement Learning\n",
    "\n",
    "| Component | Simple Explanation | Example |\n",
    "| --- | --- | --- |\n",
    "| Agent | The learner or decision maker | A robot, game player, or stock trading algorithm |\n",
    "| Environment | Everything the agent interacts with | A game world, physical space, or market |\n",
    "| State | Current situation of the agent | Position in a maze, cards in a poker hand |\n",
    "| Action | Moves the agent can make | Move left/right, buy/sell a stock |\n",
    "| Reward | Feedback from the environment | Points in a game, profit in trading |\n",
    "| Policy | Strategy to decide actions | \"When in state X, take action Y\" |\n",
    "| Value | Expected future rewards | How good it is to be in a particular state |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0c4b4",
   "metadata": {},
   "source": [
    "## 3. How Reinforcement Learning Works\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Agent sees state] --> B[Agent chooses action]\n",
    "    B --> C[Environment changes]\n",
    "    C --> D[Agent gets reward]\n",
    "    D --> E[Agent sees new state]\n",
    "    E --> B\n",
    "```\n",
    "\n",
    "The basic process works like this:\n",
    "\n",
    "1. The agent observes its current state\n",
    "2. Based on this state, the agent selects an action\n",
    "3. The environment changes in response to this action\n",
    "4. The agent receives a reward and observes its new state\n",
    "5. The agent learns from this experience to make better decisions in the future\n",
    "6. The process repeats, and the agent gets better over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35725e4f",
   "metadata": {},
   "source": [
    "## 4. Types of Reinforcement Learning Algorithms\n",
    "\n",
    "There are several different ways to approach reinforcement learning:\n",
    "\n",
    "### Value-Based Methods\n",
    "- Learn how good each state or action is (the \"value\")\n",
    "- Example: Q-learning where we build a table of state-action values\n",
    "\n",
    "### Policy-Based Methods\n",
    "- Directly learn what action to take in each state (the \"policy\")\n",
    "- Example: Policy Gradients that learn to select actions without needing values\n",
    "\n",
    "### Model-Based Methods\n",
    "- Build a model of how the environment works and use it to plan ahead\n",
    "- Example: AlphaZero which builds a model of the game to simulate future moves\n",
    "\n",
    "### Combined Methods\n",
    "- Actor-Critic: Uses both policy (actor) and value (critic) components\n",
    "- Deep RL: Uses neural networks to handle complex states like images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea158d62",
   "metadata": {},
   "source": [
    "## 5. Q-Learning: A Simple RL Algorithm\n",
    "\n",
    "Q-Learning is one of the most basic and popular reinforcement learning algorithms. It's a good place to start understanding RL.\n",
    "\n",
    "### The Basic Idea:\n",
    "\n",
    "1. Build a table (Q-table) with rows for states and columns for actions\n",
    "2. Fill the table with values representing how good each action is in each state\n",
    "3. Update these values as the agent interacts with the environment\n",
    "\n",
    "### The Q-Learning Process:\n",
    "\n",
    "1. Start with a Q-table filled with zeros\n",
    "2. For each episode of training:\n",
    "   - Start in an initial state\n",
    "   - While not at a terminal state:\n",
    "     - Choose an action (using exploration vs. exploitation)\n",
    "     - Take the action, observe reward and new state\n",
    "     - Update Q-value using the formula\n",
    "     - Move to new state\n",
    "\n",
    "### The Q-Learning Update Formula (simplified):\n",
    "\n",
    "Q[state, action] = Q[state, action] + learning_rate * (reward + discount_factor * max(Q[new_state]) - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae532108",
   "metadata": {},
   "source": [
    "### 5.1 Q-Learning Example\n",
    "\n",
    "Let's imagine teaching an agent to navigate a simple 3x3 grid world:\n",
    "\n",
    "```\n",
    "+---------+\n",
    "| S |   |   |\n",
    "+---------+\n",
    "|   | X |   |\n",
    "+---------+\n",
    "|   |   | G |\n",
    "+---------+\n",
    "```\n",
    "\n",
    "Where:\n",
    "- S = Start position\n",
    "- G = Goal (reward +10)\n",
    "- X = Obstacle (can't go here)\n",
    "- Each move has a small penalty (-0.1) to encourage finding the shortest path\n",
    "\n",
    "#### Q-Table Progress:\n",
    "\n",
    "At the beginning (untrained):\n",
    "```\n",
    "State/Action | Up    | Down  | Left  | Right\n",
    "-------------|-------|-------|-------|-------\n",
    "(0,0)        | 0.0   | 0.0   | 0.0   | 0.0\n",
    "(0,1)        | 0.0   | 0.0   | 0.0   | 0.0\n",
    "...\n",
    "```\n",
    "\n",
    "After training:\n",
    "```\n",
    "State/Action | Up    | Down  | Left  | Right\n",
    "-------------|-------|-------|-------|-------\n",
    "(0,0)        | 0.0   | 0.7   | 0.0   | 0.8\n",
    "(0,1)        | 0.0   | 1.5   | 0.6   | 0.7\n",
    "...\n",
    "```\n",
    "\n",
    "The highest values in each row tell us the best action to take in each state!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525c845",
   "metadata": {},
   "source": [
    "## 6. Policies: How Agents Make Decisions\n",
    "\n",
    "A policy is the strategy that an agent uses to decide which actions to take. There are two main types:\n",
    "\n",
    "### Deterministic Policies\n",
    "- Always take the same action in a given state\n",
    "- Example: \"In state A, always move right\"\n",
    "\n",
    "### Stochastic (Random) Policies\n",
    "- Take actions with certain probabilities\n",
    "- Example: \"In state A, move right with 80% probability and left with 20% probability\"\n",
    "\n",
    "### Common Policy Types:\n",
    "\n",
    "#### Random Policy\n",
    "- Take completely random actions (useful for exploration)\n",
    "\n",
    "#### Greedy Policy\n",
    "- Always take the action with the highest expected reward\n",
    "- Problem: Might miss better solutions it hasn't discovered yet\n",
    "\n",
    "#### ε-Greedy Policy\n",
    "- With probability ε: Take a random action (exploration)\n",
    "- With probability 1-ε: Take the best known action (exploitation)\n",
    "- Good balance between trying new things and using what we know works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af9702",
   "metadata": {},
   "source": [
    "## 7. Challenges in Reinforcement Learning\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[RL Challenges] --> B[Exploration vs. Exploitation]\n",
    "    A --> C[Delayed Rewards]\n",
    "    A --> D[Sample Efficiency]\n",
    "    A --> E[Generalization]\n",
    "    A --> F[Stability]\n",
    "```\n",
    "\n",
    "### Exploration vs. Exploitation\n",
    "- **Challenge**: Balancing trying new actions (exploration) vs. using actions known to work well (exploitation)\n",
    "- **Solution**: Strategies like ε-greedy, where ε decreases over time\n",
    "\n",
    "### Delayed Rewards\n",
    "- **Challenge**: Actions might not give immediate rewards - rewards might come much later\n",
    "- **Solution**: Techniques like credit assignment and discounting future rewards\n",
    "\n",
    "### Sample Efficiency\n",
    "- **Challenge**: Learning can require many interactions with the environment\n",
    "- **Solution**: Experience replay, model-based methods, transfer learning\n",
    "\n",
    "### Generalization\n",
    "- **Challenge**: Applying learning to new situations not seen during training\n",
    "- **Solution**: Function approximation like neural networks, good state representations\n",
    "\n",
    "### Stability\n",
    "- **Challenge**: Learning can be unstable, especially with complex algorithms\n",
    "- **Solution**: Target networks, careful learning rate scheduling, proper initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22015c5a",
   "metadata": {},
   "source": [
    "### 7.1 The Exploration-Exploitation Dilemma\n",
    "\n",
    "One of the biggest challenges in RL is knowing when to explore (try new things) versus exploit (use what you know works).\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"Too Much Exploration\"] --> B[\"Wasteful, random behavior\"]\n",
    "    C[\"Too Much Exploitation\"] --> D[\"Get stuck in bad solutions\"]\n",
    "    E[\"Good Balance\"] --> F[\"Find optimal solutions\"]\n",
    "```\n",
    "\n",
    "Think of it like this:\n",
    "- **Exploration**: Trying a new restaurant you've never been to before\n",
    "- **Exploitation**: Going to your favorite restaurant that you know you'll enjoy\n",
    "\n",
    "Popular exploration strategies:\n",
    "- **ε-greedy**: Choose random action with probability ε, best action with probability 1-ε\n",
    "- **Softmax**: Choose actions with probabilities proportional to their expected rewards\n",
    "- **Optimistic initialization**: Start believing all actions are amazing, then learn reality\n",
    "- **Count-based**: Prefer actions that have been tried less frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21bd682",
   "metadata": {},
   "source": [
    "## 8. Simple RL Algorithms for Beginners\n",
    "\n",
    "Let's look at some basic algorithms you can start with:\n",
    "\n",
    "### 1. Q-Learning\n",
    "- Builds a table of state-action values\n",
    "- Easy to understand and implement\n",
    "- Works well for small environments\n",
    "\n",
    "```python\n",
    "# Pseudo-code for Q-Learning\n",
    "Initialize Q-table with zeros\n",
    "For each episode:\n",
    "    Initialize state\n",
    "    While not done:\n",
    "        With probability ε, select random action\n",
    "        Otherwise, select action with highest Q-value\n",
    "        Take action, observe reward and next state\n",
    "        Update Q[state, action] using the Q-learning formula\n",
    "        Move to next state\n",
    "```\n",
    "\n",
    "### 2. SARSA (State-Action-Reward-State-Action)\n",
    "- Similar to Q-learning but \"on-policy\" (learns the value of the policy it's following)\n",
    "- Often more conservative than Q-learning\n",
    "\n",
    "### 3. Monte Carlo Methods\n",
    "- Learn from complete episodes of experience\n",
    "- Simple concept: average the returns following each state\n",
    "- Good for problems where episodes have a clear end point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a816560",
   "metadata": {},
   "source": [
    "## 9. Creating a Simple Q-Learning Agent\n",
    "\n",
    "Let's look at how you might implement a basic Q-learning agent in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70381a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros([num_states, num_actions])\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "exploration_rate = 0.1\n",
    "num_episodes = 2000\n",
    "\n",
    "# Training the agent\n",
    "rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Exploration-exploitation decision\n",
    "        if np.random.random() < exploration_rate:\n",
    "            # Explore: select random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit: select action with highest Q-value\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        \n",
    "        # Take action and observe outcome\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update Q-table using the Q-learning formula\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = new_state\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Display average reward over time\n",
    "plt.plot(np.cumsum(rewards) / (np.arange(num_episodes) + 1))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Learning Progress')\n",
    "plt.show()\n",
    "\n",
    "# Print the learned Q-table\n",
    "print(\"Q-table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325e98c",
   "metadata": {},
   "source": [
    "### Explaining the Code:\n",
    "\n",
    "1. **Environment**: We're using FrozenLake, a simple grid world where the agent must navigate from start to goal without falling in holes.\n",
    "2. **Q-table**: A table with states as rows and actions as columns, storing how good each action is in each state.\n",
    "3. **Training Loop**:\n",
    "   - For each episode, we start in the initial state\n",
    "   - At each step, we either explore (random action) or exploit (best known action)\n",
    "   - After taking an action, we update our Q-value using the Q-learning formula\n",
    "   - We continue until we reach a terminal state (goal or hole)\n",
    "4. **Results**: We plot the average reward over time to see if our agent is learning, and print the final Q-table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bcf811",
   "metadata": {},
   "source": [
    "## 10. Practical Exercises\n",
    "\n",
    "### Exercise 1: Understanding Rewards\n",
    "For the following situations, what types of rewards would help an RL agent learn effectively?\n",
    "- Training a robot to walk\n",
    "- Teaching an agent to play Tic-Tac-Toe\n",
    "- Getting an agent to solve a maze\n",
    "\n",
    "### Exercise 2: Q-Learning by Hand\n",
    "Consider a 2x2 grid world where the top-right corner is the goal (+1 reward) and each step has a small penalty (-0.1). Starting with a Q-table of zeros, trace through the first few updates by hand using the Q-learning formula with learning rate = 0.1 and discount factor = 0.9.\n",
    "\n",
    "### Exercise 3: Algorithm Selection\n",
    "For each scenario, which RL approach might be most appropriate and why?\n",
    "- Teaching a computer to play Chess\n",
    "- Training a robot to balance a pole\n",
    "- Creating a restaurant recommendation system\n",
    "- Optimizing traffic lights in a city\n",
    "\n",
    "### Exercise 4: Exploration-Exploitation Strategy\n",
    "Design an exploration strategy for a food delivery robot that needs to learn the fastest routes around a college campus. How would your strategy change from initial deployment to after several weeks of operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a166261b",
   "metadata": {},
   "source": [
    "## 11. Comparing RL with Other Machine Learning Types\n",
    "\n",
    "| Aspect | Supervised Learning | Unsupervised Learning | Reinforcement Learning |\n",
    "| --- | --- | --- | --- |\n",
    "| Learning from | Labeled examples | Patterns in unlabeled data | Interaction & feedback |\n",
    "| Goal | Predict correct output | Find structure in data | Maximize reward over time |\n",
    "| Example tasks | Classification, Regression | Clustering, Dimensionality Reduction | Game playing, Robot control |\n",
    "| Feedback | Immediate (right/wrong) | None | Delayed (rewards) |\n",
    "| Real-world analogy | Learning with a teacher | Learning without guidance | Learning through trial and error |\n",
    "| Example algorithm | Decision Trees, Neural Networks | K-means, PCA | Q-Learning, Policy Gradients |\n",
    "| Example application | Spam detection | Customer segmentation | Self-driving cars |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738ebc2",
   "metadata": {},
   "source": [
    "## 12. Resources for Further Learning\n",
    "\n",
    "### Books for Beginners:\n",
    "- \"Reinforcement Learning: An Introduction\" by Sutton and Barto (first few chapters)\n",
    "- \"Grokking Deep Reinforcement Learning\" by Miguel Morales\n",
    "\n",
    "### Online Courses:\n",
    "- [David Silver's RL Course](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) (DeepMind)\n",
    "- [Reinforcement Learning Specialization](https://www.coursera.org/specializations/reinforcement-learning) (Coursera)\n",
    "\n",
    "### Tutorials and Hands-On Resources:\n",
    "- [Gymnasium Documentation](https://www.gymlibrary.dev/content/tutorials/)\n",
    "- [Stable Baselines3 Documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html)\n",
    "- [Hugging Face RL Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction)\n",
    "\n",
    "### Interesting Projects to Try:\n",
    "- Train an agent to play simple Atari games\n",
    "- Build a bot that learns to balance an inverted pendulum\n",
    "- Create a simple traffic management system\n",
    "- Develop an agent that learns to play card games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54008b",
   "metadata": {},
   "source": [
    "## 13. Real-World Applications of RL\n",
    "\n",
    "| Application Area | Examples | How RL Helps |\n",
    "| --- | --- | --- |\n",
    "| Games | Chess (DeepMind's AlphaZero), Dota 2 (OpenAI Five) | Learning optimal strategies through self-play |\n",
    "| Robotics | Robot navigation, manipulation tasks | Learning motor skills through trial and error |\n",
    "| Business | Product recommendations, ad placement | Optimizing user engagement over time |\n",
    "| Healthcare | Treatment planning, drug discovery | Personalizing treatments based on patient responses |\n",
    "| Transportation | Traffic light control, ride-sharing | Optimizing resource allocation in dynamic systems |\n",
    "| Energy Management | Smart grid control, data center cooling | Balancing efficiency and stability in complex systems |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4396291",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Reinforcement Learning\"] --> B[\"Learning through interaction\"]\n",
    "    A --> C[\"Balances exploration and exploitation\"]\n",
    "    A --> D[\"Uses rewards to guide learning\"]\n",
    "    A --> E[\"Works without explicit training data\"]\n",
    "    A --> F[\"Applicable to many real-world problems\"]\n",
    "```\n",
    "\n",
    "### What We've Covered:\n",
    "\n",
    "1. **The Fundamentals**: States, actions, rewards, and how they work together\n",
    "2. **Key Algorithms**: Q-learning, SARSA, and their basic principles\n",
    "3. **Common Challenges**: Exploration vs. exploitation, delayed rewards\n",
    "4. **Implementation**: How to create a simple Q-learning agent\n",
    "5. **Applications**: Real-world uses of reinforcement learning\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "If you're interested in diving deeper into RL, consider:\n",
    "- Implementing more advanced algorithms like Deep Q-Networks (DQN)\n",
    "- Exploring policy gradient methods\n",
    "- Working with more complex environments\n",
    "- Studying multi-agent reinforcement learning\n",
    "\n",
    "Remember that reinforcement learning is a powerful approach for problems where an agent needs to make sequences of decisions and learn from feedback over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
