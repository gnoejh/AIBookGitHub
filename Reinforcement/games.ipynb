{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f242830",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Reinforcement/games\n",
    ".ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16a750",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with OpenAI Gym Environments\n",
    "\n",
    "This notebook demonstrates reinforcement learning using OpenAI Gym's classic control problems. We'll implement a Q-learning agent and visualize the learning process across different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f02eff",
   "metadata": {},
   "source": [
    "## Brick Breaker with Reinforcement Learning\n",
    "\n",
    "In this section, we'll implement a Brick Breaker game and train a reinforcement learning agent to play it using Gymnasium and Stable Baselines 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install gymnasium stable-baselines3[extra] pygame numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0539e19b",
   "metadata": {},
   "source": [
    "### Custom Brick Breaker Environment\n",
    "\n",
    "We'll create a custom Gymnasium environment for the classic Brick Breaker game where:\n",
    "- The player controls a paddle at the bottom of the screen\n",
    "- A ball bounces around, breaking bricks when it hits them\n",
    "- The goal is to break all bricks without letting the ball fall below the paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrickBreakerEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 30}\n",
    "    \n",
    "    def __init__(self, render_mode=None, width=400, height=500):\n",
    "        super(BrickBreakerEnv, self).__init__()\n",
    "        \n",
    "        # Game settings\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # Paddle settings\n",
    "        self.paddle_width = 80\n",
    "        self.paddle_height = 10\n",
    "        self.paddle_speed = 10\n",
    "        \n",
    "        # Ball settings\n",
    "        self.ball_radius = 8\n",
    "        self.ball_speed = 5\n",
    "        \n",
    "        # Brick settings\n",
    "        self.brick_rows = 5\n",
    "        self.brick_cols = 8\n",
    "        self.brick_width = (self.width - 20) // self.brick_cols\n",
    "        self.brick_height = 20\n",
    "        \n",
    "        # Define action space: 0=left, 1=stay, 2=right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        # Define observation space\n",
    "        # [paddle_x, ball_x, ball_y, ball_vx, ball_vy, flattened brick grid]\n",
    "        obs_size = 5 + (self.brick_rows * self.brick_cols)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(obs_size,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize pygame if rendering\n",
    "        if self.render_mode is not None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.width, self.height))\n",
    "            self.clock = pygame.time.Clock()\n",
    "            self.font = pygame.font.SysFont(None, 24)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset paddle position\n",
    "        self.paddle_x = (self.width - self.paddle_width) // 2\n",
    "        self.paddle_y = self.height - 30\n",
    "        \n",
    "        # Reset ball position and velocity\n",
    "        self.ball_x = self.width // 2\n",
    "        self.ball_y = self.height // 2\n",
    "        \n",
    "        # Random initial ball velocity\n",
    "        angle = self.np_random.uniform(0.1, 0.9) * np.pi\n",
    "        self.ball_vx = self.ball_speed * np.cos(angle)\n",
    "        self.ball_vy = self.ball_speed * np.sin(angle)\n",
    "        \n",
    "        # Reset bricks\n",
    "        self.bricks = np.ones((self.brick_rows, self.brick_cols), dtype=np.int8)\n",
    "        \n",
    "        # Reset game state\n",
    "        self.score = 0\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        \n",
    "        # Return initial observation\n",
    "        observation = self._get_observation()\n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        # Normalize values to [0,1] range\n",
    "        paddle_x_norm = self.paddle_x / self.width\n",
    "        ball_x_norm = self.ball_x / self.width\n",
    "        ball_y_norm = self.ball_y / self.height\n",
    "        \n",
    "        # Normalize velocities to [-1,1] range and then map to [0,1]\n",
    "        ball_vx_norm = (self.ball_vx / self.ball_speed + 1) / 2\n",
    "        ball_vy_norm = (self.ball_vy / self.ball_speed + 1) / 2\n",
    "        \n",
    "        # Flatten brick grid\n",
    "        brick_grid_flat = self.bricks.flatten()\n",
    "        \n",
    "        # Combine all components into a single array\n",
    "        observation = np.concatenate([\n",
    "            [paddle_x_norm, ball_x_norm, ball_y_norm, ball_vx_norm, ball_vy_norm],\n",
    "            brick_grid_flat\n",
    "        ])\n",
    "        \n",
    "        return observation.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Process action: move paddle\n",
    "        if action == 0:  # Move left\n",
    "            self.paddle_x = max(0, self.paddle_x - self.paddle_speed)\n",
    "        elif action == 2:  # Move right\n",
    "            self.paddle_x = min(self.width - self.paddle_width, self.paddle_x + self.paddle_speed)\n",
    "        # action 1 means stay in place\n",
    "        \n",
    "        # Small negative reward for each step to encourage finishing quickly\n",
    "        reward -= 0.01\n",
    "        \n",
    "        # Move ball\n",
    "        self.ball_x += self.ball_vx\n",
    "        self.ball_y += self.ball_vy\n",
    "        \n",
    "        # Check wall collisions\n",
    "        if self.ball_x <= self.ball_radius or self.ball_x >= self.width - self.ball_radius:\n",
    "            self.ball_vx = -self.ball_vx\n",
    "        if self.ball_y <= self.ball_radius:\n",
    "            self.ball_vy = -self.ball_vy\n",
    "        \n",
    "        # Check if ball is below paddle (game over)\n",
    "        if self.ball_y >= self.height:\n",
    "            self.done = True\n",
    "            reward -= 10  # Big penalty for losing the ball\n",
    "        \n",
    "        # Check paddle collision\n",
    "        if (self.ball_y + self.ball_radius >= self.paddle_y and \n",
    "            self.ball_y - self.ball_radius <= self.paddle_y + self.paddle_height and \n",
    "            self.ball_x + self.ball_radius >= self.paddle_x and \n",
    "            self.ball_x - self.ball_radius <= self.paddle_x + self.paddle_width):\n",
    "            \n",
    "            # Reverse y velocity\n",
    "            self.ball_vy = -abs(self.ball_vy)\n",
    "            \n",
    "            # Adjust x velocity based on where the ball hit the paddle\n",
    "            relative_intersect_x = (self.paddle_x + (self.paddle_width / 2)) - self.ball_x\n",
    "            normalized_intersect_x = relative_intersect_x / (self.paddle_width / 2)\n",
    "            bounce_angle = normalized_intersect_x * (np.pi / 3)  # Max angle: 60 degrees\n",
    "            self.ball_vx = -self.ball_speed * np.sin(bounce_angle)\n",
    "            \n",
    "            # Reward for keeping the ball in play\n",
    "            reward += 0.5\n",
    "        \n",
    "        # Check brick collisions\n",
    "        for row in range(self.brick_rows):\n",
    "            for col in range(self.brick_cols):\n",
    "                if self.bricks[row, col] == 1:  # If brick exists\n",
    "                    brick_x = col * self.brick_width + 10\n",
    "                    brick_y = row * self.brick_height + 40\n",
    "                    \n",
    "                    # Check collision with this brick\n",
    "                    if (self.ball_x + self.ball_radius >= brick_x and \n",
    "                        self.ball_x - self.ball_radius <= brick_x + self.brick_width and \n",
    "                        self.ball_y + self.ball_radius >= brick_y and \n",
    "                        self.ball_y - self.ball_radius <= brick_y + self.brick_height):\n",
    "                        \n",
    "                        # Remove brick\n",
    "                        self.bricks[row, col] = 0\n",
    "                        self.score += 1\n",
    "                        \n",
    "                        # Reward for breaking a brick\n",
    "                        reward += 1.0\n",
    "                        \n",
    "                        # Determine bounce direction based on side of collision\n",
    "                        dx = min(abs(self.ball_x - brick_x), abs(self.ball_x - (brick_x + self.brick_width)))\n",
    "                        dy = min(abs(self.ball_y - brick_y), abs(self.ball_y - (brick_y + self.brick_height)))\n",
    "                        \n",
    "                        if dx < dy:  # Horizontal collision\n",
    "                            self.ball_vx = -self.ball_vx\n",
    "                        else:  # Vertical collision\n",
    "                            self.ball_vy = -self.ball_vy\n",
    "                        \n",
    "                        break  # Only process one brick collision per step\n",
    "        \n",
    "        # Check if all bricks are broken (win condition)\n",
    "        if np.sum(self.bricks) == 0:\n",
    "            self.done = True\n",
    "            reward += 50  # Big reward for clearing all bricks\n",
    "        \n",
    "        # Return observation, reward, done flag, truncated flag, and info\n",
    "        observation = self._get_observation()\n",
    "        info = {\"score\": self.score}\n",
    "        \n",
    "        return observation, reward, self.done, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            return\n",
    "            \n",
    "        if self.render_mode == \"human\":\n",
    "            # Clear the screen\n",
    "            self.window.fill((0, 0, 0))\n",
    "            \n",
    "            # Draw paddle\n",
    "            pygame.draw.rect(self.window, (255, 255, 255), \n",
    "                             (self.paddle_x, self.paddle_y, self.paddle_width, self.paddle_height))\n",
    "            \n",
    "            # Draw ball\n",
    "            pygame.draw.circle(self.window, (255, 255, 255), \n",
    "                              (int(self.ball_x), int(self.ball_y)), self.ball_radius)\n",
    "            \n",
    "            # Draw bricks\n",
    "            colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]\n",
    "            for row in range(self.brick_rows):\n",
    "                color = colors[row % len(colors)]\n",
    "                for col in range(self.brick_cols):\n",
    "                    if self.bricks[row, col] == 1:\n",
    "                        pygame.draw.rect(self.window, color, \n",
    "                                        (col * self.brick_width + 10, row * self.brick_height + 40, \n",
    "                                         self.brick_width - 2, self.brick_height - 2))\n",
    "            \n",
    "            # Draw score\n",
    "            score_text = self.font.render(f'Score: {self.score}', True, (255, 255, 255))\n",
    "            self.window.blit(score_text, (10, 10))\n",
    "            \n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            return None\n",
    "            \n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            # Return an RGB array representation of the game\n",
    "            return np.zeros((self.height, self.width, 3), dtype=np.uint8)\n",
    "    \n",
    "    def close(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a1445",
   "metadata": {},
   "source": [
    "### Register Custom Environment with Gymnasium\n",
    "\n",
    "Register our Brick Breaker environment with Gymnasium so we can use it with Stable Baselines3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da354a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom environment\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='BrickBreaker-v0',\n",
    "    entry_point='__main__:BrickBreakerEnv',\n",
    "    max_episode_steps=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b1d163",
   "metadata": {},
   "source": [
    "### Training with Stable Baselines3\n",
    "\n",
    "Now we'll set up and train a reinforcement learning agent using PPO from Stable Baselines3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and wrap the environment\n",
    "def make_env():\n",
    "    env = gym.make('BrickBreaker-v0')\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "# Create a vectorized environment\n",
    "env = DummyVecEnv([make_env])\n",
    "\n",
    "# Create a callback for saving checkpoints\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10000,\n",
    "    save_path=\"./brickbreaker_model_checkpoints/\",\n",
    "    name_prefix=\"brickbreaker_model\"\n",
    ")\n",
    "\n",
    "# Define policy network parameters\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[64, 64]\n",
    ")\n",
    "\n",
    "# Set up the PPO agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=0.0003,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e48f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=300000,\n",
    "    callback=checkpoint_callback,\n",
    "    progress_bar=False  # Disable progress bar to avoid LiveError\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"brickbreaker_ppo_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d1b99",
   "metadata": {},
   "source": [
    "### Evaluation and Visualization\n",
    "\n",
    "Let's evaluate the trained agent and watch it play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained agent\n",
    "eval_env = gym.make('BrickBreaker-v0', render_mode='human')\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# Run a few episodes and watch the agent play\n",
    "obs, info = eval_env.reset()\n",
    "total_reward = 0\n",
    "for i in range(5000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = eval_env.step(action)\n",
    "    total_reward += reward\n",
    "    eval_env.render()\n",
    "    \n",
    "    if done or truncated:\n",
    "        print(f\"Episode finished with reward {total_reward} and score {info['score']}\")\n",
    "        obs, info = eval_env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055a397",
   "metadata": {},
   "source": [
    "### Compare Different RL Algorithms\n",
    "\n",
    "Let's train and compare different algorithms from Stable Baselines3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate an algorithm\n",
    "def train_and_evaluate(algo_class, algo_name, timesteps=100000):\n",
    "    # Create a fresh environment\n",
    "    train_env = DummyVecEnv([make_env])\n",
    "    \n",
    "    # Create the model\n",
    "    model = algo_class(\n",
    "        \"MlpPolicy\",\n",
    "        train_env,\n",
    "        policy_kwargs=dict(net_arch=[64, 64]),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"Training {algo_name}...\")\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "    print(f\"{algo_name} achieved mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(f\"brickbreaker_{algo_name.lower()}\")\n",
    "    \n",
    "    return model, mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf30341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate different algorithms\n",
    "algorithms = [\n",
    "    (PPO, \"PPO\"),\n",
    "    (A2C, \"A2C\"),\n",
    "    (DQN, \"DQN\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for algo_class, algo_name in algorithms:\n",
    "    _, mean_reward, std_reward = train_and_evaluate(algo_class, algo_name, timesteps=100000)\n",
    "    results.append((algo_name, mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c507802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "algo_names = [result[0] for result in results]\n",
    "mean_rewards = [result[1] for result in results]\n",
    "std_rewards = [result[2] for result in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(algo_names, mean_rewards, yerr=std_rewards, capsize=10)\n",
    "plt.title('Performance Comparison of RL Algorithms on Brick Breaker')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a3b17",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented a Brick Breaker game as a custom Gymnasium environment and trained a reinforcement learning agent to play it using Stable Baselines3. We've seen how:\n",
    "\n",
    "1. Custom game environments can be created with Gymnasium\n",
    "2. Different RL algorithms (PPO, A2C, DQN) can be applied to the same problem\n",
    "3. RL agents can learn complex game strategies through trial and error\n",
    "\n",
    "The trained agent has learned to control the paddle to keep the ball in play and break bricks efficiently. This approach can be extended to other game environments and more complex control problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
