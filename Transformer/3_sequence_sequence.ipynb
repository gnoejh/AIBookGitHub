{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc0036a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/3_sequence_sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbda49",
   "metadata": {},
   "source": [
    "\n",
    "# Sequence-Sequence Models\n",
    "\n",
    "![type_sequence_Modeling](type_sequence_Modeling.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8594355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision # Uncomment if running on Colab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad06fc",
   "metadata": {},
   "source": [
    "### 1. One-to-One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52360890",
   "metadata": {},
   "source": [
    "\n",
    "**Description**: A single input corresponds to a single output. This is typical in simple classification tasks.\n",
    "\n",
    "**Example**: Image Classification\n",
    "\n",
    "In this example, we'll simulate an image classification task using PyTorch. Although it's a simplified example, the idea is to classify a single image into one of multiple categories.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98746d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-to-One Model Output (Logits): tensor([[ 0.3612,  0.0005, -0.0102, -0.0513,  0.2687, -0.0353, -0.3247,  0.0128,\n",
      "          0.1543,  0.2045]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simulated input image (1, 3, 32, 32) -> Batch size of 1, 3 channels (RGB), 32x32 pixels\n",
    "image = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Simple CNN for classification\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, num_classes) # 8*8 becuase of 2 maxpool layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and run model\n",
    "num_classes = 10  # e.g., 10 classes\n",
    "model = SimpleCNN(num_classes)\n",
    "output = model(image)\n",
    "print(\"One-to-One Model Output (Logits):\", output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd43bb",
   "metadata": {},
   "source": [
    "### 2. One-to-Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e444e5e",
   "metadata": {},
   "source": [
    "\n",
    "**Description**: A single input produces a sequence of outputs. This is useful in tasks where a single piece of information is expanded into a sequence.\n",
    "\n",
    "**Example**: Image Captioning\n",
    "\n",
    "In this example, we simulate a basic image captioning model. An image is used as input to generate a caption (sequence of words) describing the image.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d82b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-to-Many Model Output (Caption logits): tensor([[ 0.0638, -0.0109, -0.0022, -0.0155, -0.0134,  0.0366, -0.0775,  0.0764,\n",
      "         -0.0830,  0.0148,  0.0457, -0.0639,  0.0202, -0.0286,  0.1380,  0.0446,\n",
      "         -0.0092,  0.0021,  0.0749, -0.1163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.cnn = nn.Conv2d(3, embed_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Pooling to reduce spatial dimensions\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, image):\n",
    "        features = torch.relu(self.cnn(image))\n",
    "        features = self.pool(features).view(image.size(0), 1, -1)  # Pool and flatten\n",
    "        outputs, _ = self.rnn(features)\n",
    "        captions = self.fc(outputs.squeeze(1))\n",
    "        return captions\n",
    "\n",
    "\n",
    "# Simulated image input and vocabulary\n",
    "vocab_size = 20\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "\n",
    "# Simulated input image (1, 3, 32, 32) -> Batch size of 1, 3 channels (RGB), 32x32 pixels\n",
    "image = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "model = ImageCaptioningModel(vocab_size, embed_size, hidden_size)\n",
    "caption_output = model(image)\n",
    "print(\"One-to-Many Model Output (Caption logits):\", caption_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf36a4",
   "metadata": {},
   "source": [
    "### 3. Many-to-One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c367e",
   "metadata": {},
   "source": [
    "\n",
    "**Description**: A sequence of inputs produces a single output. This is useful for summarizing or classifying entire sequences.\n",
    "\n",
    "**Example**: Sentiment Analysis\n",
    "\n",
    "In this example, a sequence of words is classified into a single sentiment label, such as positive or negative sentiment.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a09540ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many-to-One Model Output (Sentiment logits): tensor([[-0.0282,  0.0227]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "# Simulated input sequence\n",
    "sequence = torch.randint(0, vocab_size, (1, 5))  # Batch size of 1, sequence length of 5\n",
    "\n",
    "output_size = 2  # Binary sentiment classification (positive/negative)\n",
    "model = SentimentAnalysisModel(vocab_size, embed_size, hidden_size, output_size)\n",
    "sentiment_output = model(sequence)\n",
    "print(\"Many-to-One Model Output (Sentiment logits):\", sentiment_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1aff1",
   "metadata": {},
   "source": [
    "### 4. Many-to-Many (Aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c88d0",
   "metadata": {},
   "source": [
    "\n",
    "**Description**: Each input in the sequence corresponds to an output in the sequence, maintaining alignment between input and output.\n",
    "\n",
    "**Example**: Part-of-Speech Tagging\n",
    "\n",
    "In this example, each word in a sentence is tagged with its part of speech (POS). The input and output sequences have the same length.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96c2fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many-to-Many (Aligned) Model Output (POS Tag logits): tensor([[[-0.1526, -0.0360, -0.0051,  0.1109, -0.0153],\n",
      "         [-0.0389, -0.0623, -0.0084,  0.0766, -0.0446],\n",
      "         [-0.0445, -0.0105, -0.0255,  0.0409, -0.0622],\n",
      "         [-0.0868, -0.0670, -0.0670,  0.0085, -0.0557],\n",
      "         [-0.0679, -0.0116, -0.1109, -0.0032, -0.0228]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class POSTaggingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(POSTaggingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        tags = self.fc(rnn_out)\n",
    "        return tags\n",
    "\n",
    "# Simulated input sentence (sequence of word indices)\n",
    "sentence = torch.randint(0, vocab_size, (1, 5))  # Batch size of 1, sequence length of 5\n",
    "\n",
    "output_size = 5  # Number of possible POS tags\n",
    "model = POSTaggingModel(vocab_size, embed_size, hidden_size, output_size)\n",
    "pos_tags_output = model(sentence)\n",
    "print(\"Many-to-Many (Aligned) Model Output (POS Tag logits):\", pos_tags_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a79e65",
   "metadata": {},
   "source": [
    "### 5. Many-to-Many (Non-Aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d98c18",
   "metadata": {},
   "source": [
    "\n",
    "**Description**: A sequence of inputs produces a sequence of outputs with potentially different lengths. Here, the entire input sequence affects the entire output sequence.\n",
    "\n",
    "**Example**: Machine Translation\n",
    "\n",
    "In this example, a sequence in one language (e.g., English) is translated into a sequence in another language (e.g., French) with a potentially different length.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46b40c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many-to-Many (Non-Aligned) Model Output (Translation logits): tensor([[[ 0.0397, -0.0178, -0.0525,  ..., -0.1130, -0.0455,  0.1471],\n",
      "         [ 0.1677, -0.0168,  0.0174,  ...,  0.0165, -0.0347,  0.0858],\n",
      "         [ 0.0698, -0.0674,  0.0535,  ...,  0.0376,  0.0115,  0.0411],\n",
      "         ...,\n",
      "         [ 0.0496, -0.0804, -0.0178,  ..., -0.0640, -0.0081,  0.0620],\n",
      "         [ 0.0917, -0.0692, -0.0396,  ..., -0.0282, -0.1171,  0.0887],\n",
      "         [ 0.0170, -0.1550, -0.0283,  ..., -0.0355, -0.0411,  0.0718]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MachineTranslationModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, hidden_size):\n",
    "        super(MachineTranslationModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.encoder_rnn = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
    "        self.decoder_rnn = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.src_embedding(src)\n",
    "        _, (hidden, _) = self.encoder_rnn(src_embedded)\n",
    "\n",
    "        tgt_embedded = self.tgt_embedding(tgt)\n",
    "        decoder_out, _ = self.decoder_rnn(tgt_embedded, (hidden, hidden))\n",
    "        output = self.fc(decoder_out)\n",
    "        return output\n",
    "\n",
    "# Simulated input and target sequences\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "src_sequence = torch.randint(0, src_vocab_size, (1, 6))  # Source sequence\n",
    "tgt_sequence = torch.randint(0, tgt_vocab_size, (1, 8))  # Target sequence\n",
    "\n",
    "model = MachineTranslationModel(src_vocab_size, tgt_vocab_size, embed_size, hidden_size)\n",
    "translation_output = model(src_sequence, tgt_sequence)\n",
    "print(\"Many-to-Many (Non-Aligned) Model Output (Translation logits):\", translation_output)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
