{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ad6140",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/12_transformer_code_russian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ced21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbfe5b",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5bf12",
   "metadata": {},
   "source": [
    "### 1. Using nn.Transformer for the Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7884a5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, embed_size, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / embed_size)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i) / embed_size)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class TransformerConfig:\n",
    "    \"\"\"Configuration class to store all transformer related parameters\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.src_vocab_size = kwargs.get('src_vocab_size', 15)\n",
    "        self.tgt_vocab_size = kwargs.get('tgt_vocab_size', 15)\n",
    "        self.embed_size = kwargs.get('embed_size', 16)\n",
    "        self.num_heads = kwargs.get('num_heads', 4)\n",
    "        self.num_layers = kwargs.get('num_layers', 3)\n",
    "        self.forward_expansion = kwargs.get('forward_expansion', 4)\n",
    "        self.dropout = kwargs.get('dropout', 0.1)\n",
    "        self.max_length = kwargs.get('max_length', 100)\n",
    "        self.num_epochs = kwargs.get('num_epochs', 100)\n",
    "        self.learning_rate = kwargs.get('learning_rate', 0.001)\n",
    "        self.batch_size = kwargs.get('batch_size', 2)\n",
    "        self.pad_token = kwargs.get('pad_token', '<pad>')\n",
    "        self.sos_token = kwargs.get('sos_token', '<sos>')\n",
    "        self.eos_token = kwargs.get('eos_token', '<eos>')\n",
    "        self.pad_idx = kwargs.get('pad_idx', 0)\n",
    "        self.sos_idx = kwargs.get('sos_idx', 1)\n",
    "        self.eos_idx = kwargs.get('eos_idx', 2)\n",
    "\n",
    "class VocabularyManager:\n",
    "    \"\"\"Class to manage vocabulary creation and token mapping\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.src_vocab = {}\n",
    "        self.tgt_vocab = {}\n",
    "        self.idx_to_src = {}\n",
    "        self.idx_to_tgt = {}\n",
    "\n",
    "    def set_predefined_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.idx_to_src = {v: k for k, v in src_vocab.items()}\n",
    "        self.idx_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "    def tokenize(self, sentence, vocab):\n",
    "        tokens = sentence.split()\n",
    "        return [vocab.get(token, vocab.get(self.config.pad_token)) for token in tokens]\n",
    "\n",
    "    def detokenize(self, indices, idx_to_token):\n",
    "        return ' '.join([idx_to_token.get(idx, '<unk>') for idx in indices])\n",
    "\n",
    "    def prepare_batch(self, src_sentences, tgt_sentences=None):\n",
    "        src_batch = []\n",
    "        for sent in src_sentences:\n",
    "            tokens = self.tokenize(sent, self.src_vocab) + [self.src_vocab[self.config.eos_token]]\n",
    "            while len(tokens) < 5:\n",
    "                tokens.append(self.src_vocab[self.config.pad_token])\n",
    "            src_batch.append(tokens)\n",
    "\n",
    "        tgt_batch = None\n",
    "        if tgt_sentences:\n",
    "            tgt_batch = []\n",
    "            for sent in tgt_sentences:\n",
    "                tokens = [self.tgt_vocab[self.config.sos_token]] + self.tokenize(sent, self.tgt_vocab) + [self.tgt_vocab[self.config.eos_token]]\n",
    "                tgt_batch.append(tokens)\n",
    "\n",
    "        return torch.tensor(src_batch), torch.tensor(tgt_batch) if tgt_batch else None\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(config.src_vocab_size, config.embed_size)\n",
    "        self.tgt_embedding = nn.Embedding(config.tgt_vocab_size, config.embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(config.embed_size, max_len=config.max_length)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.embed_size, \n",
    "            nhead=config.num_heads, \n",
    "            num_encoder_layers=config.num_layers, \n",
    "            num_decoder_layers=config.num_layers, \n",
    "            dim_feedforward=config.forward_expansion * config.embed_size, \n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(config.embed_size, config.tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_embedded = self.positional_encoding(self.src_embedding(src))\n",
    "        tgt_embedded = self.positional_encoding(self.tgt_embedding(tgt))\n",
    "        transformer_output = self.transformer(\n",
    "            src_embedded, tgt_embedded, src_key_padding_mask=src_mask, tgt_key_padding_mask=tgt_mask\n",
    "        )\n",
    "        out = self.fc_out(transformer_output)\n",
    "        return out\n",
    "\n",
    "class TransformerTrainer:\n",
    "    \"\"\"Class to handle training the transformer model\"\"\"\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=config.pad_idx)\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    def train_epoch(self, src, tgt):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(src, tgt[:, :-1])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        tgt_output = tgt[:, 1:].reshape(-1)\n",
    "        loss = self.criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def evaluate(self, src, tgt, vocab_manager):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            eval_output = self.model(src, tgt[:, :-1])\n",
    "            _, predicted = torch.max(eval_output, dim=2)\n",
    "            return predicted\n",
    "\n",
    "    def train(self, src, tgt, vocab_manager):\n",
    "        print(\"\\nTraining the model to translate English to Russian:\")\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            loss = self.train_epoch(src, tgt)\n",
    "            predicted = self.evaluate(src, tgt, vocab_manager)\n",
    "            if epoch == self.config.num_epochs - 1 or epoch % 3 == 0:\n",
    "                print(f\"\\nEpoch [{epoch + 1}/{self.config.num_epochs}], Loss: {loss:.4f}\")\n",
    "                print(f\"Source: I love books\")\n",
    "                pred_sentence = [vocab_manager.idx_to_tgt[idx.item()] for idx in predicted[0]]\n",
    "                print(f\"Predicted translation: {' '.join(pred_sentence)}\")\n",
    "                target_sentence = [vocab_manager.idx_to_tgt[idx.item()] for idx in tgt[0, 1:]]\n",
    "                print(f\"Target translation: {' '.join(target_sentence)}\")\n",
    "\n",
    "class TranslationService:\n",
    "    \"\"\"Class to handle translation services\"\"\"\n",
    "    def __init__(self, model, vocab_manager, config):\n",
    "        self.model = model\n",
    "        self.vocab_manager = vocab_manager\n",
    "        self.config = config\n",
    "\n",
    "    def translate_sentence(self, sentence, max_length=None):\n",
    "        if max_length is None:\n",
    "            max_length = self.config.max_length\n",
    "        self.model.eval()\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = sentence.split()\n",
    "            src_indices = [self.vocab_manager.src_vocab.get(token, self.vocab_manager.src_vocab[self.config.pad_token]) for token in tokens]\n",
    "            src_indices.append(self.vocab_manager.src_vocab[self.config.eos_token])\n",
    "            while len(src_indices) < 5:\n",
    "                src_indices.append(self.vocab_manager.src_vocab[self.config.pad_token])\n",
    "        else:\n",
    "            src_indices = sentence\n",
    "        src_tensor = torch.tensor([src_indices])\n",
    "        tgt_tensor = torch.tensor([[self.vocab_manager.tgt_vocab[self.config.sos_token]]])\n",
    "        for _ in range(max_length - 1):\n",
    "            with torch.no_grad():\n",
    "                output = self.model(src_tensor, tgt_tensor)\n",
    "            pred_token = output[0, -1].argmax().item()\n",
    "            new_tgt = torch.zeros(1, tgt_tensor.size(1) + 1, dtype=torch.long)\n",
    "            new_tgt[0, :-1] = tgt_tensor\n",
    "            new_tgt[0, -1] = pred_token\n",
    "            tgt_tensor = new_tgt\n",
    "            if pred_token == self.vocab_manager.tgt_vocab[self.config.eos_token]:\n",
    "                break\n",
    "        predicted_indices = tgt_tensor[0].tolist()\n",
    "        translated_tokens = []\n",
    "        for idx in predicted_indices[1:]:\n",
    "            if idx == self.vocab_manager.tgt_vocab[self.config.eos_token]:\n",
    "                break\n",
    "            translated_tokens.append(self.vocab_manager.idx_to_tgt.get(idx, '<unk>'))\n",
    "        return translated_tokens\n",
    "\n",
    "def main():\n",
    "    config_params = {\n",
    "        'src_vocab_size': 50,\n",
    "        'tgt_vocab_size': 50,\n",
    "        'embed_size': 32,\n",
    "        'num_heads': 4,\n",
    "        'num_layers': 3,\n",
    "        'forward_expansion': 4,\n",
    "        'dropout': 0.1,\n",
    "        'max_length': 100,\n",
    "        'num_epochs': 1000,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 2,\n",
    "        'pad_token': '<pad>',\n",
    "        'sos_token': '<sos>',\n",
    "        'eos_token': '<eos>',\n",
    "        'pad_idx': 0,\n",
    "        'sos_idx': 1,\n",
    "        'eos_idx': 2,\n",
    "    }\n",
    "    config = TransformerConfig(**config_params)\n",
    "    vocab_manager = VocabularyManager(config)\n",
    "    eng_vocab = {\n",
    "        config.pad_token: config.pad_idx, \n",
    "        config.sos_token: config.sos_idx, \n",
    "        config.eos_token: config.eos_idx, \n",
    "        'I': 3, 'you': 4, 'love': 5, 'like': 6, 'books': 7, 'music': 8, 'movies': 9, 'food': 10,\n",
    "        'we': 11, 'they': 12, 'he': 13, 'she': 14, 'it': 15,\n",
    "        'read': 16, 'watch': 17, 'eat': 18, 'play': 19, 'write': 20,\n",
    "        'good': 21, 'bad': 22, 'beautiful': 23, 'interesting': 24, 'boring': 25,\n",
    "        'coffee': 26, 'tea': 27, 'water': 28, 'cake': 29, 'bread': 30,\n",
    "        'computer': 31, 'phone': 32, 'car': 33, 'house': 34, 'school': 35,\n",
    "        'friend': 36, 'family': 37, 'mother': 38, 'father': 39, 'sister': 40,\n",
    "        'brother': 41, 'dog': 42, 'cat': 43, 'bird': 44, 'fish': 45,\n",
    "        'and': 46, 'or': 47, 'but': 48, 'because': 49\n",
    "    }\n",
    "    ru_vocab = {\n",
    "        config.pad_token: config.pad_idx, \n",
    "        config.sos_token: config.sos_idx, \n",
    "        config.eos_token: config.eos_idx, \n",
    "        'я': 3, 'ты': 4, 'люблю': 5, 'нравится': 6, 'книги': 7, 'музыка': 8, 'фильмы': 9, 'еда': 10,\n",
    "        'мы': 11, 'они': 12, 'он': 13, 'она': 14, 'оно': 15,\n",
    "        'читать': 16, 'смотреть': 17, 'есть': 18, 'играть': 19, 'писать': 20,\n",
    "        'хороший': 21, 'плохой': 22, 'красивый': 23, 'интересный': 24, 'скучный': 25,\n",
    "        'кофе': 26, 'чай': 27, 'вода': 28, 'торт': 29, 'хлеб': 30,\n",
    "        'компьютер': 31, 'телефон': 32, 'машина': 33, 'дом': 34, 'школа': 35,\n",
    "        'друг': 36, 'семья': 37, 'мать': 38, 'отец': 39, 'сестра': 40,\n",
    "        'брат': 41, 'собака': 42, 'кошка': 43, 'птица': 44, 'рыба': 45,\n",
    "        'и': 46, 'или': 47, 'но': 48, 'потому': 49\n",
    "    }\n",
    "    vocab_manager.set_predefined_vocab(eng_vocab, ru_vocab)\n",
    "    src = torch.tensor([\n",
    "        [eng_vocab['I'], eng_vocab['love'], eng_vocab['books'], eng_vocab[config.eos_token], eng_vocab[config.pad_token]],\n",
    "        [eng_vocab['you'], eng_vocab['like'], eng_vocab['music'], eng_vocab[config.eos_token], eng_vocab[config.pad_token]]\n",
    "    ])\n",
    "    print(\"Source sentences:\")\n",
    "    for i in range(src.shape[0]):\n",
    "        print([vocab_manager.idx_to_src[idx.item()] for idx in src[i]])\n",
    "    tgt = torch.tensor([\n",
    "        [ru_vocab[config.sos_token], ru_vocab['я'], ru_vocab['люблю'], ru_vocab['книги'], ru_vocab[config.eos_token]],\n",
    "        [ru_vocab[config.sos_token], ru_vocab['ты'], ru_vocab['нравится'], ru_vocab['музыка'], ru_vocab[config.eos_token]]\n",
    "    ])\n",
    "    print(\"Target sentences:\")\n",
    "    for i in range(tgt.shape[0]):\n",
    "        print([vocab_manager.idx_to_tgt[idx.item()] for idx in tgt[i]])\n",
    "    transformer = TransformerModel(config)\n",
    "    output = transformer(src, tgt)\n",
    "    print(f\"Transformer Model Output Shape: {output.shape}\")\n",
    "    _, predicted = torch.max(output, dim=2)\n",
    "    print(\"Predicted sequences:\")\n",
    "    for i in range(predicted.shape[0]):\n",
    "        print([vocab_manager.idx_to_tgt.get(idx.item(), '<unknown>') for idx in predicted[i]])\n",
    "    trainer = TransformerTrainer(transformer, config)\n",
    "    trainer.train(src, tgt, vocab_manager)\n",
    "    translation_service = TranslationService(transformer, vocab_manager, config)\n",
    "    print(\"\\nTesting the model with new sentences:\")\n",
    "    test_sentences = [\n",
    "        \"I love music\",\n",
    "        \"you like books\", \n",
    "        \"I like movies\"\n",
    "    ]\n",
    "    for test_sent in test_sentences:\n",
    "        translated_tokens = translation_service.translate_sentence(test_sent)\n",
    "        print(f\"Test Session\")\n",
    "        print(f\"Source: {test_sent}\")\n",
    "        print(f\"Translation: {' '.join(translated_tokens)}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
