{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be61ef5e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/2_word embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9f6b2",
   "metadata": {},
   "source": [
    "# Word Embedding and Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19281b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy==1.24.3 # colab``\n",
    "# %pip install gensim # colab\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7fbd36",
   "metadata": {},
   "source": [
    "### 1. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37d795",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Represent each word as a binary vector where only one position (corresponding to the word's index) is set to 1, while the rest are set to 0.\n",
    "\n",
    "**Explanation**: One-hot encoding is a straightforward method for representing words, but it has limitations. It creates sparse vectors (mostly zeros) and does not capture any semantic relationship between words. For example, 'cat' and 'dog' would be just as unrelated as 'cat' and 'car' in this representation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed90d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding for 'hello': [1. 0. 0. 0.]\n",
      "One-hot encoding for 'world': [0. 1. 0. 0.]\n",
      "One-hot encoding for 'nlp': [0. 0. 1. 0.]\n",
      "One-hot encoding for 'tutorial': [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Sample vocabulary and sentences\n",
    "vocabulary = [\"hello\", \"world\", \"nlp\", \"tutorial\"]\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# One-hot encode a sample word\n",
    "def one_hot_encode(word, vocab_size=len(vocabulary)):\n",
    "    vector = np.zeros(vocab_size)\n",
    "    vector[word_to_index[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# Example usage\n",
    "for word in vocabulary:\n",
    "    print(f\"One-hot encoding for '{word}':\", one_hot_encode(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094dca0",
   "metadata": {},
   "source": [
    "### 2. Frequency-Based Embeddings: Bag of Words (BoW) and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d442a7",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Represent documents as vectors based on the frequency of each word in the document.\n",
    "\n",
    "**Explanation**: \n",
    "- **Bag of Words (BoW)**: This method creates vectors that represent word counts for each term in a document, disregarding word order and capturing only the frequency of terms. While simple, BoW can result in large vectors and may lose contextual meaning.\n",
    "- **TF-IDF (Term Frequency - Inverse Document Frequency)**: This method enhances BoW by weighting terms based on their importance in a document, calculated by the frequency within a document and rarity across documents. It reduces the influence of commonly used words and highlights unique terms, which is beneficial in document classification tasks.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ebf9332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Vectors: [[1 0 0 1]\n",
      " [1 1 0 0]\n",
      " [0 1 1 0]]\n",
      "Vocabulary: ['hello' 'nlp' 'tutorial' 'world']\n",
      "TF-IDF Vectors: [[0.60534851 0.         0.         0.79596054]\n",
      " [0.70710678 0.70710678 0.         0.        ]\n",
      " [0.         0.60534851 0.79596054 0.        ]]\n",
      "Vocabulary: ['hello' 'nlp' 'tutorial' 'world']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample sentences\n",
    "sentences = [\"hello world\", \"hello nlp\", \"nlp tutorial\"]\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "bow_vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "print(\"Bag of Words Vectors:\", bow_vectors)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(sentences).toarray()\n",
    "print(\"TF-IDF Vectors:\", tfidf_vectors)\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2943dd",
   "metadata": {},
   "source": [
    "### 3. Distributed Embeddings: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2a3f1",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Capture word meanings based on surrounding context using large text corpora, making similar words closer in vector space.\n",
    "\n",
    "**Explanation**: Word2Vec creates word embeddings by training on contexts within sentences. It produces dense, low-dimensional vectors where words with similar meanings have similar embeddings. This is achieved through two training methods:\n",
    "- **Skip-gram**: Predicts surrounding words given a target word, capturing word associations.\n",
    "- **CBOW (Continuous Bag of Words)**: Predicts a target word from its context, which can be more efficient but less accurate with rare words.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83dc33bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec embedding for 'hello': [-0.14233617  0.12917745  0.17945977 -0.10030856 -0.07526743]\n",
      "Word2Vec embedding for 'world': [-0.03632035  0.0575316   0.01983747 -0.1657043  -0.18897636]\n",
      "Word2Vec embedding for 'nlp': [-0.01072454  0.00472863  0.10206699  0.18018547 -0.186059  ]\n",
      "Word2Vec embedding for 'tutorial': [ 0.1476101  -0.03066943 -0.09073226  0.13108103 -0.09720321]\n"
     ]
    }
   ],
   "source": [
    "# Sample corpus\n",
    "corpus = [[\"hello\", \"world\"], [\"hello\", \"nlp\"], [\"nlp\", \"tutorial\"]]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=corpus, vector_size=5, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Get vector for a word\n",
    "# print(\"Word2Vec embedding for 'hello':\", model.wv[\"hello\"])\n",
    "for word in vocabulary:\n",
    "    print(f\"Word2Vec embedding for '{word}':\", model.wv[word])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236ef25",
   "metadata": {},
   "source": [
    "### 4. GloVe (Global Vectors for Word Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65e748",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Capture word semantics by modeling global word co-occurrence statistics.\n",
    "\n",
    "**Explanation**: Unlike Word2Vec, which learns from local contexts, GloVe (Global Vectors) uses global word co-occurrence statistics across a corpus to capture meaning. It results in embeddings where semantic relationships are preserved, allowing vector arithmetic to reveal analogies (e.g., \"king\" - \"man\" + \"woman\" â‰ˆ \"queen\"). GloVe embeddings are typically pre-trained on large corpora and can be used as-is without further training on smaller datasets.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "705ef8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embedding for 'hello': [-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ]\n",
      "GloVe embedding for 'world': [-0.41486   0.71848  -0.3045    0.87445   0.22441  -0.56488  -0.37566\n",
      " -0.44801   0.61347  -0.11359   0.74556  -0.10598  -1.1882    0.50974\n",
      "  1.3511    0.069851  0.73314   0.26773  -1.1787   -0.148     0.039853\n",
      "  0.033107 -0.27406   0.25125   0.41507  -1.6188   -0.81778  -0.73892\n",
      " -0.28997   0.57277   3.4719    0.73817  -0.044495 -0.15119  -0.93503\n",
      " -0.13152  -0.28562   0.76327  -0.83332  -0.6793   -0.39099  -0.64466\n",
      "  1.0044   -0.2051    0.46799   0.99314  -0.16221  -0.46022  -0.37639\n",
      " -0.67542 ]\n",
      "GloVe embedding for 'nlp': [-0.6721    -0.17858    0.20188    0.63581   -0.31304    1.2183\n",
      " -0.13314   -1.1776    -0.27009    0.52236   -0.0086308 -0.056211\n",
      "  1.3483    -1.0131    -1.0985    -0.24086   -0.0066808 -0.14822\n",
      " -0.044672   0.54472   -0.92966   -0.69065    0.91675    0.054691\n",
      " -0.2081     1.1201     0.92071   -1.2295     0.107      0.65846\n",
      " -0.84775   -0.14577   -0.69941    0.83514    0.90995   -0.70647\n",
      " -0.78513    0.82611    1.0785     0.29806    1.0306     0.19589\n",
      " -0.5562     0.43684    0.5979     0.77427    0.40238    0.57069\n",
      "  0.29321    1.0723   ]\n",
      "GloVe embedding for 'tutorial': [ 0.088317  0.14001  -0.61435  -0.77257  -0.45286  -0.95054  -0.15452\n",
      " -0.60561   0.37697   0.70308  -0.87226   0.25407   0.32701   0.40269\n",
      " -0.45457  -0.79642  -0.56227   0.51386   0.68498   0.23393   0.35828\n",
      "  0.76635  -0.12768   0.80817   0.4478    0.46906   0.30906  -0.40404\n",
      " -0.71069  -0.3132    1.1494    0.056342 -0.2083   -0.40506   0.58674\n",
      "  1.1443    0.43665   0.24287  -0.056824 -0.4656    0.62845  -0.40073\n",
      " -0.94485   1.1755   -0.10217   0.1407    1.6052    0.6785    0.038437\n",
      "  0.55929 ]\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings (this may take a while)\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Get vector for a word\n",
    "# print(\"GloVe embedding for 'hello':\", glove_vectors[\"hello\"])\n",
    "for word in vocabulary:\n",
    "    print(f\"GloVe embedding for '{word}':\", glove_vectors[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530292d",
   "metadata": {},
   "source": [
    "### 5. Contextual Embeddings (Overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec0ef5",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Generate word embeddings that change based on context, which is important for capturing the nuances in sentences.\n",
    "\n",
    "**Explanation**: Traditional embeddings like Word2Vec and GloVe assign each word a single vector, but contextual embeddings, used in models like BERT, generate different vectors for the same word based on surrounding words. This is particularly useful for disambiguating words with multiple meanings (e.g., \"bank\" as a financial institution vs. \"bank\" of a river). Contextual embeddings allow for richer representations, enhancing performance in tasks like sentiment analysis, translation, and question answering.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddddc90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual embeddings shape: torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and get embeddings\n",
    "input_text = \"NLP is fascinating!\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract contextual embeddings for each token\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(\"Contextual embeddings shape:\", embeddings.shape)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
