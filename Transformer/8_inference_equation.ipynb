{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923bcb4b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/8_inference_equation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65cc91",
   "metadata": {},
   "source": [
    "# Transformer Inference Process with Autoregressive Generation\n",
    "\n",
    "This notebook provides a mathematical representation of the Transformer inference process, focusing on how tokens are generated one at a time, \n",
    "with each new token depending on the previously generated tokens. This autoregressive process allows the model to generate sequences without \n",
    "access to the true target sequence, relying instead on its own predictions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f126ed6",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Encoder Output (Static for All Time Steps)\n",
    "\n",
    "The input sequence is encoded once, providing a constant representation that the decoder uses for each time step during inference.\n",
    "\n",
    "Given:\n",
    "- $ X = \\{x_1, x_2, \\dots, x_n\\} $, the **input sequence** of length $ n $.\n",
    "\n",
    "The encoder transforms $ X $ into an encoded representation $ Z_{\\text{encoder}} $:\n",
    "$$\n",
    "Z_{\\text{encoder}} = \\text{Encoder}(X)\n",
    "$$\n",
    "This output $ Z_{\\text{encoder}} $ remains constant for each generated token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90627b98",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Autoregressive Loop for Generating Tokens\n",
    "\n",
    "The model generates tokens one at a time in a loop, relying on its own predictions to continue generating the sequence.\n",
    "\n",
    "For each time step $ t = 1 $ to $ m $ (until reaching the desired output length or a stopping condition):\n",
    "\n",
    "### a. Decoder Input at Time Step $ t $\n",
    "- The decoder receives all previously generated tokens $ \\{ \\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_{t-1} \\} $ as input to predict the next token.\n",
    "- Formally, let $ \\hat{Y}_{\\text{input}}^{(t)} = \\{ \\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_{t-1} \\} $.\n",
    "\n",
    "### b. Decoder Output at Time Step $ t $\n",
    "- The decoder uses $ \\hat{Y}_{\\text{input}}^{(t)} $ and the encoder output $ Z_{\\text{encoder}} $ to produce an output representation:\n",
    "$$\n",
    "Z_{\\text{decoder}}^{(t)} = \\text{Decoder}(\\hat{Y}_{\\text{input}}^{(t)}, Z_{\\text{encoder}})\n",
    "$$\n",
    "\n",
    "### c. Prediction of Next Token at Time Step $ t $\n",
    "- The decoder output $ Z_{\\text{decoder}}^{(t)} $ is transformed into a probability distribution over the vocabulary using a softmax layer:\n",
    "$$\n",
    "\\hat{y}_t = \\text{softmax}(Z_{\\text{decoder}}^{(t)} W_O)\n",
    "$$\n",
    "where $ W_O $ is the learned output weight matrix.\n",
    "\n",
    "### d. Token Selection\n",
    "- Based on the probability distribution $ \\hat{y}_t $, the next token is selected (e.g., by choosing the most likely token, or by sampling from the distribution).\n",
    "- Append $ \\hat{y}_t $ to the generated sequence: $ \\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_t \\} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97903f",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Stopping Condition\n",
    "\n",
    "The loop continues until a stopping condition is met:\n",
    "- The model generates an **end-of-sequence** token, or\n",
    "- The sequence reaches a predefined **maximum length**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-section",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Overall Inference Process\n",
    "\n",
    "The overall inference process involves generating a sequence token by token using the autoregressive approach. The model starts with an initial input and generates tokens until a stopping condition is met.\n",
    "\n",
    "### Inference Loop\n",
    "1. Encode the input sequence $ X $ to obtain $ Z_{\\text{encoder}} $.\n",
    "2. Initialize the generated sequence with a start token.\n",
    "3. For each time step $ t $:\n",
    "   - Use the previously generated tokens to predict the next token.\n",
    "   - Append the predicted token to the generated sequence.\n",
    "   - Check the stopping condition (end-of-sequence token or maximum length).\n",
    "\n",
    "Repeat the process until the stopping condition is met.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
