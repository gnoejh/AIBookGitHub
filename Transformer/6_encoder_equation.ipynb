{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548ce3a1",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/6_encoder_equation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172ea9f",
   "metadata": {},
   "source": [
    "# Encoder Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2bbd3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Token Embedding and Positional Encoding\n",
    "\n",
    "### Token Embedding\n",
    "Given an input sequence of tokens represented by indices, we start by embedding these tokens into dense vector representations.\n",
    "\n",
    "Let:\n",
    "- The input token sequence be represented by indices $ \\text{Token Indices} \\in \\mathbb{R}^{n} $, where $ n $ is the sequence length (number of tokens).\n",
    "- The embedding matrix be $ W_E \\in \\mathbb{R}^{V \\times d} $, where $ V $ is the vocabulary size and $ d $ is the embedding dimension.\n",
    "\n",
    "The token embedding process maps each token index to a dense vector in $ \\mathbb{R}^{d} $:\n",
    "\n",
    "$$\n",
    "X = \\text{Token Indices} \\cdot W_E \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "### Positional Encoding\n",
    "Since the Transformer lacks inherent sequence order awareness, we add **positional encodings** to the embeddings.\n",
    "\n",
    "For each position $ i $ and each embedding dimension $ j $, the positional encoding is defined as:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{i, 2j} = \\sin\\left(\\frac{i}{10000^{\\frac{2j}{d}}}\\right), \\quad \\text{PE}_{i, 2j+1} = \\cos\\left(\\frac{i}{10000^{\\frac{2j}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "Adding positional encoding to the token embeddings gives:\n",
    "\n",
    "$$\n",
    "X_{\\text{input}} = X + \\text{PE} \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "where $ \\text{PE} \\in \\mathbb{R}^{n \\times d} $ is the positional encoding matrix, and $ X_{\\text{input}} $ is the input to the first encoder block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a45ae",
   "metadata": {},
   "source": [
    "## 2. Self-Attention Mechanism\n",
    "\n",
    "Self-attention enables each token to attend to every other token in the sequence, using three vector representations: **queries** $ Q $, **keys** $ K $, and **values** $ V $.\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "To compute self-attention, we perform the following steps:\n",
    "\n",
    "1. **Queries, Keys, and Values**:\n",
    "   Given input $ X $, we compute:\n",
    "   $$\n",
    "   Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "   $$\n",
    "   where $ W_Q $, $ W_K $, and $ W_V $ are learned weight matrices with dimensions $ d \\times k $, $ d \\times k $, and $ d \\times d $, respectively.\n",
    "\n",
    "2. **Attention Scores**:\n",
    "   Calculate the attention scores by taking the dot product of $ Q $ with $ K^T $, followed by scaling by $ \\frac{1}{\\sqrt{k}} $:\n",
    "   $$\n",
    "   \\text{Score Matrix} = \\frac{Q K^T}{\\sqrt{k}} \\in \\mathbb{R}^{n \\times n}\n",
    "   $$\n",
    "\n",
    "3. **Apply Softmax to Obtain Weights**:\n",
    "   Convert the scores into probabilities by applying the softmax function across each row:\n",
    "   $$\n",
    "   A = \\text{softmax}_{\\text{row}}\\left(\\frac{Q K^T}{\\sqrt{k}}\\right) \\in \\mathbb{R}^{n \\times n}\n",
    "   $$\n",
    "\n",
    "4. **Weighted Sum of Values**:\n",
    "   The final self-attention output for each token is a weighted sum of the values $ V $, where weights are given by the matrix $ A $:\n",
    "   $$\n",
    "   \\text{Self-Attention Output} = A V \\in \\mathbb{R}^{n \\times d}\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab884231",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention (MHA) Mechanism\n",
    "\n",
    "Multi-head attention allows the model to capture multiple types of relationships in parallel by using multiple self-attention heads.\n",
    "\n",
    "1. **Project Inputs for Each Head**:\n",
    "   For each of the $ h $ attention heads, we compute separate queries, keys, and values. Each head operates on a smaller dimension, where $ k = \\frac{d}{h} $. For head $ i $:\n",
    "   $$\n",
    "   Q^{(i)} = X W_Q^{(i)} \\in \\mathbb{R}^{n \\times k}, \\quad K^{(i)} = X W_K^{(i)} \\in \\mathbb{R}^{n \\times k}, \\quad V^{(i)} = X W_V^{(i)} \\in \\mathbb{R}^{n \\times k}\n",
    "   $$\n",
    "   where $ W_Q^{(i)}, W_K^{(i)}, W_V^{(i)} \\in \\mathbb{R}^{d \\times k} $.\n",
    "\n",
    "2. **Scaled Dot-Product Attention for Each Head**:\n",
    "   Each head performs self-attention independently using its own query, key, and value matrices:\n",
    "   $$\n",
    "   \\text{Head}^{(i)} = \\text{softmax}\\left(\\frac{Q^{(i)} {K^{(i)}}^T}{\\sqrt{k}}\\right) V^{(i)} \\in \\mathbb{R}^{n \\times k}\n",
    "   $$\n",
    "\n",
    "3. **Concatenate Heads and Apply Final Linear Projection**:\n",
    "   Once each head has produced an output, we concatenate the outputs of all heads along the last dimension:\n",
    "   $$\n",
    "   \\text{Concatenated Output} = \\text{Concat}(\\text{Head}^{(1)}, \\text{Head}^{(2)}, \\dots, \\text{Head}^{(h)}) \\in \\mathbb{R}^{n \\times d}\n",
    "   $$\n",
    "   Then, we apply a final linear transformation with a weight matrix $ W_O \\in \\mathbb{R}^{d \\times d} $:\n",
    "   $$\n",
    "   \\text{Multi-Head Attention Output} = \\text{Concatenated Output} \\cdot W_O \\in \\mathbb{R}^{n \\times d}\n",
    "   $$\n",
    "\n",
    "### Self-Attention vs. Cross-Attention\n",
    "\n",
    "- **Self-Attention**: In self-attention, the same input sequence provides queries, keys, and values, enabling tokens to attend to all other tokens within the same sequence.\n",
    "- **Cross-Attention**: In cross-attention (used in the decoder), the encoder’s output serves as the keys and values, while the decoder’s previous layer output serves as the queries. This allows the decoder to attend to encoder information relevant to each token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf066145",
   "metadata": {},
   "source": [
    "## 4. Encoder Block\n",
    "\n",
    "The encoder block combines multi-head self-attention, a feed-forward network, and residual connections with layer normalization. Each encoder block refines the input representation by adding contextual information at every layer.\n",
    "\n",
    "### Step-by-Step Encoder Block\n",
    "\n",
    "1. **Multi-Head Attention with Residual Connection**:\n",
    "   - The input $ X_{\\text{input}} $ (from embeddings plus positional encoding) goes through multi-head self-attention, producing an output that is then added back to $ X_{\\text{input}} $ as a residual connection, followed by layer normalization:\n",
    "   $$\n",
    "   Z_1 = \\text{LayerNorm}(X_{\\text{input}} + \\text{Multi-Head Attention}(X_{\\text{input}}, X_{\\text{input}}, X_{\\text{input}})) \\in \\mathbb{R}^{n \\times d}\n",
    "   $$\n",
    "\n",
    "2. **Feed-Forward Network with Residual Connection**:\n",
    "   - The output $ Z_1 $ is passed through a position-wise feed-forward network, followed by another residual connection and layer normalization:\n",
    "   $$\n",
    "   Z_2 = \\text{LayerNorm}(Z_1 + \\text{FFN}(Z_1)) \\in \\mathbb{R}^{n \\times d}\n",
    "   $$\n",
    "\n",
    "   The feed-forward network (FFN) consists of two linear transformations with a ReLU activation in between:\n",
    "   $$\n",
    "   \\text{FFN}(Z_1) = \\text{ReLU}(Z_1 W_1 + b_1) W_2 + b_2\n",
    "   $$\n",
    "   where:\n",
    "   - $ W_1 \\in \\mathbb{R}^{d \\times d_{ff}} $ and $ W_2 \\in \\mathbb{R}^{d_{ff} \\times d} $,\n",
    "   - $ b_1 \\in \\mathbb{R}^{d_{ff}} $ and $ b_2 \\in \\mathbb{R}^{d} $,\n",
    "   - and $ d_{ff} $ is the hidden layer dimension in the feed-forward network.\n",
    "\n",
    "3. **Final Output of Encoder Block**:\n",
    "   - The final output $ Z_2 $ is the output of the encoder block, which serves as input to the next encoder block (if present) or to the decoder (in the full Transformer model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-section",
   "metadata": {},
   "source": [
    "## 5. Overall Encoder Process\n",
    "\n",
    "The overall encoder process involves passing the input sequence through multiple encoder blocks to generate a refined representation.\n",
    "\n",
    "## Encoder Loop\n",
    "1. Embed the input tokens and add positional encodings.\n",
    "2. For each encoder block:\n",
    "   - Apply multi-head self-attention with residual connection and layer normalization.\n",
    "   - Apply feed-forward network with residual connection and layer normalization.\n",
    "\n",
    "The final output of the last encoder block is the encoded representation $ Z_{\\text{encoder}} $.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
