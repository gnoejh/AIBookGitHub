{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1a3c4b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/11_decoder_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394d619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144dcee",
   "metadata": {},
   "source": [
    "# Attention, Positional Encoding, Decoder Block\n",
    "- https://newsletter.theaiedge.io/p/the-transformer-architecture-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f5ee6",
   "metadata": {},
   "source": [
    "### 1. Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524eb2ed",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Enable models to focus on relevant parts of input sequences, essential for handling longer dependencies.\n",
    "\n",
    "**Explanation**: \n",
    "- Attention allows a model to “attend” to specific input parts when generating each output. It’s especially useful for tasks requiring selective referencing of input tokens (e.g., translation, summarization).\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210ffad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output: tensor([[[-0.4368,  1.1387, -0.0765, -0.4963,  0.6597,  0.8202,  1.0736,\n",
      "          -0.4982],\n",
      "         [-0.2681,  1.0187, -0.1171, -0.4373,  0.7556,  0.8140,  0.9195,\n",
      "          -0.8436],\n",
      "         [-0.7186,  1.3628,  0.2090, -0.7164, -0.0629,  1.0361,  2.0843,\n",
      "           1.2794]],\n",
      "\n",
      "        [[ 0.5862,  1.1620, -0.6905, -0.6694, -0.0634,  0.3326,  0.6173,\n",
      "          -0.4638],\n",
      "         [ 0.6313,  1.2360, -0.8401, -0.5667, -0.3285,  0.3045,  0.5720,\n",
      "          -0.7707],\n",
      "         [ 0.2021,  0.9877, -0.2883, -0.7976,  0.1314,  0.2375,  0.7687,\n",
      "           0.0092]],\n",
      "\n",
      "        [[-0.6210,  1.3632, -0.9874,  0.0380, -0.3837, -1.3748, -0.1222,\n",
      "           0.1058],\n",
      "         [-0.7731,  1.2723, -0.4795,  0.2371, -0.2152, -0.6736,  0.1619,\n",
      "           0.1475],\n",
      "         [-1.0058,  1.0546,  0.2966,  0.3289, -0.0406,  0.5681,  0.5247,\n",
      "           0.1245]],\n",
      "\n",
      "        [[-0.9264, -0.6532,  0.1170, -0.4256,  0.2450, -0.5173, -0.0366,\n",
      "           0.8772],\n",
      "         [-0.4929, -0.5228,  0.1945,  0.4260,  0.3900, -0.1580, -0.0447,\n",
      "           0.3971],\n",
      "         [-0.1784, -0.4342,  0.3843,  1.2979,  0.5942, -0.0634, -0.2684,\n",
      "          -0.0574]],\n",
      "\n",
      "        [[ 0.9646, -0.6407, -1.7468, -0.7346, -1.1641,  0.3579,  0.3803,\n",
      "           0.9438],\n",
      "         [ 1.0502,  0.0170, -1.3603, -0.5342, -1.3995,  0.1868,  0.5778,\n",
      "           0.2111],\n",
      "         [ 0.7639,  1.0663, -0.6666, -0.1357, -1.0971,  0.0623,  0.7812,\n",
      "          -0.7392]]])\n",
      "Attention Weights: tensor([[[0.2744, 0.2447, 0.4808],\n",
      "         [0.1548, 0.2894, 0.5558],\n",
      "         [0.7184, 0.2463, 0.0353]],\n",
      "\n",
      "        [[0.2422, 0.4911, 0.2666],\n",
      "         [0.1967, 0.6202, 0.1831],\n",
      "         [0.4241, 0.2355, 0.3404]],\n",
      "\n",
      "        [[0.0895, 0.0848, 0.8258],\n",
      "         [0.2039, 0.2897, 0.5064],\n",
      "         [0.4866, 0.4986, 0.0148]],\n",
      "\n",
      "        [[0.1280, 0.2148, 0.6573],\n",
      "         [0.2806, 0.3745, 0.3449],\n",
      "         [0.2502, 0.6353, 0.1145]],\n",
      "\n",
      "        [[0.1695, 0.0162, 0.8144],\n",
      "         [0.0419, 0.3563, 0.6019],\n",
      "         [0.1587, 0.7167, 0.1247]]])\n"
     ]
    }
   ],
   "source": [
    "# Simple attention mechanism\n",
    "def attention(query, key, value):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(key.size(-1), dtype=torch.float32))\n",
    "    attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example inputs\n",
    "query = torch.randn(5, 3, 8)  # Batch size = 5, Sequence length = 3, Embedding size = 8\n",
    "key = torch.randn(5, 3, 8)\n",
    "value = torch.randn(5, 3, 8)\n",
    "\n",
    "# Apply attention\n",
    "output, weights = attention(query, key, value)\n",
    "print(\"Attention Output:\", output)\n",
    "print(\"Attention Weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119e732",
   "metadata": {},
   "source": [
    "### 2. Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0607cb9d",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Understand how self-attention allows a model to relate each word in a sentence to every other word, helping capture context effectively.\n",
    "\n",
    "**Explanation**:\n",
    "   - **Queries, Keys, and Values**: In self-attention, each word is represented by three vectors: a query, a key, and a value.\n",
    "   - **Dot Product and Scaling**: The query and key vectors are multiplied to determine attention scores, representing the similarity between words. These scores are scaled and then passed through a softmax function to obtain attention weights.\n",
    "   - **Weighted Sum**: Each word’s final representation is the sum of all value vectors, weighted by attention scores, allowing the model to focus on relevant words in context.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c70684",
   "metadata": {},
   "source": [
    "### 3. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619332e8",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Enhance the model’s ability to capture diverse word relationships by using multiple attention heads.\n",
    "\n",
    "**Explanation**:\n",
    "   - **Multiple Heads**: Multi-head attention applies self-attention multiple times in parallel, each with different learned projections. This allows the model to capture various types of dependencies, such as grammatical or semantic.\n",
    "   - **Concatenation and Linear Transformation**: Each head’s output is concatenated and passed through a linear layer, blending the insights from all heads into a single representation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8561afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Output: tensor([[[ 2.2437e-01, -6.2121e-02,  3.3677e-01,  3.4617e-01, -1.2561e-01,\n",
      "          -1.4320e-02, -3.0945e-01, -1.2908e-01,  9.7997e-02, -1.2948e-01,\n",
      "           1.8598e-01, -1.2567e-01, -1.3778e-01, -1.3313e-01,  4.0892e-01,\n",
      "           2.7462e-01],\n",
      "         [ 2.7985e-01, -9.5385e-02,  2.7260e-01,  3.0782e-01, -8.0252e-03,\n",
      "          -3.4150e-02, -3.4385e-01, -8.9990e-02,  1.1554e-01, -1.2106e-01,\n",
      "           1.8222e-01, -1.0269e-01, -6.9136e-02, -2.5624e-01,  3.5380e-01,\n",
      "           3.0314e-01],\n",
      "         [ 2.1652e-01, -1.2097e-01,  1.3035e-01,  2.7102e-01, -7.1895e-02,\n",
      "          -1.0088e-01, -2.7516e-01, -1.3812e-01,  1.3554e-01, -1.2206e-01,\n",
      "           8.2788e-02, -1.4428e-01, -9.2552e-02, -1.0975e-01,  1.7394e-01,\n",
      "           3.4829e-01],\n",
      "         [-1.9586e-02, -2.2503e-01, -1.5532e-02,  2.9430e-01,  3.6480e-04,\n",
      "          -2.1269e-01, -1.4501e-01, -1.2479e-01,  1.9416e-01,  3.2542e-02,\n",
      "           2.2505e-02, -1.6535e-01, -2.5836e-01, -9.1432e-02,  1.7192e-01,\n",
      "           5.2043e-01],\n",
      "         [ 1.6081e-01, -1.7806e-01,  1.6312e-01,  4.2157e-01, -2.8406e-01,\n",
      "          -1.0672e-02, -2.8795e-01, -2.1235e-01,  8.4427e-02, -1.0675e-02,\n",
      "          -5.1514e-02, -1.7657e-01, -2.7666e-01,  4.2631e-02,  1.7129e-01,\n",
      "           4.2254e-01]],\n",
      "\n",
      "        [[ 4.2157e-01, -1.1872e-01, -1.5393e-01,  6.2533e-02,  8.6977e-02,\n",
      "           1.2595e-01, -1.8865e-01, -2.6413e-01,  4.3978e-02,  2.9554e-02,\n",
      "           2.0665e-01, -1.0954e-01,  7.6943e-02, -3.2371e-01, -6.9507e-02,\n",
      "           6.3295e-02],\n",
      "         [ 3.7253e-01, -1.6955e-01, -3.2963e-01, -1.5967e-01,  1.3271e-01,\n",
      "           1.3519e-01, -1.3494e-01, -2.4910e-01,  1.0906e-01,  3.2366e-02,\n",
      "           1.8087e-01, -1.2794e-01,  2.4245e-01, -2.3763e-01, -5.1852e-02,\n",
      "           3.5663e-02],\n",
      "         [ 4.3904e-01, -1.7239e-01, -1.4288e-01, -6.3411e-02,  1.0302e-01,\n",
      "           1.5524e-01, -1.8393e-01, -3.1132e-01,  2.9448e-02,  1.4243e-03,\n",
      "           2.4492e-01, -1.5296e-01,  1.9728e-01, -3.4888e-01, -8.2608e-02,\n",
      "          -1.0724e-02],\n",
      "         [ 2.9199e-01, -1.4194e-01, -1.8019e-01, -1.3827e-01,  4.9957e-02,\n",
      "           1.8130e-01, -1.5712e-01, -2.8461e-01,  1.1172e-01,  6.7207e-02,\n",
      "           1.3327e-01, -8.8209e-02,  1.5696e-01, -2.2857e-01, -5.9026e-02,\n",
      "           1.4172e-01],\n",
      "         [ 4.5473e-01, -1.4471e-01, -1.7445e-01, -3.1990e-02,  1.3704e-01,\n",
      "           1.3445e-01, -1.8466e-01, -2.6435e-01,  6.0686e-02,  1.5973e-02,\n",
      "           2.3683e-01, -1.2030e-01,  1.7896e-01, -3.4177e-01, -5.3036e-02,\n",
      "           1.5733e-02]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_size % num_heads == 0\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Linear layers for queries, keys, values\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        # Final linear layer after concatenation\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size, seq_len, embed_size = query.size()\n",
    "\n",
    "        # Transform inputs into multiple heads\n",
    "        query = self.query(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = self.key(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = self.value(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attention = torch.matmul(attn_weights, value)\n",
    "        \n",
    "        # Concatenate heads and pass through the final linear layer\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_size)\n",
    "        return self.fc_out(attention)\n",
    "\n",
    "# Example usage\n",
    "embed_size = 16\n",
    "num_heads = 4\n",
    "x = torch.randn(2, 5, embed_size)  # Batch size = 2, Sequence length = 5\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(embed_size, num_heads)\n",
    "output = multi_head_attention(x, x, x)\n",
    "print(\"Multi-Head Attention Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029571e9",
   "metadata": {},
   "source": [
    "### 4. Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23d337",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Introduce positional information into word embeddings, as Transformers process words in parallel without inherent sequence order.\n",
    "\n",
    "**Explanation**:\n",
    "   - Since Transformers do not process words sequentially, positional encodings are added to the embeddings to give the model information about the order of words.\n",
    "   - **Sine and Cosine Functions**: Positional encodings are computed using sine and cosine functions with varying frequencies, creating unique patterns for each position in the sequence.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b00f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Output: tensor([[[-0.3780,  2.4281,  1.1620,  2.2261,  0.3933,  3.6151, -0.3845,\n",
      "          -0.5912, -0.7609,  1.5486, -0.8610,  3.1962, -0.3819,  0.4404,\n",
      "           0.8772,  3.1241],\n",
      "         [ 0.5593,  1.5919, -0.8094,  0.7374, -0.2747,  0.1606, -0.4240,\n",
      "           1.1753,  0.2719, -0.4462, -0.2870,  0.7103,  0.2695, -0.2540,\n",
      "          -0.6235,  0.8658],\n",
      "         [ 0.5411,  1.9566, -0.7689,  2.7376,  2.2273,  1.1636, -0.0636,\n",
      "           0.1820, -0.6461,  1.4622, -1.7594,  0.7083, -1.5733,  1.4677,\n",
      "           0.6176,  0.3939],\n",
      "         [ 1.2280,  0.0967,  0.8716,  1.9796,  0.0933,  1.1175, -0.6185,\n",
      "          -0.5824, -1.0138,  1.4257,  0.7839,  0.7707,  0.5574,  2.9357,\n",
      "           1.6869,  1.6726],\n",
      "         [-1.1834, -0.7026, -0.2265,  1.2105,  0.1016,  1.0678,  0.4235,\n",
      "           1.6662, -0.5722, -0.1417,  0.7989, -0.1837, -0.7619, -0.9646,\n",
      "          -0.0545,  1.5960],\n",
      "         [-2.2983,  0.0287,  0.1830,  1.7864,  1.2916,  1.4007, -0.3100,\n",
      "           1.2188,  0.2925,  2.9277,  1.1332,  0.9386,  0.8477,  1.1623,\n",
      "          -0.6680,  1.1194],\n",
      "         [-1.2129,  0.9954, -1.2279,  1.0575, -0.2058,  1.3488,  0.2323,\n",
      "           0.4392, -1.2818,  2.6777,  1.3724, -0.2276,  1.0322,  1.9673,\n",
      "          -0.4938,  1.1586],\n",
      "         [ 1.5300,  0.3976,  1.3411,  2.3445,  0.4774, -0.8585,  0.7033,\n",
      "           1.2848, -1.2468,  1.3609, -0.3387,  0.9692, -0.2681,  1.9368,\n",
      "           1.4323,  0.3833],\n",
      "         [ 0.7694, -0.6122,  0.7617, -0.0737, -0.4722,  0.4023,  1.4398,\n",
      "           0.6276, -0.3635,  1.2113, -0.0453,  1.9992,  0.7762, -0.4039,\n",
      "          -1.1257,  0.3573],\n",
      "         [-0.0114, -1.7579, -1.9106,  0.1397,  0.9022,  1.7287,  0.2467,\n",
      "           2.3924, -1.0221,  1.8560,  2.1829, -0.8148, -1.5212, -0.1417,\n",
      "           2.3485,  1.2443]]])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, embed_size, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / embed_size)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i) / embed_size)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "embed_size = 16\n",
    "seq_len = 10\n",
    "pos_encoding = PositionalEncoding(embed_size)\n",
    "x = torch.randn(1, seq_len, embed_size)\n",
    "output = pos_encoding(x)\n",
    "print(\"Positional Encoding Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48aa01e",
   "metadata": {},
   "source": [
    "### 5. Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213fb7f",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Understand how the Transformer decoder combines self-attention, encoder-decoder attention, and feedforward layers, the fundamental units in a Transformer.\n",
    "\n",
    "**Explanation**:\n",
    "   - **Self-Attention Layer**: The decoder starts with a masked multi-head attention layer, allowing the model to attend to relevant parts of the input while preventing attending to future tokens.\n",
    "   - **Encoder-Decoder Attention**: The second attention layer attends to the encoder’s output, allowing the decoder to focus on relevant parts of the input sequence.\n",
    "   - **Add & Norm**: After each attention layer, a residual connection is added, followed by layer normalization to stabilize learning.\n",
    "   - **Feedforward Layer**: The attention output is passed through a fully connected layer for further processing.\n",
    "   - **Final Add & Norm**: A second residual connection and layer normalization complete the block, making the Transformer’s decoder robust to varying input sequences.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a25968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Decoder Block Output: tensor([[[-0.0504,  0.2236, -1.9815,  1.2312,  0.4428, -0.4968, -1.0361,\n",
      "          -0.2568,  0.3426,  0.2296,  2.6510,  0.5192, -0.9043, -0.0385,\n",
      "          -0.0826, -0.7931],\n",
      "         [ 1.0573,  0.5727, -1.7287,  0.5727,  1.4477,  0.3912,  0.1467,\n",
      "          -1.9621, -0.3355, -0.2418, -0.9247,  0.4133, -0.1908, -1.3024,\n",
      "           0.9555,  1.1290],\n",
      "         [ 2.4449, -1.0871, -0.1706, -1.1433,  0.0641, -0.1058,  0.4111,\n",
      "           1.0630, -0.8163, -1.0199, -1.0326, -1.0673,  0.8826, -0.0682,\n",
      "           0.5570,  1.0883],\n",
      "         [-0.2866, -1.5279, -0.0509,  1.1634, -0.2142,  0.0341,  0.3061,\n",
      "          -0.3619,  1.1847, -1.8800, -0.9231,  0.6996, -1.1453,  0.3403,\n",
      "           1.9252,  0.7363],\n",
      "         [ 1.6752, -1.7031, -0.9129,  0.5246, -1.7744, -0.1799,  1.0348,\n",
      "          -0.4354,  0.4402, -0.8662,  1.2825,  0.2872, -0.5341, -0.1110,\n",
      "          -0.0582,  1.3306]],\n",
      "\n",
      "        [[ 0.5203, -0.7214, -1.0901,  0.0691, -0.6485, -1.1235,  0.8946,\n",
      "           1.4188, -0.6328, -2.3706, -0.0839,  0.4195,  0.9146,  0.6499,\n",
      "           1.3746,  0.4093],\n",
      "         [ 0.8739, -0.1527, -1.2784,  0.7898, -1.4826,  0.7749,  1.0150,\n",
      "          -2.2100, -0.1974,  1.8265, -0.0441,  0.2064,  0.3914, -0.3098,\n",
      "          -0.6272,  0.4243],\n",
      "         [ 0.6332, -1.2298, -2.1617,  0.0870,  0.3797, -0.7676,  0.9225,\n",
      "          -0.5871,  1.4400, -0.6289, -1.1245,  0.9785, -0.0028,  0.7855,\n",
      "           1.4610, -0.1848],\n",
      "         [-0.0907, -1.9132, -1.7509,  1.3771,  0.0161, -0.8506,  1.5766,\n",
      "          -1.2041,  0.3364,  0.6250,  0.1398, -0.5414,  1.1519,  0.1079,\n",
      "           0.6411,  0.3789],\n",
      "         [-0.0442, -0.6170, -2.0674, -0.2116,  0.6279,  1.1796,  0.9987,\n",
      "          -0.8217, -0.8910, -0.4051,  0.1345,  0.5455,  0.5375, -0.7950,\n",
      "          -0.4946,  2.3240]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, forward_expansion):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.cross_attention = MultiHeadAttention(embed_size, num_heads)  # Cross-attention layer\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.norm4 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        self_attention, _ = self.self_attention(x, x, x)\n",
    "        x = self.norm1(self_attention + x)  # Add & Norm\n",
    "        cross_attention, _ = self.cross_attention(x, enc_out, enc_out)  # Cross-attention\n",
    "        x = self.norm2(cross_attention + x)  # Add & Norm\n",
    "        encoder_decoder_attention, _ = self.encoder_decoder_attention(x, enc_out, enc_out)\n",
    "        x = self.norm3(encoder_decoder_attention + x)  # Add & Norm\n",
    "        forward = self.feed_forward(x)\n",
    "        x = self.norm4(forward + x)  # Add & Norm\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "embed_size = 16\n",
    "num_heads = 4\n",
    "forward_expansion = 4\n",
    "decoder_block = TransformerDecoderBlock(embed_size, num_heads, forward_expansion)\n",
    "x = torch.randn(2, 5, embed_size)\n",
    "enc_out = torch.randn(2, 5, embed_size)\n",
    "output = decoder_block(x, enc_out)\n",
    "print(\"Transformer Decoder Block Output:\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
