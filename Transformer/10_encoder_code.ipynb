{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e036f9",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/10_encoder_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7ea551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144dcee",
   "metadata": {},
   "source": [
    "# Attention, Positional Encoding, Encoder Block\n",
    "- https://newsletter.theaiedge.io/p/the-transformer-architecture-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f5ee6",
   "metadata": {},
   "source": [
    "### 1. Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524eb2ed",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Enable models to focus on relevant parts of input sequences, essential for handling longer dependencies.\n",
    "\n",
    "**Explanation**: \n",
    "- Attention allows a model to “attend” to specific input parts when generating each output. It’s especially useful for tasks requiring selective referencing of input tokens (e.g., translation, summarization).\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210ffad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output: tensor([[[ 9.6740e-01, -3.0352e-02,  1.1156e-01,  1.9646e-01, -6.1691e-01,\n",
      "           7.4783e-01, -5.7474e-02,  1.5947e-02],\n",
      "         [ 8.2061e-01, -5.0180e-02, -9.8467e-01, -1.8920e-01, -1.6075e+00,\n",
      "           1.0993e+00, -2.2059e-01, -6.5414e-01],\n",
      "         [ 1.3272e+00,  3.0562e-01,  7.5185e-01,  1.5077e-01, -2.8726e-01,\n",
      "           3.6313e-01,  3.9887e-01,  1.0497e+00]],\n",
      "\n",
      "        [[ 3.4469e-01,  2.5661e-01,  5.7576e-01,  6.3296e-01,  2.7943e-01,\n",
      "          -7.2584e-01,  1.0602e+00,  2.8766e-01],\n",
      "         [ 8.6278e-01,  2.7242e-01,  2.6027e-01,  2.0195e-01,  4.0122e-01,\n",
      "          -5.4352e-01,  8.1289e-01,  3.4031e-01],\n",
      "         [ 1.6067e+00,  2.1239e-01,  1.1643e-01, -2.6463e-01,  8.8893e-01,\n",
      "          -3.1012e-01,  2.2290e-01,  5.0179e-01]],\n",
      "\n",
      "        [[ 1.8186e-01, -2.7800e-01, -7.5871e-02,  1.2186e+00, -1.1200e+00,\n",
      "          -1.0662e+00, -1.0065e+00, -1.1341e+00],\n",
      "         [-1.8343e-02, -1.4870e-03, -7.7953e-02,  1.2999e+00, -1.2093e+00,\n",
      "          -9.5335e-01, -9.0747e-01, -1.3500e+00],\n",
      "         [-4.4585e-01,  4.1254e-01, -3.1792e-02,  1.4485e+00, -1.4309e+00,\n",
      "          -7.8220e-01, -7.2500e-01, -1.8361e+00]],\n",
      "\n",
      "        [[-1.4128e-01,  1.3196e-01,  3.2802e-01,  2.1627e-01,  1.0854e+00,\n",
      "           2.4447e-01,  3.4642e-01,  5.5213e-01],\n",
      "         [ 1.0815e-02, -9.7129e-02,  2.9182e-01,  5.6970e-02,  1.1064e+00,\n",
      "           4.4050e-02,  5.3507e-01,  6.2482e-01],\n",
      "         [-1.6502e-01,  2.1587e-01,  3.2903e-01,  5.2677e-01,  1.3094e+00,\n",
      "           7.6540e-01,  6.0693e-02, -5.1307e-01]],\n",
      "\n",
      "        [[ 5.0639e-01,  5.1775e-02, -5.6245e-01,  8.1565e-01, -4.4547e-02,\n",
      "          -1.0663e+00,  7.7811e-02,  1.0227e+00],\n",
      "         [ 3.1625e-01,  4.7182e-01, -4.0297e-01,  3.7881e-01, -3.6461e-02,\n",
      "          -7.5920e-01,  2.1683e-01,  1.0660e+00],\n",
      "         [ 1.5350e-01,  3.2005e-01, -4.5797e-01, -2.3131e-01, -2.0001e-01,\n",
      "          -5.2933e-01, -2.6787e-01,  1.2159e+00]]])\n",
      "Attention Weights: tensor([[[0.4109, 0.3931, 0.1960],\n",
      "         [0.0924, 0.7852, 0.1224],\n",
      "         [0.4548, 0.1230, 0.4222]],\n",
      "\n",
      "        [[0.0470, 0.2147, 0.7383],\n",
      "         [0.3711, 0.1630, 0.4659],\n",
      "         [0.5401, 0.4000, 0.0599]],\n",
      "\n",
      "        [[0.1629, 0.1480, 0.6890],\n",
      "         [0.1563, 0.3723, 0.4714],\n",
      "         [0.0924, 0.8238, 0.0838]],\n",
      "\n",
      "        [[0.2452, 0.1015, 0.6532],\n",
      "         [0.4220, 0.0272, 0.5508],\n",
      "         [0.1104, 0.7167, 0.1729]],\n",
      "\n",
      "        [[0.3472, 0.4801, 0.1726],\n",
      "         [0.2978, 0.3547, 0.3476],\n",
      "         [0.4635, 0.2082, 0.3284]]])\n"
     ]
    }
   ],
   "source": [
    "# Simple attention mechanism\n",
    "def attention(query, key, value):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(key.size(-1), dtype=torch.float32))\n",
    "    attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example inputs\n",
    "query = torch.randn(5, 3, 8)  # Batch size = 5, Sequence length = 3, Embedding size = 8\n",
    "key = torch.randn(5, 3, 8)\n",
    "value = torch.randn(5, 3, 8)\n",
    "\n",
    "# Apply attention\n",
    "output, weights = attention(query, key, value)\n",
    "print(\"Attention Output:\", output)\n",
    "print(\"Attention Weights:\", weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119e732",
   "metadata": {},
   "source": [
    "### 2. Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0607cb9d",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Understand how self-attention allows a model to relate each word in a sentence to every other word, helping capture context effectively.\n",
    "\n",
    "**Explanation**:\n",
    "   - **Queries, Keys, and Values**: In self-attention, each word is represented by three vectors: a query, a key, and a value.\n",
    "   - **Dot Product and Scaling**: The query and key vectors are multiplied to determine attention scores, representing the similarity between words. These scores are scaled and then passed through a softmax function to obtain attention weights.\n",
    "   - **Weighted Sum**: Each word’s final representation is the sum of all value vectors, weighted by attention scores, allowing the model to focus on relevant words in context.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2572189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output: tensor([[[ 0.0643,  0.3878, -0.9651, -0.1585,  0.2823,  0.5144,  0.4199,\n",
      "           1.3144],\n",
      "         [-1.5024,  0.9853,  0.1165, -0.4015, -0.5092,  0.0443,  0.7778,\n",
      "          -0.2300],\n",
      "         [-0.5930,  0.6945, -0.6236, -0.4940, -0.0053,  0.6527,  0.2250,\n",
      "           0.5691],\n",
      "         [-1.0035,  1.0931, -0.1644, -0.6018,  0.1497,  0.8812,  0.3046,\n",
      "           0.4224]],\n",
      "\n",
      "        [[-1.1846, -0.6857, -0.8063,  0.5842, -0.5376,  0.2940, -0.3155,\n",
      "          -0.2056],\n",
      "         [-1.2179, -0.6285, -0.5397,  0.7711, -0.2013, -0.3624, -0.6052,\n",
      "           0.1188],\n",
      "         [-0.9526, -0.6172, -0.5162,  0.8198, -0.1445, -0.4041, -0.1650,\n",
      "           0.2313],\n",
      "         [-0.5575, -0.6047, -0.5012,  0.8782, -0.0858, -0.4164,  0.5090,\n",
      "           0.3747]]])\n",
      "Attention Weights: tensor([[[0.1811, 0.1012, 0.7108, 0.0070],\n",
      "         [0.0116, 0.4177, 0.0339, 0.5368],\n",
      "         [0.2914, 0.3495, 0.3362, 0.0229],\n",
      "         [0.0639, 0.5883, 0.1632, 0.1845]],\n",
      "\n",
      "        [[0.2246, 0.4163, 0.1650, 0.1941],\n",
      "         [0.1776, 0.2188, 0.4897, 0.1139],\n",
      "         [0.1120, 0.1825, 0.4124, 0.2931],\n",
      "         [0.0187, 0.1429, 0.2736, 0.5648]]])\n"
     ]
    }
   ],
   "source": [
    "# Self-attention function\n",
    "def self_attention(query, key, value):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example inputs\n",
    "query = torch.randn(2, 4, 8)  # Batch size = 2, Sequence length = 4, Embedding size = 8\n",
    "key = torch.randn(2, 4, 8)\n",
    "value = torch.randn(2, 4, 8)\n",
    "\n",
    "# Apply self-attention\n",
    "output, attn_weights = self_attention(query, key, value)\n",
    "print(\"Self-Attention Output:\", output)\n",
    "print(\"Attention Weights:\", attn_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c70684",
   "metadata": {},
   "source": [
    "### 3. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619332e8",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Enhance the model’s ability to capture diverse word relationships by using multiple attention heads.\n",
    "\n",
    "**Explanation**:\n",
    "   - **Multiple Heads**: Multi-head attention applies self-attention multiple times in parallel, each with different learned projections. This allows the model to capture various types of dependencies, such as grammatical or semantic.\n",
    "   - **Concatenation and Linear Transformation**: Each head’s output is concatenated and passed through a linear layer, blending the insights from all heads into a single representation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8561afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Output: tensor([[[ 0.2883, -0.0666,  0.0324,  0.0903, -0.0224,  0.1709,  0.3777,\n",
      "          -0.1263, -0.1861,  0.1183,  0.0483,  0.0571,  0.3380, -0.0030,\n",
      "          -0.0444,  0.1921],\n",
      "         [ 0.0937, -0.2319, -0.2270,  0.2127, -0.3615,  0.1010,  0.1613,\n",
      "           0.2324, -0.1731,  0.1133,  0.1282,  0.1121,  0.1108,  0.0730,\n",
      "          -0.0035,  0.0394],\n",
      "         [ 0.1270, -0.0730, -0.0799,  0.1946, -0.3110, -0.0063,  0.2045,\n",
      "           0.2427, -0.1767,  0.0796,  0.0379,  0.2217,  0.1492,  0.1154,\n",
      "           0.0604,  0.0234],\n",
      "         [ 0.4188,  0.0238,  0.2415,  0.1716,  0.0192,  0.0526,  0.4497,\n",
      "          -0.1370, -0.0547,  0.2190,  0.1902,  0.1459,  0.3794,  0.0985,\n",
      "           0.0621,  0.1025],\n",
      "         [-0.0127, -0.2505, -0.2466,  0.1567, -0.4309,  0.0406,  0.1680,\n",
      "           0.2939, -0.2072,  0.0328,  0.0527,  0.1321,  0.0940,  0.0553,\n",
      "           0.0515,  0.1003]],\n",
      "\n",
      "        [[-0.1013, -0.0444, -0.0998,  0.1317, -0.4518, -0.1567,  0.0364,\n",
      "           0.2895,  0.0336,  0.2601,  0.2905,  0.1132,  0.0335,  0.0057,\n",
      "           0.0021,  0.1408],\n",
      "         [-0.1686, -0.0575, -0.1396,  0.1369, -0.5500, -0.1330,  0.0538,\n",
      "           0.3536,  0.0445,  0.3017,  0.3045,  0.0722,  0.0310,  0.0423,\n",
      "           0.0619,  0.1111],\n",
      "         [-0.1547,  0.0043, -0.1180,  0.1702, -0.4416, -0.1854,  0.0451,\n",
      "           0.2791,  0.0634,  0.2185,  0.3395,  0.3209, -0.0130,  0.0985,\n",
      "           0.0597, -0.0143],\n",
      "         [-0.0620, -0.1832, -0.2277,  0.1156, -0.4324, -0.0390,  0.0674,\n",
      "           0.2324,  0.0989,  0.2693,  0.2950,  0.1469,  0.0895,  0.1720,\n",
      "           0.0187, -0.0101],\n",
      "         [-0.0694, -0.1053, -0.2065,  0.2382, -0.5210, -0.1181,  0.1080,\n",
      "           0.3345,  0.1577,  0.3122,  0.3553,  0.1451,  0.0796,  0.0286,\n",
      "           0.0597,  0.1117]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_size % num_heads == 0\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Linear layers for queries, keys, values\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        # Final linear layer after concatenation\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.size()\n",
    "\n",
    "        # Transform inputs into multiple heads\n",
    "        query = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attention = torch.matmul(attn_weights, value)\n",
    "        \n",
    "        # Concatenate heads and pass through the final linear layer\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_size)\n",
    "        return self.fc_out(attention)\n",
    "\n",
    "# Example usage\n",
    "embed_size = 16\n",
    "num_heads = 4\n",
    "x = torch.randn(2, 5, embed_size)  # Batch size = 2, Sequence length = 5\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(embed_size, num_heads)\n",
    "output = multi_head_attention(x)\n",
    "print(\"Multi-Head Attention Output:\", output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029571e9",
   "metadata": {},
   "source": [
    "### 4. Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23d337",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Introduce positional information into word embeddings, as Transformers process words in parallel without inherent sequence order.\n",
    "\n",
    "**Explanation**:\n",
    "   - Since Transformers do not process words sequentially, positional encodings are added to the embeddings to give the model information about the order of words.\n",
    "   - **Sine and Cosine Functions**: Positional encodings are computed using sine and cosine functions with varying frequencies, creating unique patterns for each position in the sequence.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b00f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Output: tensor([[[-0.0142,  0.6519, -0.4046,  1.5159,  0.5459,  1.1051,  0.9110,\n",
      "           3.0094,  1.1891,  2.7101,  0.0316,  1.0957,  0.3445,  0.5408,\n",
      "           0.3892,  0.4563],\n",
      "         [ 1.9320,  0.4930,  0.7246,  1.5846,  0.6008,  1.0921,  0.3042,\n",
      "           1.9109,  0.4135,  0.1733, -1.7470,  0.5978, -0.9461,  1.1420,\n",
      "          -0.3217,  1.4612],\n",
      "         [ 2.0854,  0.3954,  2.5952,  2.0235, -1.1602,  1.6995,  0.7881,\n",
      "          -0.0250,  0.0606,  2.2339, -1.9072,  1.5465, -0.0992,  1.2900,\n",
      "           0.1494,  0.2316],\n",
      "         [ 1.0135,  0.3130, -0.4570, -0.3857,  0.6012,  0.7225,  1.1795,\n",
      "           2.0650,  0.0155,  0.5062, -0.4983,  1.7248, -0.3680,  1.2916,\n",
      "          -0.9639,  2.2793],\n",
      "         [-0.3510,  0.4673,  0.3528,  0.4085, -0.5552,  1.6738, -0.1249,\n",
      "           1.5577,  1.7393,  1.4887, -1.9180,  2.1472, -0.2435,  0.0221,\n",
      "           0.5600,  0.5158],\n",
      "         [-1.5962, -1.0358,  0.3558,  2.9229,  0.7078,  4.4950, -0.6757,\n",
      "           1.4326,  1.9591,  2.2441,  0.1618,  1.6166,  0.7926,  1.3273,\n",
      "           1.3135, -1.2747],\n",
      "         [-1.3585,  0.4492,  0.1457,  2.8692,  1.7540,  0.3940,  1.4533,\n",
      "           2.6290, -2.9457,  0.4646,  1.0487, -1.4302,  1.0282,  0.7820,\n",
      "           0.2590,  1.5984],\n",
      "         [ 0.5611,  0.3367,  0.6155,  1.9040, -0.8517, -0.5083, -1.0634,\n",
      "          -0.4541,  0.1752,  0.0585, -0.7970,  0.7046,  0.6251,  1.2219,\n",
      "          -0.4886,  2.1462],\n",
      "         [ 2.1244, -0.6047,  0.8441, -0.0285,  2.3691,  3.2697,  0.5993,\n",
      "           1.4731,  2.2229,  1.5091, -1.1142,  0.3953,  0.5867,  2.0992,\n",
      "          -1.2069,  0.5761],\n",
      "         [ 0.4337, -1.1330,  1.6291,  1.8880,  0.3489,  1.6795, -0.7823,\n",
      "           0.8047, -1.3175, -0.1772,  0.6109,  1.0846,  0.0226,  1.5766,\n",
      "          -0.5210,  0.4389]]])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, embed_size, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / embed_size)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i) / embed_size)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "embed_size = 16\n",
    "seq_len = 10\n",
    "pos_encoding = PositionalEncoding(embed_size)\n",
    "x = torch.randn(1, seq_len, embed_size)\n",
    "output = pos_encoding(x)\n",
    "print(\"Positional Encoding Output:\", output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48aa01e",
   "metadata": {},
   "source": [
    "### 4. Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213fb7f",
   "metadata": {},
   "source": [
    "\n",
    "**Objective**: Understand how the Transformer encoder combines self-attention and feedforward layers, the fundamental units in a Transformer.\n",
    "\n",
    "**Explanation**:\n",
    "   - **Self-Attention Layer**: The encoder starts with a multi-head attention layer, allowing the model to attend to relevant parts of the input.\n",
    "   - **Add & Norm**: After self-attention, a residual connection is added, followed by layer normalization to stabilize learning.\n",
    "   - **Feedforward Layer**: The attention output is passed through a fully connected layer for further processing.\n",
    "   - **Final Add & Norm**: A second residual connection and layer normalization complete the block, making the Transformer’s encoder robust to varying input sequences.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a25968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Encoder Block Output: tensor([[[ 0.3580, -2.3309,  0.9199,  1.8216,  0.1357, -0.4394, -0.0730,\n",
      "          -0.1890, -0.8685,  0.2046, -0.7075,  1.4934, -0.8587, -0.8029,\n",
      "           0.3374,  0.9994],\n",
      "         [-0.5780, -0.1122, -1.0606, -0.9381,  0.7764, -0.6726,  0.2579,\n",
      "          -2.0317, -0.1308,  0.3224,  0.5965,  0.6775, -1.1284,  2.0658,\n",
      "           0.9089,  1.0469],\n",
      "         [-0.2449, -0.3956, -1.1125, -0.1097, -1.4519,  0.5094, -0.1645,\n",
      "          -1.7579,  1.3889, -1.3745,  1.6421,  0.4637,  0.2255,  1.2707,\n",
      "           0.6900,  0.4212],\n",
      "         [ 0.7663,  0.8497, -1.3042,  0.7077, -0.1119,  1.0356, -1.3789,\n",
      "          -0.9106, -0.8916, -1.4833,  0.7209,  0.5240, -1.1699,  0.1378,\n",
      "           0.8356,  1.6728],\n",
      "         [-0.7347,  0.5657,  0.1316, -0.7882, -1.1169,  0.0235,  1.2757,\n",
      "           0.6790, -2.7670,  0.9572, -0.4825,  1.2487,  0.5235, -0.3385,\n",
      "           0.2200,  0.6029]],\n",
      "\n",
      "        [[-1.6383,  1.8912, -0.9375, -0.1615,  0.6343, -0.2352, -0.8187,\n",
      "          -0.1180, -0.0802, -0.7250,  0.9576,  1.2949, -0.6809, -0.8176,\n",
      "           1.8140, -0.3793],\n",
      "         [-1.2991, -0.9401, -0.0968,  2.7938,  0.4131, -0.6223,  0.2999,\n",
      "           0.9487, -0.6367, -1.2841, -0.6980,  0.7370,  0.3413,  0.7623,\n",
      "          -0.4716, -0.2475],\n",
      "         [-2.4852,  1.4418, -0.1923,  0.1175, -1.3024,  0.2904, -0.0844,\n",
      "          -0.3633, -0.9521,  0.0907,  1.4298,  0.6279,  0.1332,  1.1645,\n",
      "           0.7644, -0.6805],\n",
      "         [ 2.3329, -0.4729, -2.0507, -0.5382,  0.4875,  0.0186, -0.2462,\n",
      "           0.1640,  0.6484, -0.3835,  1.4309,  0.4744, -0.7850, -1.1208,\n",
      "          -0.6122,  0.6528],\n",
      "         [ 0.7141,  0.9343,  0.2771, -0.4708,  0.1954, -0.3484, -0.7007,\n",
      "          -1.1293,  1.1313, -0.8503, -0.9124,  2.7742, -0.7231, -1.0871,\n",
      "          -0.1646,  0.3602]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, forward_expansion):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        x = self.norm1(attention + x)  # Add & Norm\n",
    "        forward = self.feed_forward(x)\n",
    "        x = self.norm2(forward + x)  # Add & Norm\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "embed_size = 16\n",
    "num_heads = 4\n",
    "forward_expansion = 4\n",
    "encoder_block = TransformerEncoderBlock(embed_size, num_heads, forward_expansion)\n",
    "x = torch.randn(2, 5, embed_size)\n",
    "output = encoder_block(x)\n",
    "print(\"Transformer Encoder Block Output:\", output)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
