{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded7604e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/9_training_equation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b05c47",
   "metadata": {},
   "source": [
    "\n",
    "# Transformer Training Process\n",
    "\n",
    "This notebook provides a mathematical representation of the Transformer training process, focusing on each step in a loop over the target sequence tokens. \n",
    "The Transformer model is trained by predicting each token in the target sequence one at a time, with a loop structure that advances one token at a time \n",
    "to calculate and accumulate the loss.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7aa627",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Input and Target Sequences\n",
    "\n",
    "Let:\n",
    "- $ X = \\{x_1, x_2, \\dots, x_n\\} $ be the **input sequence** of length $ n $.\n",
    "- $ Y = \\{y_1, y_2, \\dots, y_m\\} $ be the **target sequence** of length $ m $.\n",
    "- $ \\hat{y}_t $ be the predicted token distribution at time step $ t $.\n",
    "\n",
    "The model will predict each token in the target sequence by using teacher forcing, where each previously known token in the target sequence is used as input \n",
    "to predict the next token in the sequence.\n",
    "\n",
    "### Teacher Forcing\n",
    "Teacher forcing is a training strategy where the true target tokens are used as inputs to the decoder during training, instead of using the decoder's own previous predictions. This helps the model to converge faster and learn more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa88c3f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Forward Pass\n",
    "\n",
    "### Encoder Output (Static for All Time Steps)\n",
    "Encode the input sequence $ X $ once for the entire target sequence:\n",
    "$$\n",
    "Z_{\\text{encoder}} = \\text{Encoder}(X)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Loop Over Target Sequence\n",
    "For $ t = 1 $ to $ m $ (the length of the target sequence):\n",
    "\n",
    "#### a. Decoder Input at Time Step $ t $\n",
    "- The decoder receives all previous tokens $ \\{y_1, y_2, \\dots, y_{t-1}\\} $ as input to predict the next token $ y_t $.\n",
    "- Formally, let $ Y_{\\text{input}}^{(t)} = \\{y_1, y_2, \\dots, y_{t-1}\\} $.\n",
    "\n",
    "#### b. Decoder Output at Time Step $ t $\n",
    "- The decoder generates an output representation based on $ Y_{\\text{input}}^{(t)} $ and the encoder output $ Z_{\\text{encoder}} $:\n",
    "$$\n",
    "Z_{\\text{decoder}}^{(t)} = \\text{Decoder}(Y_{\\text{input}}^{(t)}, Z_{\\text{encoder}})\n",
    "$$\n",
    "\n",
    "#### c. Prediction at Time Step $ t $\n",
    "- The decoder output $ Z_{\\text{decoder}}^{(t)} $ is transformed into a probability distribution over the vocabulary to predict the next token:\n",
    "$$\n",
    "\\hat{y}_t = \\text{softmax}(Z_{\\text{decoder}}^{(t)} W_O)\n",
    "$$\n",
    "where $ W_O $ is the learned output weight matrix.\n",
    "\n",
    "#### d. Cross-Entropy Loss at Time Step $ t $\n",
    "- The loss for predicting $ y_t $ is computed using cross-entropy between the predicted distribution $ \\hat{y}_t $ and the true token $ y_t $:\n",
    "$$\n",
    "\\mathcal{L}^{(t)} = - \\sum_{v=1}^V y_{t, v} \\log \\hat{y}_{t, v}\n",
    "$$\n",
    "where $ y_{t, v} $ is 1 for the correct token and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675af8fb",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Total Loss for the Sequence\n",
    "\n",
    "The total loss for the sequence is the sum of losses over all time steps:\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{t=1}^m \\mathcal{L}^{(t)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb9961",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Backpropagation and Parameter Update\n",
    "\n",
    "Using **backpropagation**, gradients of the loss $ \\mathcal{L} $ with respect to each parameter are calculated. The optimizer (e.g., Adam) updates these \n",
    "parameters to minimize the loss:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}\n",
    "$$\n",
    "where:\n",
    "- $ \\theta $ represents all model parameters,\n",
    "- $ \\eta $ is the learning rate,\n",
    "- $ \\nabla_{\\theta} \\mathcal{L} $ is the gradient of the loss with respect to $ \\theta $.\n",
    "\n",
    "This completes the training step for one sequence pair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-section",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Overall Training Process\n",
    "\n",
    "The overall training process involves iterating over multiple epochs, where each epoch processes the entire training dataset. For each sequence pair in the dataset, the model performs the forward pass, computes the loss, and updates the parameters using backpropagation.\n",
    "\n",
    "### Training Loop\n",
    "For each epoch:\n",
    "1. Shuffle the training dataset.\n",
    "2. For each sequence pair (X, Y) in the dataset:\n",
    "   - Perform the forward pass to compute predictions.\n",
    "   - Compute the loss for the sequence.\n",
    "   - Perform backpropagation to compute gradients.\n",
    "   - Update the model parameters using the optimizer.\n",
    "\n",
    "Repeat the process for the desired number of epochs or until convergence.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
