{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3436e081",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/4_sequence_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609548c",
   "metadata": {},
   "source": [
    "# Sequence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "223deca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (if not already installed)\n",
    "%pip install torch\n",
    "# Import necessary libraries\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375b97e",
   "metadata": {},
   "source": [
    "### 1. Traditional Sequence Models: n-grams and Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6bb2f",
   "metadata": {},
   "source": [
    "**Objective**: Capture word sequences by analyzing fixed-length contexts, useful for generating text based on word sequence probabilities.\n",
    "\n",
    "**Explanation**: \n",
    "- **n-grams**: A sequence of `n` items from text. For example, in a trigram model (n=3), each sequence of three words has an assigned probability based on frequency. However, n-grams struggle with long-term dependencies.\n",
    "- **Markov Models**: Use state transitions with probabilities to predict the next word based on the current state (e.g., previous word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fe3b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: The sun sets over the leaves, carrying with it the\n"
     ]
    }
   ],
   "source": [
    "# Sample text for Markov chain\n",
    "text = \"The sun sets over the distant hills as a gentle breeze rustles through the leaves, carrying with it the fragrance of blooming flowers and the promise of a peaceful evening.\"\n",
    "# Generate bigrams and compute probabilities\n",
    "def generate_bigrams(text):\n",
    "    words = text.split()\n",
    "    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "    return bigrams\n",
    "\n",
    "def markov_chain(bigrams):\n",
    "    chain = defaultdict(Counter)\n",
    "    for word1, word2 in bigrams:\n",
    "        chain[word1][word2] += 1\n",
    "    # Normalize counts to probabilities\n",
    "    for word1 in chain:\n",
    "        total_count = float(sum(chain[word1].values()))\n",
    "        for word2 in chain[word1]:\n",
    "            chain[word1][word2] /= total_count\n",
    "    return chain\n",
    "\n",
    "bigrams = generate_bigrams(text)\n",
    "chain = markov_chain(bigrams)\n",
    "\n",
    "# Generate text based on the Markov chain\n",
    "def generate_text(chain, start_word, length=10):\n",
    "    word = start_word\n",
    "    text = [word]\n",
    "    for _ in range(length - 1):\n",
    "        if word not in chain:\n",
    "            break\n",
    "        next_words = list(chain[word].keys())\n",
    "        probabilities = list(chain[word].values())\n",
    "        word = random.choices(next_words, probabilities)[0]\n",
    "        text.append(word)\n",
    "    return ' '.join(text)\n",
    "\n",
    "print(\"Generated Text:\", generate_text(chain, start_word=\"The\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb873e31",
   "metadata": {},
   "source": [
    "### 2. Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b596c",
   "metadata": {},
   "source": [
    "**Objective**: Model sequences by retaining information from previous steps, enabling context across time steps.\n",
    "\n",
    "**Explanation**: \n",
    "- RNNs process sequences element by element, retaining a hidden state that captures information from prior steps. They struggle with long sequences due to issues like the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65646bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Outputs: tensor([[ 0.1883],\n",
      "        [ 0.1001],\n",
      "        [-0.0622],\n",
      "        [ 0.0175],\n",
      "        [ 0.1789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Sample RNN\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Using the last output for prediction\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 1\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "inputs = torch.randn(5, 3, input_size)\n",
    "outputs = rnn(inputs)\n",
    "print(\"RNN Outputs:\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92acc606",
   "metadata": {},
   "source": [
    "### 3. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99caa4",
   "metadata": {},
   "source": [
    "**Objective**: Overcome RNN limitations with gates to selectively retain or forget information.\n",
    "\n",
    "**Explanation**:\n",
    "- **LSTM**: Adds input, forget, and output gates to control the flow of information, addressing vanishing gradient issues.\n",
    "- **GRU**: A simplified LSTM with fewer gates, offering similar performance with less computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a70a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Outputs: tensor([[ 0.0953],\n",
      "        [-0.0282],\n",
      "        [ 0.0621],\n",
      "        [ 0.0555],\n",
      "        [ 0.0997]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Sample LSTM\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, (hidden, cell) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "lstm = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "outputs = lstm(inputs)\n",
    "print(\"LSTM Outputs:\", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c5a6a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU Outputs: tensor([[0.0555],\n",
      "        [0.0071],\n",
      "        [0.2153],\n",
      "        [0.2888],\n",
      "        [0.2065]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Sample GRU\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, hidden = self.gru(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "gru = SimpleGRU(input_size, hidden_size, output_size)\n",
    "outputs = gru(inputs)\n",
    "print(\"GRU Outputs:\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0318ef0",
   "metadata": {},
   "source": [
    "### 4. Introduction to the Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945de56",
   "metadata": {},
   "source": [
    "**Objective**: Enable models to focus on relevant parts of input sequences, essential for handling longer dependencies.\n",
    "\n",
    "**Explanation**: \n",
    "- Attention allows a model to “attend” to specific input parts when generating each output. It’s especially useful for tasks requiring selective referencing of input tokens (e.g., translation, summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2731fcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output: tensor([[[-0.8891,  0.4020,  0.7994,  0.3439,  0.3556,  0.6809,  0.4139,\n",
      "          -0.3774],\n",
      "         [ 0.0418,  0.3143, -0.2218,  0.9471, -0.6442,  1.3846, -0.3155,\n",
      "          -0.4482],\n",
      "         [-0.7372,  0.4270,  0.6818,  0.3993,  0.2088,  0.7749,  0.3235,\n",
      "          -0.3409]],\n",
      "\n",
      "        [[-0.0585,  1.3033, -0.3759, -0.8232,  0.3286,  0.4324, -0.0348,\n",
      "          -1.2970],\n",
      "         [ 0.0905,  1.4962, -0.4545, -0.9702,  0.6231,  0.5169, -0.0101,\n",
      "          -1.1888],\n",
      "         [-1.1606, -0.0238,  0.0884,  0.2125, -1.5161, -0.0946, -0.1975,\n",
      "          -1.9829]],\n",
      "\n",
      "        [[ 0.5571,  0.2656,  0.8578,  0.1008, -0.2550,  0.8173,  0.3074,\n",
      "          -0.0349],\n",
      "         [-0.5294,  0.3932,  0.7593,  0.8918, -0.2605,  0.7131,  0.5405,\n",
      "           0.2413],\n",
      "         [-0.1079,  0.3202,  0.7876,  0.8017, -0.1750,  0.3296,  0.3486,\n",
      "           0.0338]],\n",
      "\n",
      "        [[-0.3111, -0.6088, -0.7258, -0.8805,  0.3606,  0.5548, -0.2823,\n",
      "          -0.1240],\n",
      "         [-0.3587, -0.1623, -0.3428, -1.3550,  0.1730,  0.8864, -0.5644,\n",
      "          -0.2089],\n",
      "         [-0.3910, -0.5017, -0.3649, -0.9417,  0.2734,  0.5359, -0.8926,\n",
      "          -0.2055]],\n",
      "\n",
      "        [[ 0.1822, -0.7686,  0.9559,  0.1308,  0.2321,  0.9175, -0.8055,\n",
      "          -0.4139],\n",
      "         [-0.6184, -0.8971,  0.7894,  0.1294,  0.3219, -0.2796,  0.5924,\n",
      "           0.1266],\n",
      "         [ 0.4797, -0.4712,  0.7226,  0.4803,  0.0974,  1.4437, -0.7349,\n",
      "          -0.5561]]])\n",
      "Attention Weights: tensor([[[0.2679, 0.4628, 0.2693],\n",
      "         [0.0470, 0.0683, 0.8847],\n",
      "         [0.2565, 0.3800, 0.3636]],\n",
      "\n",
      "        [[0.8286, 0.1534, 0.0181],\n",
      "         [0.9412, 0.0569, 0.0018],\n",
      "         [0.0400, 0.7545, 0.2055]],\n",
      "\n",
      "        [[0.7251, 0.2095, 0.0653],\n",
      "         [0.1600, 0.2566, 0.5834],\n",
      "         [0.0890, 0.4307, 0.4803]],\n",
      "\n",
      "        [[0.4468, 0.2123, 0.3409],\n",
      "         [0.1627, 0.2023, 0.6350],\n",
      "         [0.1132, 0.3694, 0.5174]],\n",
      "\n",
      "        [[0.2054, 0.6774, 0.1172],\n",
      "         [0.6072, 0.3791, 0.0136],\n",
      "         [0.1150, 0.5968, 0.2883]]])\n"
     ]
    }
   ],
   "source": [
    "# Simple attention mechanism\n",
    "def attention(query, key, value):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(key.size(-1), dtype=torch.float32))\n",
    "    attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example inputs\n",
    "query = torch.randn(5, 3, 8)\n",
    "key = torch.randn(5, 3, 8)\n",
    "value = torch.randn(5, 3, 8)\n",
    "\n",
    "# Apply attention\n",
    "output, weights = attention(query, key, value)\n",
    "print(\"Attention Output:\", output)\n",
    "print(\"Attention Weights:\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc26b6",
   "metadata": {},
   "source": [
    "**Note**: The application examples provided in this notebook (Markov Models, RNNs, LSTMs, GRUs, and Attention) are appropriate for sequence modeling tasks. Each method addresses specific challenges in sequence modeling, such as handling long-term dependencies or focusing on relevant input parts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
