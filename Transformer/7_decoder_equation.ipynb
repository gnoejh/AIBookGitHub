{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a20a975",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Transformer/7_decoder_equation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4d6e9",
   "metadata": {},
   "source": [
    "# Decoder Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978c15f",
   "metadata": {},
   "source": [
    "## 1. Token Embedding and Positional Encoding (Decoder)\n",
    "\n",
    "Just like in the encoder, the decoder begins by embedding the input tokens and adding positional encodings to introduce sequence information.\n",
    "\n",
    "- **Token Embedding**:\n",
    "  - Let the input token sequence for the decoder be represented by indices $ \\text{Token Indices} \\in \\mathbb{R}^{m} $, where $ m $ is the sequence length (number of tokens) in the decoder input.\n",
    "  - Let the embedding matrix be $ W_E \\in \\mathbb{R}^{V \\times d} $, where $ V $ is the vocabulary size and $ d $ is the embedding dimension.\n",
    "  - The token embedding maps each token index to a dense vector in $ \\mathbb{R}^{d} $:\n",
    "    $$\n",
    "    Y = \\text{Token Indices} \\cdot W_E \\in \\mathbb{R}^{m \\times d}\n",
    "    $$\n",
    "\n",
    "- **Positional Encoding**:\n",
    "  - For each position $ i $ and embedding dimension $ j $, we use sinusoidal encodings:\n",
    "    $$\n",
    "    \\text{PE}_{i, 2j} = \\sin\\left(\\frac{i}{10000^{\\frac{2j}{d}}}\\right), \\quad \\text{PE}_{i, 2j+1} = \\cos\\left(\\frac{i}{10000^{\\frac{2j}{d}}}\\right)\n",
    "    $$\n",
    "  - Adding positional encoding to the token embeddings gives:\n",
    "    $$\n",
    "    Y_{\\text{input}} = Y + \\text{PE} \\in \\mathbb{R}^{m \\times d}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd420991",
   "metadata": {},
   "source": [
    "## 2. Masked Self-Attention\n",
    "\n",
    "The **masked self-attention** mechanism ensures that each position in the sequence can only attend to previous positions, preventing the decoder from seeing future tokens.\n",
    "\n",
    "- **Queries, Keys, and Values**:\n",
    "  - We compute queries $ Q $, keys $ K $, and values $ V $ based on $ Y_{\\text{input}} $ (decoder input):\n",
    "    $$\n",
    "    Q = Y_{\\text{input}} W_Q, \\quad K = Y_{\\text{input}} W_K, \\quad V = Y_{\\text{input}} W_V\n",
    "    $$\n",
    "  - Here, $ W_Q $, $ W_K $, and $ W_V $ are learned weight matrices with dimensions $ d \\times k $, $ d \\times k $, and $ d \\times d $, respectively.\n",
    "\n",
    "- **Masked Attention Scores**:\n",
    "  - Compute attention scores by taking the dot product of $ Q $ with $ K^T $, scaled by $ \\frac{1}{\\sqrt{k}} $, and apply a mask to prevent future positions:\n",
    "    $$\n",
    "    \\text{Masked Score Matrix} = \\left(\\frac{Q K^T}{\\sqrt{k}}\\right) + \\text{mask} \\in \\mathbb{R}^{m \\times m}\n",
    "    $$\n",
    "  - The mask assigns a large negative value (e.g., $ -\\infty $) to future positions, zeroing them out in softmax.\n",
    "\n",
    "- **Masked Self-Attention Output**:\n",
    "  - The output is a weighted sum of $ V $ based on the masked attention weights:\n",
    "    $$\n",
    "    \\text{Masked Self-Attention Output} = A V \\in \\mathbb{R}^{m \\times d}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7081ed",
   "metadata": {},
   "source": [
    "## 3. Cross-Attention\n",
    "\n",
    "**Cross-attention** allows the decoder to focus on relevant parts of the encoder’s output.\n",
    "\n",
    "- **Queries, Keys, and Values**:\n",
    "  - Queries $ Q $ come from the masked self-attention output $ Z_{\\text{masked}} $, while keys $ K $ and values $ V $ come from the encoder’s final output $ Z_{\\text{encoder}} $:\n",
    "    $$\n",
    "    Q = Z_{\\text{masked}} W_Q, \\quad K = Z_{\\text{encoder}} W_K, \\quad V = Z_{\\text{encoder}} W_V\n",
    "    $$\n",
    "\n",
    "- **Cross-Attention Scores**:\n",
    "  - Compute cross-attention scores by taking the dot product of $ Q $ and $ K^T $, followed by scaling:\n",
    "    $$\n",
    "    \\text{Cross-Attention Score Matrix} = \\frac{Q K^T}{\\sqrt{k}} \\in \\mathbb{R}^{m \\times n}\n",
    "    $$\n",
    "\n",
    "- **Cross-Attention Output**:\n",
    "  - The output is a weighted sum of $ V $ based on the cross-attention weights:\n",
    "    $$\n",
    "    \\text{Cross-Attention Output} = A V \\in \\mathbb{R}^{m \\times d}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5632801",
   "metadata": {},
   "source": [
    "## 4. Feed-Forward Network (FFN) and Residual Connections\n",
    "\n",
    "Each decoder block includes a position-wise feed-forward network (FFN) with residual connections and layer normalization.\n",
    "\n",
    "- **Masked Self-Attention with Residual Connection**:\n",
    "  $$\n",
    "  Z_{\\text{masked}} = \\text{LayerNorm}(Y_{\\text{input}} + \\text{Masked Self-Attention}(Y_{\\text{input}})) \\in \\mathbb{R}^{m \\times d}\n",
    "  $$\n",
    "\n",
    "- **Cross-Attention with Residual Connection**:\n",
    "  $$\n",
    "  Z_{\\text{cross}} = \\text{LayerNorm}(Z_{\\text{masked}} + \\text{Cross-Attention}(Z_{\\text{masked}}, Z_{\\text{encoder}})) \\in \\mathbb{R}^{m \\times d}\n",
    "  $$\n",
    "\n",
    "- **Feed-Forward Network with Residual Connection**:\n",
    "  - The output of cross-attention, $ Z_{\\text{cross}} $, is passed through the feed-forward network:\n",
    "    $$\n",
    "    \\text{FFN}(Z_{\\text{cross}}) = \\text{ReLU}(Z_{\\text{cross}} W_1 + b_1) W_2 + b_2\n",
    "    $$\n",
    "    where:\n",
    "    - $ W_1 \\in \\mathbb{R}^{d \\times d_{ff}} $, $ W_2 \\in \\mathbb{R}^{d_{ff} \\times d} $,\n",
    "    - $ b_1 \\in \\mathbb{R}^{d_{ff}} $, $ b_2 \\in \\mathbb{R}^{d} $, and\n",
    "    - $ d_{ff} $ is the hidden dimension in the FFN.\n",
    "\n",
    "  - The final decoder output:\n",
    "    $$\n",
    "    Z_{\\text{decoder}} = \\text{LayerNorm}(Z_{\\text{cross}} + \\text{FFN}(Z_{\\text{cross}})) \\in \\mathbb{R}^{m \\times d}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-section",
   "metadata": {},
   "source": [
    "## 5. Overall Decoder Process\n",
    "\n",
    "The overall decoder process involves passing the input sequence through multiple decoder blocks to generate the final output sequence.\n",
    "\n",
    "## Decoder Loop\n",
    "1. Embed the input tokens and add positional encodings.\n",
    "2. For each decoder block:\n",
    "   - Apply masked self-attention with residual connection and layer normalization.\n",
    "   - Apply cross-attention with residual connection and layer normalization.\n",
    "   - Apply feed-forward network with residual connection and layer normalization.\n",
    "\n",
    "The final output of the last decoder block is the decoded representation $ Z_{\\text{decoder}} $.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
