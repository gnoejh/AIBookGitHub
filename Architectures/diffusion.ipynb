{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Diffusion Models\n",
                "\n",
                "Diffusion models are a class of generative models that have recently achieved state-of-the-art results in generating high-quality images, audio, and other data types. These models work by gradually adding noise to data and then learning to reverse this process."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conceptual Overview\n",
                "\n",
                "Diffusion models operate based on two processes:\n",
                "\n",
                "1. **Forward Process (Diffusion)**: Gradually adds noise to the data until it becomes pure noise\n",
                "2. **Reverse Process (Denoising)**: Learns to gradually remove noise to recover the data\n",
                "\n",
                "![Diffusion Process](https://i.imgur.com/uHDRgUc.png)\n",
                "\n",
                "The key insight is that while destroying information (adding noise) is easy, the model learns the challenging task of restoring information from noise. This approach creates a more stable training process compared to adversarial methods like GANs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Mathematical Foundation\n",
                "\n",
                "### Forward Process\n",
                "\n",
                "The forward diffusion process is defined as a Markov chain that gradually adds Gaussian noise to the data:\n",
                "\n",
                "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)$$\n",
                "\n",
                "where $\\beta_t$ is a variance schedule that controls the noise level at each step.\n",
                "\n",
                "An important property is that we can sample $x_t$ directly from $x_0$ without going through all intermediate steps:\n",
                "\n",
                "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$$\n",
                "\n",
                "where $\\bar{\\alpha}_t = \\prod_{i=1}^{t} (1 - \\beta_i)$.\n",
                "\n",
                "### Reverse Process\n",
                "\n",
                "The goal is to learn the reverse process, which gradually denoises the data:\n",
                "\n",
                "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
                "\n",
                "The neural network is trained to predict the parameters of this distribution."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Objective\n",
                "\n",
                "The training objective is derived from variational inference and simplifies to predicting either:\n",
                "\n",
                "1. The added noise ε (noise prediction)\n",
                "2. The clean data $x_0$ (data prediction)\n",
                "3. The mean of the reverse process distribution (μ prediction)\n",
                "\n",
                "The simplified objective is:\n",
                "\n",
                "$$L_{simple} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ ||\\epsilon - \\epsilon_\\theta(x_t, t)||^2 \\right]$$\n",
                "\n",
                "where $\\epsilon$ is the noise added during the forward process, and $\\epsilon_\\theta$ is the model's prediction of that noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "\n",
                "# Define a simple U-Net style architecture for the denoising network\n",
                "class SimpleUNet(nn.Module):\n",
                "    def __init__(self, channels=3, time_emb_dim=100):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Time embedding\n",
                "        self.time_mlp = nn.Sequential(\n",
                "            nn.Linear(1, time_emb_dim),\n",
                "            nn.SiLU(),\n",
                "            nn.Linear(time_emb_dim, time_emb_dim)\n",
                "        )\n",
                "        \n",
                "        # Encoder\n",
                "        self.conv1 = nn.Conv2d(channels, 64, 3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
                "        self.conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n",
                "        \n",
                "        # Middle with time embedding\n",
                "        self.time_proj = nn.Linear(time_emb_dim, 256)\n",
                "        self.mid_conv1 = nn.Conv2d(256, 256, 3, padding=1)\n",
                "        self.mid_conv2 = nn.Conv2d(256, 256, 3, padding=1)\n",
                "        \n",
                "        # Decoder\n",
                "        self.up1 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
                "        self.up2 = nn.ConvTranspose2d(128 + 128, 64, 4, stride=2, padding=1)  # Skip connection\n",
                "        self.final = nn.Conv2d(64 + 64, channels, 3, padding=1)  # Skip connection\n",
                "        \n",
                "    def forward(self, x, t):\n",
                "        # Time embedding\n",
                "        t_emb = self.time_mlp(t.unsqueeze(-1).float())\n",
                "        \n",
                "        # Encoder\n",
                "        x1 = F.silu(self.conv1(x))  # Save for skip connection\n",
                "        x = F.silu(self.conv2(x1))\n",
                "        x2 = x  # Save for skip connection\n",
                "        x = F.silu(self.conv3(x))\n",
                "        \n",
                "        # Middle\n",
                "        time_proj = self.time_proj(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
                "        x = x + time_proj  # Add time information\n",
                "        x = F.silu(self.mid_conv1(x))\n",
                "        x = F.silu(self.mid_conv2(x))\n",
                "        \n",
                "        # Decoder with skip connections\n",
                "        x = F.silu(self.up1(x))\n",
                "        x = torch.cat([x, x2], dim=1)  # Skip connection\n",
                "        x = F.silu(self.up2(x))\n",
                "        x = torch.cat([x, x1], dim=1)  # Skip connection\n",
                "        \n",
                "        return self.final(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Diffusion Process Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DiffusionModel(nn.Module):\n",
                "    def __init__(self, model, beta_start=1e-4, beta_end=0.02, timesteps=1000):\n",
                "        super().__init__()\n",
                "        self.model = model\n",
                "        self.timesteps = timesteps\n",
                "        \n",
                "        # Define beta schedule\n",
                "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
                "        self.alphas = 1 - self.betas\n",
                "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
                "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
                "        \n",
                "        # Pre-compute values for sampling\n",
                "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
                "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
                "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
                "        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
                "    \n",
                "    def q_sample(self, x_0, t, noise=None):\n",
                "        \"\"\"Forward diffusion process: add noise to the data\"\"\"\n",
                "        if noise is None:\n",
                "            noise = torch.randn_like(x_0)\n",
                "            \n",
                "        # Extract the corresponding alpha values\n",
                "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t]\n",
                "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t]\n",
                "        \n",
                "        # Reshape for proper broadcasting\n",
                "        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.reshape(-1, 1, 1, 1)\n",
                "        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.reshape(-1, 1, 1, 1)\n",
                "        \n",
                "        # Apply the diffusion formula\n",
                "        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
                "    \n",
                "    def p_losses(self, x_0, t, noise=None):\n",
                "        \"\"\"Calculate the loss for training\"\"\"\n",
                "        if noise is None:\n",
                "            noise = torch.randn_like(x_0)\n",
                "            \n",
                "        # Forward diffuse the data\n",
                "        x_noisy = self.q_sample(x_0, t, noise)\n",
                "        \n",
                "        # Predict the noise\n",
                "        predicted_noise = self.model(x_noisy, t)\n",
                "        \n",
                "        # Calculate the simple MSE loss\n",
                "        loss = F.mse_loss(noise, predicted_noise)\n",
                "        \n",
                "        return loss\n",
                "    \n",
                "    @torch.no_grad()\n",
                "    def p_sample(self, x_t, t):\n",
                "        \"\"\"Sample from p(x_{t-1} | x_t) - one step of the reverse process\"\"\"\n",
                "        # Get the model prediction\n",
                "        predicted_noise = self.model(x_t, t)\n",
                "        \n",
                "        # Extract parameters\n",
                "        beta_t = self.betas[t]\n",
                "        alpha_t = self.alphas[t]\n",
                "        sqrt_recip_alpha_t = self.sqrt_recip_alphas[t]\n",
                "        \n",
                "        # Reshape for broadcasting\n",
                "        beta_t = beta_t.reshape(-1, 1, 1, 1)\n",
                "        sqrt_recip_alpha_t = sqrt_recip_alpha_t.reshape(-1, 1, 1, 1)\n",
                "        \n",
                "        # Calculate the mean for the reverse process\n",
                "        mean = sqrt_recip_alpha_t * (x_t - (beta_t / torch.sqrt(1 - self.alphas_cumprod[t])) * predicted_noise)\n",
                "        \n",
                "        # Add some noise for t > 0\n",
                "        if t > 0:\n",
                "            variance = torch.sqrt(self.posterior_variance[t])\n",
                "            variance = variance.reshape(-1, 1, 1, 1)\n",
                "            noise = torch.randn_like(x_t)\n",
                "            return mean + variance * noise\n",
                "        else:\n",
                "            return mean\n",
                "    \n",
                "    @torch.no_grad()\n",
                "    def sample(self, batch_size, img_shape, device):\n",
                "        \"\"\"Generate samples by running the complete reverse process\"\"\"\n",
                "        # Start from pure noise\n",
                "        img = torch.randn(batch_size, *img_shape, device=device)\n",
                "        \n",
                "        # Iteratively denoise\n",
                "        for t in reversed(range(self.timesteps)):\n",
                "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
                "            img = self.p_sample(img, t_batch)\n",
                "            \n",
                "        return img"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Loop Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_diffusion(diffusion_model, dataloader, optimizer, device, epochs):\n",
                "    \"\"\"Train the diffusion model\"\"\"\n",
                "    diffusion_model.train()\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        running_loss = 0.0\n",
                "        \n",
                "        for i, batch in enumerate(dataloader):\n",
                "            # Get batch of images\n",
                "            x = batch[0].to(device)\n",
                "            batch_size = x.shape[0]\n",
                "            \n",
                "            # Reset gradients\n",
                "            optimizer.zero_grad()\n",
                "            \n",
                "            # Sample random timesteps\n",
                "            t = torch.randint(0, diffusion_model.timesteps, (batch_size,), device=device).long()\n",
                "            \n",
                "            # Calculate loss\n",
                "            loss = diffusion_model.p_losses(x, t)\n",
                "            \n",
                "            # Backpropagation\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            running_loss += loss.item()\n",
                "            \n",
                "            # Print progress\n",
                "            if i % 100 == 99:\n",
                "                print(f\"Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 100:.4f}\")\n",
                "                running_loss = 0.0\n",
                "        \n",
                "        # Generate samples after each epoch\n",
                "        if (epoch + 1) % 5 == 0:\n",
                "            with torch.no_grad():\n",
                "                samples = diffusion_model.sample(4, x.shape[1:], device)\n",
                "                # Here you would save or display the samples\n",
                "                \n",
                "    print(\"Training completed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example setup (not executed)\n",
                "def setup_example():\n",
                "    import torch\n",
                "    from torch.utils.data import DataLoader\n",
                "    from torchvision import datasets, transforms\n",
                "    \n",
                "    # Set device\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    \n",
                "    # Setup data\n",
                "    transform = transforms.Compose([\n",
                "        transforms.Resize(32),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize((0.5,), (0.5,))\n",
                "    ])\n",
                "    \n",
                "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
                "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
                "    \n",
                "    # Create model\n",
                "    unet = SimpleUNet(channels=3).to(device)\n",
                "    diffusion = DiffusionModel(unet).to(device)\n",
                "    \n",
                "    # Setup optimizer\n",
                "    optimizer = torch.optim.Adam(diffusion.parameters(), lr=1e-4)\n",
                "    \n",
                "    # Train model\n",
                "    train_diffusion(diffusion, dataloader, optimizer, device, epochs=30)\n",
                "    \n",
                "    # Generate samples\n",
                "    samples = diffusion.sample(16, (3, 32, 32), device)\n",
                "    \n",
                "    return samples\n",
                "\n",
                "# Note: This function is not executed in this notebook"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Variants of Diffusion Models\n",
                "\n",
                "### DDPM (Denoising Diffusion Probabilistic Models)\n",
                "- The original formulation with a fixed variance schedule\n",
                "- Excellent quality but requires many sampling steps (typically 1000)\n",
                "\n",
                "### DDIM (Denoising Diffusion Implicit Models)\n",
                "- Extends DDPM with a non-Markovian process\n",
                "- Enables faster sampling with fewer steps (10-50 steps often sufficient)\n",
                "- Supports deterministic sampling\n",
                "\n",
                "### Latent Diffusion Models (LDMs)\n",
                "- Apply diffusion in a compressed latent space instead of pixel space\n",
                "- Significantly more efficient for high-resolution images\n",
                "- Used in Stable Diffusion for text-to-image generation\n",
                "\n",
                "### Score-Based Generative Models\n",
                "- Alternative mathematical formulation using score matching\n",
                "- Shown to be equivalent to diffusion models in certain conditions\n",
                "- Often uses stochastic differential equations (SDEs) for sampling"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conditioning Techniques\n",
                "\n",
                "Diffusion models can be conditioned on various inputs to control generation:\n",
                "\n",
                "### Class Conditioning\n",
                "```python\n",
                "class ConditionalUNet(nn.Module):\n",
                "    def __init__(self, num_classes=10, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.class_embedding = nn.Embedding(num_classes, embedding_dim)\n",
                "        # Rest of the model\n",
                "        \n",
                "    def forward(self, x, t, class_labels):\n",
                "        # Get class embedding and combine with time embedding\n",
                "        class_emb = self.class_embedding(class_labels)\n",
                "        # Use in the model\n",
                "```\n",
                "\n",
                "### Text Conditioning (as in Stable Diffusion)\n",
                "- Use a pre-trained text encoder (like CLIP)\n",
                "- Extract text embeddings and inject them into the UNet\n",
                "- Often uses cross-attention mechanisms\n",
                "\n",
                "### Image Conditioning\n",
                "- For image-to-image translation tasks\n",
                "- Can use concatenation or cross-attention mechanisms"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications of Diffusion Models\n",
                "\n",
                "1. **Image Generation**\n",
                "   - Text-to-image (Stable Diffusion, DALL-E 2, Midjourney)\n",
                "   - High-resolution image synthesis\n",
                "   - Image-to-image translation\n",
                "\n",
                "2. **Audio Generation**\n",
                "   - Speech synthesis (AudioLM)\n",
                "   - Music generation\n",
                "   - Sound effects creation\n",
                "\n",
                "3. **Video Generation**\n",
                "   - Text-to-video models (Imagen Video, Make-A-Video)\n",
                "   - Video prediction and interpolation\n",
                "\n",
                "4. **3D Content Creation**\n",
                "   - Text-to-3D (DreamFusion)\n",
                "   - 3D asset generation\n",
                "\n",
                "5. **Scientific Applications**\n",
                "   - Protein structure generation\n",
                "   - Molecule design\n",
                "   - Material discovery"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advantages and Challenges\n",
                "\n",
                "### Advantages\n",
                "- High-quality outputs, often superior to GANs\n",
                "- Stable training without adversarial dynamics\n",
                "- Greater diversity in generated samples\n",
                "- Flexible conditioning mechanisms\n",
                "- Well-understood probabilistic foundation\n",
                "\n",
                "### Challenges\n",
                "- Slow sampling process (though recent optimizations help)\n",
                "- High computational requirements\n",
                "- Mode coverage sometimes at the expense of sample fidelity\n",
                "- Hyperparameter sensitivity"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References and Further Reading\n",
                "\n",
                "- Ho, J., et al. (2020). [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239). NeurIPS.\n",
                "- Song, J., et al. (2020). [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502). ICLR.\n",
                "- Rombach, R., et al. (2022). [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752). CVPR.\n",
                "- Dhariwal, P., & Nichol, A. (2021). [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233). NeurIPS.\n",
                "- Song, Y., & Ermon, S. (2019). [Generative Modeling by Estimating Gradients of the Data Distribution](https://arxiv.org/abs/1907.05600). NeurIPS.\n",
                "\n",
                "### Implementations\n",
                "- [HuggingFace Diffusers Library](https://github.com/huggingface/diffusers)\n",
                "- [Stable Diffusion](https://github.com/CompVis/stable-diffusion)\n",
                "- [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "py312sb3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
