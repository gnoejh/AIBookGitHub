{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/densenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DenseNet: Densely Connected Convolutional Networks\n",
                "\n",
                "## Introduction\n",
                "\n",
                "DenseNet (Densely Connected Convolutional Network) was introduced in 2017 by Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger in their paper \"Densely Connected Convolutional Networks\". DenseNet revolutionized deep learning architectures by establishing direct connections between any layer and all subsequent layers, creating a densely connected pattern.\n",
                "\n",
                "The key innovation of DenseNet is its connectivity pattern: each layer receives inputs from all preceding layers and passes its feature maps to all subsequent layers. This stands in contrast to traditional CNNs where each layer only connects to the previous and next layers."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Innovations and Benefits\n",
                "\n",
                "1. **Dense Connectivity**: Each layer connects to every other layer in a feed-forward fashion, which helps with gradient flow during training.\n",
                "\n",
                "2. **Feature Reuse**: All layers can access feature maps from previous layers, allowing for more efficient feature reuse.\n",
                "\n",
                "3. **Reduced Parameter Count**: Despite more connections, DenseNet requires fewer parameters than comparable networks because it avoids learning redundant feature maps.\n",
                "\n",
                "4. **Stronger Feature Propagation**: The direct connections facilitate feature propagation throughout the network.\n",
                "\n",
                "5. **Alleviated Vanishing Gradient**: The short connections create direct paths for information and gradient flow, which helps in training very deep networks.\n",
                "\n",
                "6. **Improved Feature Diversity**: Each layer adds only a small set of feature maps, encouraging the network to maintain diverse features."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architecture Details\n",
                "\n",
                "### DenseNet Structure\n",
                "\n",
                "A DenseNet consists of several dense blocks connected by transition layers:\n",
                "\n",
                "1. **Dense Blocks**: Where dense connectivity happens - each layer connects to all previous layers\n",
                "2. **Transition Layers**: Between dense blocks for downsampling (typically using convolution and pooling)\n",
                "\n",
                "```\n",
                "Input \u2192 [Dense Block 1] \u2192 [Transition Layer 1] \u2192 [Dense Block 2] \u2192 ... \u2192 [Classification Layer] \u2192 Output\n",
                "```\n",
                "\n",
                "### Dense Block Operation\n",
                "\n",
                "Within a dense block, the output of the l-th layer is defined as:\n",
                "\n",
                "$$x_l = H_l([x_0, x_1, ..., x_{l-1}])$$\n",
                "\n",
                "where $[x_0, x_1, ..., x_{l-1}]$ refers to the concatenation of the feature maps produced by layers 0 to l-1, and $H_l$ is a composite function of operations such as batch normalization, ReLU, and convolution.\n",
                "\n",
                "### Composite Function\n",
                "\n",
                "Each layer in a dense block typically applies the following operations:\n",
                "- Batch Normalization (BN)\n",
                "- ReLU activation\n",
                "- 3\u00d73 Convolution\n",
                "\n",
                "### Growth Rate\n",
                "\n",
                "A key hyperparameter of DenseNet is the growth rate (k), which controls how many new feature channels each layer contributes to the network's collective knowledge. The total number of channels in the l-th layer is:\n",
                "\n",
                "$$k_0 + k \u00d7 (l-1)$$\n",
                "\n",
                "where $k_0$ is the number of channels in the input layer.\n",
                "\n",
                "### Bottleneck Layers\n",
                "\n",
                "DenseNet-B variant incorporates bottleneck layers (1\u00d71 convolution) before each 3\u00d73 convolution to reduce the number of input feature maps, improving computational efficiency."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visual Representation\n",
                "\n",
                "```\n",
                "Traditional CNN:\n",
                "Layer 1 \u2192 Layer 2 \u2192 Layer 3 \u2192 Layer 4 \u2192 Output\n",
                "\n",
                "DenseNet:\n",
                "Layer 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "   \u2193                               \u25bc\n",
                "Layer 2 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   Layer 4 \u2192 Output\n",
                "   \u2193                           \u25bc\n",
                "Layer 3 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Layer 4 \u2192 Output\n",
                "```\n",
                "\n",
                "The arrows represent direct connections. In DenseNet, the feature maps from all previous layers are concatenated and passed to the next layer."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## DenseNet Variants\n",
                "\n",
                "Several DenseNet configurations exist:\n",
                "\n",
                "1. **DenseNet-121**: 121 layers with growth rate k=32\n",
                "2. **DenseNet-169**: 169 layers with growth rate k=32\n",
                "3. **DenseNet-201**: 201 layers with growth rate k=32\n",
                "4. **DenseNet-264**: 264 layers with growth rate k=32\n",
                "\n",
                "The number in each variant represents the total number of layers in the network."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation Example\n",
                "\n",
                "Here's an implementation of DenseNet using PyTorch:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class DenseLayer(nn.Module):\n",
                "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
                "        super(DenseLayer, self).__init__()\n",
                "        \n",
                "        # Bottleneck layer (BN-ReLU-Conv(1x1))\n",
                "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
                "        self.relu1 = nn.ReLU(inplace=True)\n",
                "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate,\n",
                "                               kernel_size=1, stride=1, bias=False)\n",
                "        \n",
                "        # Main layer (BN-ReLU-Conv(3x3))\n",
                "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
                "        self.relu2 = nn.ReLU(inplace=True)\n",
                "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
                "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
                "        \n",
                "        self.drop_rate = drop_rate\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Bottleneck\n",
                "        new_features = self.conv1(self.relu1(self.norm1(x)))\n",
                "        \n",
                "        # Main 3x3 convolution\n",
                "        new_features = self.conv2(self.relu2(self.norm2(new_features)))\n",
                "        \n",
                "        # Dropout\n",
                "        if self.drop_rate > 0:\n",
                "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
                "        \n",
                "        # Concatenate input with new features\n",
                "        return torch.cat([x, new_features], 1)\n",
                "\n",
                "\n",
                "class DenseBlock(nn.Module):\n",
                "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
                "        super(DenseBlock, self).__init__()\n",
                "        layers = []\n",
                "        for i in range(num_layers):\n",
                "            layers.append(DenseLayer(\n",
                "                num_input_features + i * growth_rate,\n",
                "                growth_rate=growth_rate,\n",
                "                bn_size=bn_size,\n",
                "                drop_rate=drop_rate\n",
                "            ))\n",
                "        self.layers = nn.Sequential(*layers)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.layers(x)\n",
                "\n",
                "\n",
                "class TransitionLayer(nn.Module):\n",
                "    def __init__(self, num_input_features, num_output_features):\n",
                "        super(TransitionLayer, self).__init__()\n",
                "        self.norm = nn.BatchNorm2d(num_input_features)\n",
                "        self.relu = nn.ReLU(inplace=True)\n",
                "        self.conv = nn.Conv2d(num_input_features, num_output_features,\n",
                "                              kernel_size=1, stride=1, bias=False)\n",
                "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.norm(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.conv(x)\n",
                "        x = self.pool(x)\n",
                "        return x\n",
                "\n",
                "\n",
                "class DenseNet(nn.Module):\n",
                "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
                "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n",
                "        super(DenseNet, self).__init__()\n",
                "        \n",
                "        # Initial convolution\n",
                "        self.features = nn.Sequential(OrderedDict([\n",
                "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
                "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
                "            ('relu0', nn.ReLU(inplace=True)),\n",
                "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
                "        ]))\n",
                "        \n",
                "        # Dense blocks\n",
                "        num_features = num_init_features\n",
                "        for i, num_layers in enumerate(block_config):\n",
                "            # Add a dense block\n",
                "            block = DenseBlock(\n",
                "                num_layers=num_layers,\n",
                "                num_input_features=num_features,\n",
                "                bn_size=bn_size,\n",
                "                growth_rate=growth_rate,\n",
                "                drop_rate=drop_rate\n",
                "            )\n",
                "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
                "            num_features = num_features + num_layers * growth_rate\n",
                "            \n",
                "            # Add a transition layer between dense blocks (except after the last block)\n",
                "            if i != len(block_config) - 1:\n",
                "                trans = TransitionLayer(\n",
                "                    num_input_features=num_features,\n",
                "                    num_output_features=num_features // 2\n",
                "                )\n",
                "                self.features.add_module('transition%d' % (i + 1), trans)\n",
                "                num_features = num_features // 2\n",
                "        \n",
                "        # Final batch norm\n",
                "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
                "        \n",
                "        # Classification layer\n",
                "        self.classifier = nn.Linear(num_features, num_classes)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        features = self.features(x)\n",
                "        out = F.relu(features, inplace=True)\n",
                "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
                "        out = torch.flatten(out, 1)\n",
                "        out = self.classifier(out)\n",
                "        return out\n",
                "\n",
                "# Missing import for OrderedDict\n",
                "from collections import OrderedDict"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using Pre-trained DenseNet Models\n",
                "\n",
                "### PyTorch Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torchvision.models as models\n",
                "\n",
                "# Load pre-trained DenseNet model\n",
                "densenet121 = models.densenet121(pretrained=True)\n",
                "densenet169 = models.densenet169(pretrained=True)\n",
                "densenet201 = models.densenet201(pretrained=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TensorFlow/Keras Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\n",
                "\n",
                "# Load pre-trained DenseNet model\n",
                "model_densenet121 = DenseNet121(weights='imagenet', include_top=True)\n",
                "model_densenet169 = DenseNet169(weights='imagenet', include_top=True)\n",
                "model_densenet201 = DenseNet201(weights='imagenet', include_top=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications and Performance\n",
                "\n",
                "DenseNet has shown excellent performance on various computer vision tasks:\n",
                "\n",
                "1. **Image Classification**: Achieved state-of-the-art results on datasets like ImageNet, CIFAR-10, and CIFAR-100.\n",
                "\n",
                "2. **Object Detection**: Served as an effective backbone for detectors like SSD and Faster R-CNN.\n",
                "\n",
                "3. **Semantic Segmentation**: Used as the encoder in many segmentation networks.\n",
                "\n",
                "4. **Medical Imaging**: Applied to various medical image analysis tasks, where the ability to preserve and reuse fine-grained feature information is particularly valuable.\n",
                "\n",
                "5. **Transfer Learning**: Excellent feature extractors for transfer learning tasks.\n",
                "\n",
                "### Advantages in Practice\n",
                "\n",
                "- **Parameter Efficiency**: Achieves similar or better performance than ResNet with fewer parameters.\n",
                "\n",
                "- **Memory Efficiency**: While dense connectivity increases memory usage during training, DenseNet variants can be more memory-efficient than comparable architectures during inference.\n",
                "\n",
                "- **Regularization Effect**: The dense connectivity pattern serves as implicit deep supervision and can reduce overfitting."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison with Other Architectures\n",
                "\n",
                "| Architecture | Key Innovation | Parameter Efficiency | Feature Reuse |\n",
                "|-------------|----------------|----------------------|---------------|\n",
                "| VGG | Uniform 3\u00d73 filters | Low | Limited |\n",
                "| ResNet | Skip connections (addition) | Medium | Partial |\n",
                "| DenseNet | Dense connections (concatenation) | High | Extensive |\n",
                "| Inception | Multi-path processing | Medium | Limited |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Challenges and Limitations\n",
                "\n",
                "1. **Memory Intensive**: Feature concatenation can lead to high memory usage during training.\n",
                "\n",
                "2. **Computational Cost**: Processing concatenated feature maps can be computationally expensive.\n",
                "\n",
                "3. **Implementation Complexity**: The dense connection pattern makes implementation more complex than sequential networks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "1. Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). [Densely connected convolutional networks](https://arxiv.org/abs/1608.06993). In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).\n",
                "\n",
                "2. PyTorch DenseNet Implementation: [torchvision.models.densenet](https://pytorch.org/vision/stable/models.html#densenet)\n",
                "\n",
                "3. TensorFlow DenseNet Implementation: [tf.keras.applications.DenseNet121](https://www.tensorflow.org/api_docs/python/tf/keras/applications/DenseNet121)\n",
                "\n",
                "4. Huang, G., Liu, Z., & Weinberger, K. Q. (2016). [Densely connected convolutional networks](https://arxiv.org/abs/1608.06993v1). arXiv preprint arXiv:1608.06993."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}