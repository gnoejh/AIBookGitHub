{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VGG: A Deep Convolutional Neural Network Architecture\n",
                "\n",
                "## Introduction\n",
                "\n",
                "VGG (Visual Geometry Group) is a deep convolutional neural network architecture created by the Visual Geometry Group at the University of Oxford. It was introduced by Karen Simonyan and Andrew Zisserman in their 2014 paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\" The network achieved impressive results in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, securing the first and second places in the localization and classification tasks respectively.\n",
                "\n",
                "VGG is notable for its simplicity and uniform architecture, using only 3\u00d73 convolutional layers stacked on top of each other with increasing depth.\n",
                "\n",
                "## Historical Importance\n",
                "\n",
                "VGG made several important contributions to the field of deep learning and computer vision:\n",
                "\n",
                "1. Demonstrated that network depth is crucial for good performance\n",
                "2. Showed the effectiveness of using small 3\u00d73 filters throughout the entire network\n",
                "3. Provided evidence that simple, homogeneous architectures can achieve state-of-the-art results\n",
                "4. Became a popular feature extractor for many computer vision tasks beyond classification\n",
                "\n",
                "Following VGG, many architectures continued to explore deeper networks, ultimately leading to architectures like ResNet which introduced skip connections to enable much deeper networks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architecture Overview\n",
                "\n",
                "The VGG architecture comes in several variants, with VGG16 and VGG19 being the most common (the numbers refer to the total number of weighted layers).\n",
                "\n",
                "### Key architectural features of VGG16:\n",
                "\n",
                "1. **Input**: 224\u00d7224\u00d73 RGB images\n",
                "2. **Convolutional Layers**: All use 3\u00d73 filters with stride 1 and same padding\n",
                "3. **Pooling Layers**: 2\u00d72 max pooling with stride 2 (no overlap)\n",
                "4. **Network Depth**: 16 weight layers (13 convolutional + 3 fully connected)\n",
                "5. **Architecture Pattern**: Blocks of convolutional layers followed by max pooling layers\n",
                "6. **Fully Connected Layers**: Three FC layers at the end (4096, 4096, 1000 neurons)\n",
                "7. **Output Layer**: 1000-way softmax (for ImageNet's 1000 classes)\n",
                "\n",
                "### Notable design principles in VGG:\n",
                "\n",
                "- **Consistent Filter Size**: Use of small 3\u00d73 filters throughout the network\n",
                "- **Stacking Small Filters**: Multiple 3\u00d73 filters have the same effective receptive field as larger filters (e.g., two 3\u00d73 filters have a 5\u00d75 receptive field) but with fewer parameters\n",
                "- **Increasing Feature Maps**: Number of feature maps increases as the spatial dimensions decrease\n",
                "- **ReLU Activation**: Used after every convolutional layer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation with PyTorch\n",
                "\n",
                "Let's implement the VGG16 architecture using PyTorch:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages if needed\n",
                "!pip install torch torchvision matplotlib numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Check if CUDA is available\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the VGG16 model\n",
                "class VGG16(nn.Module):\n",
                "    def __init__(self, num_classes=1000):\n",
                "        super(VGG16, self).__init__()\n",
                "        \n",
                "        # Block 1: 64 channels\n",
                "        self.block1 = nn.Sequential(\n",
                "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        )\n",
                "        \n",
                "        # Block 2: 128 channels\n",
                "        self.block2 = nn.Sequential(\n",
                "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        )\n",
                "        \n",
                "        # Block 3: 256 channels\n",
                "        self.block3 = nn.Sequential(\n",
                "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        )\n",
                "        \n",
                "        # Block 4: 512 channels\n",
                "        self.block4 = nn.Sequential(\n",
                "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        )\n",
                "        \n",
                "        # Block 5: 512 channels\n",
                "        self.block5 = nn.Sequential(\n",
                "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        )\n",
                "        \n",
                "        # Classifier (Fully Connected layers)\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Linear(7 * 7 * 512, 4096),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(p=0.5),\n",
                "            nn.Linear(4096, 4096),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(p=0.5),\n",
                "            nn.Linear(4096, num_classes)\n",
                "        )\n",
                "        \n",
                "        # Initialize weights according to the original paper\n",
                "        self._initialize_weights()\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.block1(x)\n",
                "        x = self.block2(x)\n",
                "        x = self.block3(x)\n",
                "        x = self.block4(x)\n",
                "        x = self.block5(x)\n",
                "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
                "        x = self.classifier(x)\n",
                "        return x\n",
                "    \n",
                "    def _initialize_weights(self):\n",
                "        for m in self.modules():\n",
                "            if isinstance(m, nn.Conv2d):\n",
                "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
                "                if m.bias is not None:\n",
                "                    nn.init.constant_(m.bias, 0)\n",
                "            elif isinstance(m, nn.Linear):\n",
                "                nn.init.normal_(m.weight, 0, 0.01)\n",
                "                nn.init.constant_(m.bias, 0)\n",
                "\n",
                "# Create an instance of VGG16\n",
                "model = VGG16(num_classes=1000).to(device)\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using Pre-Trained VGG from torchvision"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load pre-trained VGG16 model from torchvision\n",
                "pretrained_model = torchvision.models.vgg16(pretrained=True)\n",
                "pretrained_model.eval()  # Set to evaluation mode\n",
                "pretrained_model = pretrained_model.to(device)\n",
                "\n",
                "# Load ImageNet class labels\n",
                "import json\n",
                "import urllib.request\n",
                "\n",
                "# Download ImageNet class labels if needed\n",
                "try:\n",
                "    url = \"https://raw.githubusercontent.com/pytorch/examples/master/imagenet/imagenet_classes.txt\"\n",
                "    with urllib.request.urlopen(url) as response:\n",
                "        classes = [line.decode('utf-8').strip() for line in response.readlines()]\n",
                "except:\n",
                "    # Fallback to a smaller subset if download fails\n",
                "    classes = [f\"Class_{i}\" for i in range(1000)]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Image Classification with Pre-trained VGG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "from torchvision import transforms\n",
                "\n",
                "# Define image preprocessing\n",
                "preprocess = transforms.Compose([\n",
                "    transforms.Resize(256),\n",
                "    transforms.CenterCrop(224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "])\n",
                "\n",
                "# Function to make predictions on an image\n",
                "def predict_image(image_path):\n",
                "    # Load and preprocess the image\n",
                "    img = Image.open(image_path)\n",
                "    img_t = preprocess(img)\n",
                "    batch_t = torch.unsqueeze(img_t, 0).to(device)\n",
                "    \n",
                "    # Make a prediction\n",
                "    with torch.no_grad():\n",
                "        output = pretrained_model(batch_t)\n",
                "    \n",
                "    # Get the top 5 predictions\n",
                "    _, indices = torch.sort(output, descending=True)\n",
                "    percentages = torch.nn.functional.softmax(output, dim=1)[0] * 100\n",
                "    results = [(classes[idx], percentages[idx].item()) for idx in indices[0][:5]]\n",
                "    \n",
                "    # Display the image\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    plt.imshow(img)\n",
                "    plt.axis('off')\n",
                "    plt.title(\"Top predictions:\")\n",
                "    \n",
                "    # Display the top 5 predictions\n",
                "    for i, (cls, prob) in enumerate(results):\n",
                "        plt.text(5, 30 + i*20, f\"{cls}: {prob:.2f}%\", fontsize=12, \n",
                "                 bbox=dict(facecolor='white', alpha=0.8))\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# To use this function:\n",
                "# predict_image('path/to/your/image.jpg')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing VGG Filters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_filters(layer_name='features.0'):\n",
                "    \"\"\"\n",
                "    Visualize filters from a specific convolutional layer\n",
                "    layer_name: Name of the layer (e.g., 'features.0' for first conv layer)\n",
                "    \"\"\"\n",
                "    # Get the layer by name\n",
                "    layer = dict([*pretrained_model.named_modules()])[layer_name]\n",
                "    \n",
                "    if not isinstance(layer, nn.Conv2d):\n",
                "        print(f\"Layer {layer_name} is not a convolutional layer.\")\n",
                "        return\n",
                "    \n",
                "    # Get the filter weights\n",
                "    filters = layer.weight.data.cpu().numpy()\n",
                "    \n",
                "    # Number of filters\n",
                "    num_filters = filters.shape[0]\n",
                "    n_cols = 8  # Number of columns in the grid\n",
                "    n_rows = num_filters // n_cols + (1 if num_filters % n_cols != 0 else 0)\n",
                "    \n",
                "    # Create figure for all filters\n",
                "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 2))\n",
                "    \n",
                "    for i in range(n_rows * n_cols):\n",
                "        row, col = i // n_cols, i % n_cols\n",
                "        if i < num_filters:\n",
                "            # Normalize the filter for better visualization\n",
                "            filt = filters[i].transpose(1, 2, 0)\n",
                "            filt = (filt - filt.min()) / (filt.max() - filt.min() + 1e-5)\n",
                "            \n",
                "            if n_rows > 1:\n",
                "                axes[row, col].imshow(filt)\n",
                "                axes[row, col].set_title(f'Filter {i}')\n",
                "                axes[row, col].axis('off')\n",
                "            else:\n",
                "                axes[col].imshow(filt)\n",
                "                axes[col].set_title(f'Filter {i}')\n",
                "                axes[col].axis('off')\n",
                "        else:\n",
                "            if n_rows > 1:\n",
                "                axes[row, col].axis('off')\n",
                "            else:\n",
                "                axes[col].axis('off')\n",
                "                \n",
                "    plt.tight_layout()\n",
                "    plt.suptitle(f\"Layer {layer_name} Filters\", fontsize=16)\n",
                "    plt.subplots_adjust(top=0.92)\n",
                "    plt.show()\n",
                "\n",
                "# Visualize the first convolutional layer filters (64 filters)\n",
                "visualize_filters('features.0')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Map Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_feature_maps(image_path, layer_name='features.1'):\n",
                "    \"\"\"\n",
                "    Visualize feature maps produced by a specific layer\n",
                "    layer_name: Name of the layer to visualize\n",
                "    \"\"\"\n",
                "    # Load and preprocess image\n",
                "    img = Image.open(image_path)\n",
                "    img_t = preprocess(img)\n",
                "    batch_t = torch.unsqueeze(img_t, 0).to(device)\n",
                "    \n",
                "    # Create a hook to capture feature maps\n",
                "    feature_maps = []\n",
                "    \n",
                "    def hook_fn(module, input, output):\n",
                "        feature_maps.append(output.detach().cpu())\n",
                "    \n",
                "    # Get the layer by name\n",
                "    layer = dict([*pretrained_model.named_modules()])[layer_name]\n",
                "    \n",
                "    # Register the hook\n",
                "    hook = layer.register_forward_hook(hook_fn)\n",
                "    \n",
                "    # Forward pass\n",
                "    with torch.no_grad():\n",
                "        pretrained_model(batch_t)\n",
                "    \n",
                "    # Remove the hook\n",
                "    hook.remove()\n",
                "    \n",
                "    # Get feature maps\n",
                "    feature_map = feature_maps[0][0]\n",
                "    \n",
                "    # Plot the original image\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.imshow(img)\n",
                "    plt.title('Original Image')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    # Plot feature maps\n",
                "    n = min(16, feature_map.size(0))  # Display up to 16 feature maps\n",
                "    fig = plt.figure(figsize=(15, 15))\n",
                "    \n",
                "    for i in range(n):\n",
                "        a = fig.add_subplot(4, 4, i+1)\n",
                "        img_map = feature_map[i].numpy()\n",
                "        img_map = (img_map - img_map.min()) / (img_map.max() - img_map.min() + 1e-5)\n",
                "        plt.imshow(img_map, cmap='viridis')\n",
                "        plt.axis('off')\n",
                "        a.set_title(f'Feature Map {i}')\n",
                "        \n",
                "    plt.tight_layout()\n",
                "    plt.suptitle(f'Feature Maps from {layer_name}', fontsize=20)\n",
                "    plt.subplots_adjust(top=0.93)\n",
                "    plt.show()\n",
                "\n",
                "# To use this function:\n",
                "# visualize_feature_maps('path/to/your/image.jpg', 'features.1')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Different VGG Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_vgg_models():\n",
                "    \"\"\"\n",
                "    Compare VGG11, VGG13, VGG16, and VGG19 models in terms of parameters and layers\n",
                "    \"\"\"\n",
                "    models = {\n",
                "        'VGG11': torchvision.models.vgg11(pretrained=False),\n",
                "        'VGG13': torchvision.models.vgg13(pretrained=False),\n",
                "        'VGG16': torchvision.models.vgg16(pretrained=False),\n",
                "        'VGG19': torchvision.models.vgg19(pretrained=False)\n",
                "    }\n",
                "    \n",
                "    # Compare parameters\n",
                "    print(\"VGG Model Comparison:\")\n",
                "    print(\"-\" * 60)\n",
                "    print(f\"{'Model':<10} {'Total Parameters':<20} {'Conv Layers':<15} {'FC Layers':<15}\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    for name, model in models.items():\n",
                "        # Count parameters\n",
                "        total_params = sum(p.numel() for p in model.parameters())\n",
                "        \n",
                "        # Count convolutional layers\n",
                "        conv_layers = sum(1 for m in model.features.modules() if isinstance(m, nn.Conv2d))\n",
                "        \n",
                "        # Count fully connected layers\n",
                "        fc_layers = sum(1 for m in model.classifier.modules() if isinstance(m, nn.Linear))\n",
                "        \n",
                "        print(f\"{name:<10} {total_params:<20,d} {conv_layers:<15} {fc_layers:<15}\")\n",
                "    \n",
                "    print(\"-\" * 60)\n",
                "\n",
                "compare_vgg_models()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## VGG Performance and Historical Context\n",
                "\n",
                "### Performance on ImageNet\n",
                "\n",
                "| Model | Top-1 Accuracy | Top-5 Accuracy | Parameters |\n",
                "|-------|---------------|---------------|------------|\n",
                "| VGG16 (2014) | 71.3% | 90.1% | 138M |\n",
                "| VGG19 (2014) | 71.6% | 90.3% | 144M |\n",
                "| AlexNet (2012) | 57.1% | 80.2% | 60M |\n",
                "| ResNet-50 (2015) | 76.0% | 92.9% | 25M |\n",
                "\n",
                "### Impact and Legacy\n",
                "\n",
                "VGG's impact on deep learning and computer vision has been significant:\n",
                "\n",
                "1. **Simplicity**: Demonstrated that a simple, uniform architecture could achieve excellent results\n",
                "2. **Transfer Learning**: Became a popular feature extractor for many downstream tasks\n",
                "3. **Depth Study**: Provided empirical evidence that increasing network depth improves performance\n",
                "4. **Small Filter Design**: Showed the benefits of using small 3\u00d73 filters throughout the network\n",
                "5. **Feature Visualization**: Its simple structure made it easier to visualize and understand learned features\n",
                "\n",
                "### Limitations\n",
                "\n",
                "VGG also has several notable limitations:\n",
                "\n",
                "- Very large number of parameters (138M for VGG16), making it memory-intensive\n",
                "- Computationally expensive at inference time\n",
                "- No batch normalization in the original architecture\n",
                "- Prone to overfitting due to the large number of parameters\n",
                "- Limited depth compared to more modern architectures like ResNet\n",
                "\n",
                "## Conclusion\n",
                "\n",
                "VGG represents an important evolutionary step in the development of convolutional neural networks. Its simple, uniform design principles made it both effective and easy to understand, while its strong performance solidified the importance of network depth in convolutional architectures. Although newer architectures have since surpassed VGG in terms of accuracy and efficiency, VGG's impact on the field remains significant, and its feature extractors continue to be used in various computer vision applications to this day."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}