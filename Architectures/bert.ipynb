{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "BERT is a transformer-based language model developed by Google in 2018. It revolutionized NLP by introducing deep bidirectional representations, achieving state-of-the-art performance on a wide range of tasks with minimal task-specific architecture modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Innovations\n",
    "\n",
    "BERT introduced several key innovations to language modeling:\n",
    "\n",
    "1. **Bidirectional Training**: Unlike previous models that processed text either left-to-right or right-to-left, BERT processes words in relation to all other words in a sentence simultaneously.\n",
    "\n",
    "2. **Pre-training Tasks**:\n",
    "   - **Masked Language Modeling (MLM)**: Randomly mask 15% of tokens and train the model to predict the original vocabulary ID of the masked word based on its context.\n",
    "   - **Next Sentence Prediction (NSP)**: Train the model to understand relationships between sentences by predicting whether sentence B follows sentence A in the original text.\n",
    "\n",
    "3. **Transfer Learning for NLP**: BERT demonstrated that a model pre-trained on large text corpora could be fine-tuned for specific downstream tasks with minimal additional training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "BERT's architecture is based on the Transformer encoder from \"Attention is All You Need\":\n",
    "\n",
    "- **BERT-Base**: 12 transformer layers, 12 attention heads, 768 hidden dimensions (110M parameters)\n",
    "- **BERT-Large**: 24 transformer layers, 16 attention heads, 1024 hidden dimensions (340M parameters)\n",
    "\n",
    "![BERT Architecture](https://miro.medium.com/max/700/1*0bxsEnVxnHIg2g5E5Y6q0g.png)\n",
    "\n",
    "### Input Representation\n",
    "\n",
    "BERT's input representation is the sum of three embeddings:\n",
    "\n",
    "1. **Token Embeddings**: WordPiece embeddings with 30,000 token vocabulary\n",
    "2. **Segment Embeddings**: Distinguish between pairs of sentences (0 for first sentence, 1 for second)\n",
    "3. **Position Embeddings**: Indicate the position of each token in the sequence\n",
    "\n",
    "![BERT Input Representation](https://miro.medium.com/max/700/1*vLF7q-ktD73lmv6oL5bW3A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training Process\n",
    "\n",
    "BERT was pre-trained on:\n",
    "- BooksCorpus (800M words)\n",
    "- English Wikipedia (2,500M words)\n",
    "\n",
    "### Masked Language Modeling (MLM)\n",
    "\n",
    "For each training example:\n",
    "1. Randomly select 15% of tokens\n",
    "2. Replace 80% of selected tokens with [MASK]\n",
    "3. Replace 10% with random words\n",
    "4. Keep 10% unchanged\n",
    "\n",
    "The model then predicts the original token at masked positions using context from both directions.\n",
    "\n",
    "### Next Sentence Prediction (NSP)\n",
    "\n",
    "For each training example:\n",
    "1. Choose 50% of samples where sentence B actually follows sentence A (labeled \"IsNext\")\n",
    "2. Choose 50% where sentence B is a random sentence from the corpus (labeled \"NotNext\")\n",
    "3. Train the model to classify whether sentence B is the actual next sentence or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning for Downstream Tasks\n",
    "\n",
    "BERT can be fine-tuned for various NLP tasks with minimal architectural changes:\n",
    "\n",
    "- **Classification Tasks** (Sentiment analysis, NLI): Add a classification layer on top of the [CLS] token output\n",
    "- **Question Answering**: Add start/end span prediction layers\n",
    "- **Named Entity Recognition**: Use outputs from all tokens for token-level predictions\n",
    "- **Paraphrasing**: Use the [CLS] token for sentence pair classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT with Hugging Face Transformers\n",
    "\n",
    "The Transformers library provides a convenient way to use BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text\n",
    "text = \"Here's a sentence to encode with BERT.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Last hidden states contains contextual embeddings for each token\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(f\"Shape of output embeddings: {last_hidden_states.shape}\")\n",
    "\n",
    "# CLS token embedding (often used for classification tasks)\n",
    "cls_embedding = last_hidden_states[:, 0, :]\n",
    "print(f\"Shape of CLS embedding: {cls_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sample dataset class (replace with real dataset)\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Example fine-tuning setup\n",
    "def fine_tune_bert(train_texts, train_labels, val_texts, val_labels):\n",
    "    # Load pre-trained BERT model for sequence classification\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(train_loader) * 3  # 3 epochs\n",
    "    )\n",
    "    \n",
    "    # Training loop (simplified)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(3):  # 3 epochs\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        # ... validation code ...\n",
    "        \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Variants and Evolution\n",
    "\n",
    "Since its introduction, several variants of BERT have been developed:\n",
    "\n",
    "1. **RoBERTa** (Facebook, 2019): Removes NSP, uses dynamic masking, larger batches, more data\n",
    "2. **DistilBERT** (Hugging Face, 2019): Knowledge distillation to create smaller, faster BERT model\n",
    "3. **ALBERT** (Google, 2020): Parameter reduction techniques for more efficient training\n",
    "4. **ELECTRA** (Google, 2020): Replaced MLM with replaced token detection for more efficient training\n",
    "5. **DeBERTa** (Microsoft, 2021): Enhanced attention mechanisms with disentangled attention\n",
    "\n",
    "Each variant addresses specific limitations or improves certain aspects of the original BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of BERT\n",
    "\n",
    "BERT has been successfully applied to a wide range of NLP tasks:\n",
    "\n",
    "- **Text Classification**: Sentiment analysis, topic classification, toxicity detection\n",
    "- **Question Answering**: SQuAD, Natural Questions\n",
    "- **Named Entity Recognition**: Identifying persons, locations, organizations in text\n",
    "- **Text Summarization**: When combined with generation components\n",
    "- **Information Retrieval**: Search engine results ranking\n",
    "- **Language Understanding**: GLUE benchmark tasks (NLI, paraphrasing)\n",
    "- **Document Classification**: Legal, medical, scientific document categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of BERT\n",
    "\n",
    "Despite its success, BERT has several limitations:\n",
    "\n",
    "1. **Maximum Sequence Length**: Limited to 512 tokens, making it unsuitable for long document processing\n",
    "2. **Computational Requirements**: Large model size requires significant computational resources\n",
    "3. **Encoder-only Architecture**: Not directly suitable for text generation tasks\n",
    "4. **Static Knowledge**: Knowledge limited to pre-training data, no ability to update knowledge\n",
    "5. **Domain Specificity**: May require domain adaptation for specialized fields (medical, legal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "BERT represents a major milestone in NLP, introducing techniques that have become standard in modern language models:\n",
    "\n",
    "- Bidirectional context understanding\n",
    "- Pre-training and fine-tuning paradigm\n",
    "- Transformer-based architectures for language understanding\n",
    "\n",
    "Its success paved the way for subsequent models like GPT, T5, and other transformer-based architectures that continue to advance the state of natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Devlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). arXiv.\n",
    "- Vaswani, A., et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). NIPS.\n",
    "- Liu, Y., et al. (2019). [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692). arXiv.\n",
    "- Sanh, V., et al. (2019). [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). arXiv.\n",
    "- Clark, K., et al. (2020). [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555). ICLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
