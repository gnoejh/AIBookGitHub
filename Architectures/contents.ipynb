{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/AI/blob/main/Book/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architectures\n",
    "\n",
    "This document catalogs various neural network architectures that have been developed and widely used across different domains. These architectures represent different ways of organizing the fundamental building blocks to solve specific types of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Library\n",
    "\n",
    "### Feedforward Networks\n",
    "- [Multilayer Perceptron](./mlp.ipynb) - Classic fully-connected neural networks\n",
    "- [Deep Belief Networks](./dbn.ipynb) - Stacked Restricted Boltzmann Machines with deep architectures\n",
    "\n",
    "### Convolutional Networks\n",
    "- [LeNet](./lenet.ipynb) - One of the earliest CNNs for digit recognition\n",
    "- [AlexNet](./alexnet.ipynb) - Breakthrough CNN architecture that advanced image classification\n",
    "- [VGG](./vgg.ipynb) - Deep CNN with uniform 3x3 convolutions throughout\n",
    "- [ResNet](./resnet.ipynb) - Extremely deep networks with residual connections\n",
    "- [Inception/GoogLeNet](./inception.ipynb) - Network architecture with parallel convolution paths\n",
    "- [DenseNet](./densenet.ipynb) - Dense connections between layers to maximize information flow\n",
    "- [EfficientNet](./efficientnet.ipynb) - Scaling networks for improved accuracy and efficiency\n",
    "- [MobileNet](./mobilenet.ipynb) - Lightweight models optimized for mobile and embedded devices\n",
    "- [U-Net](./unet.ipynb) - Encoder-decoder architecture with skip connections for image segmentation\n",
    "\n",
    "### Sequence Models\n",
    "- [Recurrent Neural Network](./rnn.ipynb) - Networks with feedback connections for sequence data\n",
    "- [LSTM](./lstm.ipynb) - Long Short-Term Memory networks for long-range dependencies\n",
    "- [GRU](./gru.ipynb) - Gated Recurrent Units, simplified variant of LSTMs\n",
    "\n",
    "### Attention-Based Models\n",
    "- [Transformer](./transformer.ipynb) - Attention-based architecture that revolutionized NLP\n",
    "- [BERT](./bert.ipynb) - Bidirectional Encoder from Transformer for language understanding\n",
    "- [GPT](./gpt.ipynb) - Generative Pre-trained Transformer for text generation\n",
    "- [Gemini](./gemini.ipynb) - Multimodal model capable of understanding and generating text, images and other modalities\n",
    "- [Vision Transformer](./vit.ipynb) - Applying transformer architecture to computer vision tasks\n",
    "\n",
    "### Generative Models\n",
    "- [Autoencoder](./autoencoder.ipynb) - Encoding and decoding for unsupervised learning\n",
    "- [Variational Autoencoder](./vae.ipynb) - Probabilistic autoencoders for generating new data\n",
    "- [GAN](./gan.ipynb) - Generative Adversarial Networks for realistic data generation\n",
    "- [Diffusion Models](./diffusion.ipynb) - Noise-based generative models with high fidelity\n",
    "- [Stable Diffusion](./stable_diffusion.ipynb) - Latent diffusion models for high-quality image generation\n",
    "\n",
    "### Specialized Architectures\n",
    "- [Siamese Networks](./siamese.ipynb) - Twin networks for similarity learning and one-shot learning\n",
    "- [Graph Neural Networks](./gnn.ipynb) - Networks operating on graph-structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Applications\n",
    "\n",
    "Different neural network architectures excel in specific domains:\n",
    "\n",
    "- **Computer Vision**: CNN-based architectures (LeNet, AlexNet, ResNet, etc.)\n",
    "- **Natural Language Processing**: Sequence models and Transformers (LSTM, BERT, GPT)\n",
    "- **Speech Recognition**: Hybrid CNN-RNN architectures\n",
    "- **Generative Tasks**: GANs, VAEs, Diffusion Models\n",
    "- **Reinforcement Learning**: Deep Q-Networks, Actor-Critic Networks\n",
    "- **Graph Analysis**: Graph Neural Networks\n",
    "- **Medical Imaging**: U-Net and its variants for segmentation tasks\n",
    "- **Mobile Applications**: MobileNet, EfficientNet for resource-constrained environments\n",
    "\n",
    "Each architecture notebook contains historical context, architectural details, and common use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Evolution\n",
    "\n",
    "Neural network architectures have evolved to address limitations of previous designs:\n",
    "\n",
    "- **Deeper Networks**: From shallow MLPs to extremely deep ResNets (152+ layers)\n",
    "- **Vanishing Gradients**: RNNs → LSTMs/GRUs with gating mechanisms\n",
    "- **Sequential Bottlenecks**: RNNs → Transformers with parallel processing\n",
    "- **Information Flow**: Plain networks → Skip connections, Dense connections\n",
    "- **Computational Efficiency**: Large models → Techniques like MobileNet, EfficientNet\n",
    "\n",
    "Understanding this evolutionary path helps in designing new architectures for emerging problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Frameworks\n",
    "\n",
    "Most modern architectures are implemented in standard deep learning frameworks:\n",
    "\n",
    "### TensorFlow/Keras\n",
    "```python\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, InceptionV3\n",
    "\n",
    "# Load pre-trained model\n",
    "model = ResNet50(weights='imagenet')\n",
    "```\n",
    "\n",
    "### PyTorch\n",
    "```python\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "```\n",
    "\n",
    "### Hugging Face (for NLP models)\n",
    "```python\n",
    "from transformers import BertModel, GPT2Model\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "\n",
    "- LeCun, Y., et al. (1998). [Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf). Proceedings of the IEEE.\n",
    "- Krizhevsky, A., et al. (2012). [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). NIPS.\n",
    "- He, K., et al. (2015). [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385). CVPR.\n",
    "- Hochreiter, S., & Schmidhuber, J. (1997). [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf). Neural Computation.\n",
    "- Vaswani, A., et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). NIPS.\n",
    "- Goodfellow, I., et al. (2014). [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661). NIPS.\n",
    "- Devlin, J., et al. (2018). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). arXiv.\n",
    "- Ronneberger, O., et al. (2015). [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597). MICCAI.\n",
    "- Howard, A. G., et al. (2017). [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861). arXiv."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
