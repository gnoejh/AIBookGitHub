{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/mobilenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MobileNet Architecture\n",
                "\n",
                "## Overview\n",
                "\n",
                "MobileNet is a class of lightweight convolutional neural networks designed for mobile and embedded vision applications. These models are specifically engineered to be efficient in terms of size and speed while maintaining reasonable accuracy, making them ideal for deployment on resource-constrained devices.\n",
                "\n",
                "## Architecture\n",
                "\n",
                "The key innovation in MobileNet is the use of **depthwise separable convolutions** instead of standard convolutions. Each standard convolution is factored into:\n",
                "\n",
                "1. A **depthwise convolution** - applies a single filter per input channel\n",
                "2. A **pointwise convolution** (1\u00d71 convolution) - combines the outputs from depthwise convolution\n",
                "\n",
                "This factorization dramatically reduces computation and model size:\n",
                "- Standard convolution: O(D_K \u00b7 D_K \u00b7 M \u00b7 N \u00b7 D_F \u00b7 D_F)\n",
                "- Depthwise separable: O(D_K \u00b7 D_K \u00b7 M \u00b7 D_F \u00b7 D_F + M \u00b7 N \u00b7 D_F \u00b7 D_F)\n",
                "\n",
                "Where:\n",
                "- D_K: Kernel size\n",
                "- M: Input channels\n",
                "- N: Output channels\n",
                "- D_F: Feature map size\n",
                "\n",
                "## MobileNet Versions\n",
                "\n",
                "### MobileNet V1\n",
                "- Introduced depthwise separable convolutions\n",
                "- Used ReLU6 activation\n",
                "- Added width multiplier and resolution multiplier to further reduce computation\n",
                "\n",
                "### MobileNet V2\n",
                "- Added inverted residual blocks\n",
                "- Linear bottlenecks between layers\n",
                "- Skip connections for feature reuse\n",
                "\n",
                "### MobileNet V3\n",
                "- Combined automated architecture search with human design\n",
                "- Squeeze-and-Excitation blocks\n",
                "- Redesigned expensive layers\n",
                "- Hard-Swish activation\n",
                "\n",
                "## Applications\n",
                "\n",
                "- Mobile and embedded computer vision\n",
                "- Real-time object detection\n",
                "- Facial recognition on edge devices\n",
                "- Augmented reality\n",
                "- Low-power IoT vision systems\n",
                "\n",
                "## TensorFlow Implementation (MobileNet V2)\n",
                "\n",
                "```python\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.applications import MobileNetV2\n",
                "\n",
                "# Load pre-trained MobileNetV2\n",
                "model = MobileNetV2(\n",
                "    input_shape=(224, 224, 3),\n",
                "    alpha=1.0,  # Width multiplier\n",
                "    include_top=True,\n",
                "    weights='imagenet',\n",
                "    classes=1000\n",
                ")\n",
                "\n",
                "# For transfer learning\n",
                "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
                "base_model.trainable = False\n",
                "\n",
                "# Add custom classification head\n",
                "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
                "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
                "predictions = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
                "custom_model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
                "```\n",
                "\n",
                "## Performance Metrics\n",
                "\n",
                "| Model | Parameters | MAdds | Top-1 Accuracy | Top-5 Accuracy |\n",
                "|---|---|---|---|---|\n",
                "| MobileNet V1 (1.0) | 4.2M | 569M | 70.6% | 89.5% |\n",
                "| MobileNet V2 (1.0) | 3.4M | 300M | 72.0% | 91.0% |\n",
                "| MobileNet V3-Large | 5.4M | 219M | 75.2% | 92.2% |\n",
                "| MobileNet V3-Small | 2.5M | 66M | 67.4% | 86.4% |\n",
                "\n",
                "## References\n",
                "\n",
                "- Howard, A. G., et al. (2017). [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861). arXiv.\n",
                "- Sandler, M., et al. (2018). [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381). CVPR.\n",
                "- Howard, A., et al. (2019). [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244). ICCV."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}