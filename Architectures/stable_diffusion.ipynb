{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/stable_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stable Diffusion\n",
                "\n",
                "Stable Diffusion is a latent text-to-image diffusion model released in 2022 that has revolutionized AI-based image generation. It was developed by researchers at CompVis, Stability AI, and LAION."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architecture Overview\n",
                "\n",
                "Stable Diffusion differs from earlier diffusion models by operating in the latent space rather than pixel space. The architecture consists of three main components:\n",
                "\n",
                "1. **Variational Autoencoder (VAE)**: Compresses images into a lower-dimensional latent space and decompresses latents back to images\n",
                "2. **U-Net**: Performs the denoising process in the latent space\n",
                "3. **Text Encoder**: Converts text prompts into embeddings to condition the generation process\n",
                "\n",
                "![Stable Diffusion Architecture](https://miro.medium.com/v2/resize:fit:1400/1*y881_vJSBuz7bHDv9O7cpA.png)\n",
                "*Simplified architecture diagram of Stable Diffusion*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Components\n",
                "\n",
                "### Variational Autoencoder (VAE)\n",
                "\n",
                "The VAE in Stable Diffusion is used to:\n",
                "- **Encode**: Convert high-dimensional images (e.g., 512×512×3) into lower-dimensional latent representations (e.g., 64×64×4)\n",
                "- **Decode**: Convert the generated latent representations back into full images\n",
                "\n",
                "This compression allows diffusion to work in a much smaller space, dramatically reducing computational requirements."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### U-Net with Cross-Attention\n",
                "\n",
                "The core denoising U-Net includes:\n",
                "- **Residual blocks**: For efficient gradient flow\n",
                "- **Self-attention layers**: To model long-range dependencies in the spatial domain\n",
                "- **Cross-attention layers**: To condition the generation on text embeddings\n",
                "\n",
                "```\n",
                "U-Net Structure:\n",
                "- Downsampling path (encoder)\n",
                "- Bottleneck\n",
                "- Upsampling path (decoder) with skip connections\n",
                "```\n",
                "\n",
                "The cross-attention mechanism is key to text conditioning and works by:\n",
                "1. Using text embeddings as keys and values\n",
                "2. Using latent representations as queries\n",
                "3. Computing attention maps between them"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Text Encoder (CLIP)\n",
                "\n",
                "Stable Diffusion uses a pretrained text encoder from CLIP (Contrastive Language-Image Pre-training) to convert text prompts into embeddings. These embeddings:\n",
                "\n",
                "- Have a rich semantic understanding of language\n",
                "- Guide the denoising process through cross-attention\n",
                "- Allow for complex, nuanced text prompts\n",
                "\n",
                "The text encoder is typically frozen during training and inference."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Diffusion Process\n",
                "\n",
                "Stable Diffusion follows a standard diffusion process but in latent space:\n",
                "\n",
                "1. **Forward Process**: Gradually adds Gaussian noise to latent representations\n",
                "2. **Reverse Process**: Learns to denoise step-by-step, conditioned on text\n",
                "\n",
                "During inference, the process starts with pure noise and progressively denoises it using the U-Net:\n",
                "\n",
                "```python\n",
                "# Simplified pseudocode for the inference process\n",
                "def generate_image(prompt, steps=50):\n",
                "    # Encode text prompt\n",
                "    text_embedding = text_encoder(prompt)\n",
                "    \n",
                "    # Start with random noise\n",
                "    latent = random_noise(shape=(batch_size, 4, 64, 64))\n",
                "    \n",
                "    # Gradually denoise\n",
                "    for t in reversed(range(steps)):\n",
                "        noise_pred = unet(latent, timestep=t, context=text_embedding)\n",
                "        latent = denoising_step(latent, noise_pred, t)\n",
                "    \n",
                "    # Decode to image\n",
                "    image = vae_decoder(latent)\n",
                "    return image\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Process\n",
                "\n",
                "Training Stable Diffusion involves:\n",
                "\n",
                "1. **Pretraining the VAE**: To learn efficient latent representations of images\n",
                "2. **Training the U-Net**: To denoise latent representations conditioned on text\n",
                "\n",
                "The model is typically trained on millions of image-text pairs from datasets like LAION-5B."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advantages of Latent Diffusion\n",
                "\n",
                "Working in the latent space offers several advantages:\n",
                "\n",
                "1. **Computational Efficiency**: Reducing dimensions cuts memory and computation by ~10-20×\n",
                "2. **Training Stability**: Smoother training in the compressed latent space\n",
                "3. **Image Quality**: Maintains high fidelity despite working in a compressed space\n",
                "4. **Versatility**: Easily extended to various conditioning types (text, image, class labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Variants and Extensions\n",
                "\n",
                "Stable Diffusion has evolved with several variants:\n",
                "\n",
                "- **Stable Diffusion XL**: Larger model with improved image quality and text understanding\n",
                "- **Stable Diffusion 2.0/2.1**: Improved versions with better generation quality\n",
                "- **ControlNet**: Extension allowing fine-grained control over image generation with additional inputs\n",
                "- **Img2Img**: Modification technique to transform existing images\n",
                "- **Inpainting**: Targeted image editing while preserving surroundings\n",
                "- **DreamBooth**: Fine-tuning for personalized content generation\n",
                "- **Textual Inversion**: Learning new concepts from a few example images"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation Example\n",
                "\n",
                "Here's how to use Stable Diffusion with the Diffusers library:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if needed\n",
                "# !pip install diffusers transformers accelerate torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'diffusers'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pipeline\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
                        "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diffusers'"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from diffusers import StableDiffusionPipeline\n",
                "\n",
                "# Load the pipeline\n",
                "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
                "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
                "pipe = pipe.to(\"cuda\")\n",
                "\n",
                "# Generate an image\n",
                "prompt = \"A beautiful mountain landscape with a lake at sunset, highly detailed digital art\"\n",
                "image = pipe(prompt).images[0]\n",
                "\n",
                "# Display the image\n",
                "image.save(\"generated_landscape.png\")\n",
                "display(image)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications\n",
                "\n",
                "Stable Diffusion has found applications in:\n",
                "\n",
                "- **Creative Art**: Digital art creation, concept art, illustrations\n",
                "- **Design**: Product design, UI/UX mockups, architectural visualization\n",
                "- **Content Creation**: Marketing materials, social media assets\n",
                "- **Entertainment**: Game asset creation, film pre-visualization\n",
                "- **Education**: Visual aids, educational content\n",
                "- **Research**: Data augmentation, synthetic training data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advanced Techniques\n",
                "\n",
                "### Prompt Engineering\n",
                "\n",
                "Creating effective prompts is crucial for good results:\n",
                "\n",
                "```\n",
                "# Basic prompt structure\n",
                "[Subject], [Details], [Style], [Artist], [Quality]\n",
                "\n",
                "# Example\n",
                "\"A majestic lion, detailed fur, standing on a rock at sunset, wildlife photography, 8K, sharp focus, by National Geographic\"\n",
                "```\n",
                "\n",
                "### Negative Prompts\n",
                "\n",
                "Specifying what to avoid in the generation:\n",
                "\n",
                "```python\n",
                "negative_prompt = \"blurry, low quality, distorted, deformed, disfigured, bad anatomy\"\n",
                "image = pipe(prompt, negative_prompt=negative_prompt).images[0]\n",
                "```\n",
                "\n",
                "### Guidance Scale\n",
                "\n",
                "Controlling adherence to the prompt (higher values = more prompt adherence):\n",
                "\n",
                "```python\n",
                "image = pipe(prompt, guidance_scale=7.5).images[0]  # Default is 7.5\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Limitations\n",
                "\n",
                "Despite its capabilities, Stable Diffusion has several limitations:\n",
                "\n",
                "- **Text Understanding**: Still struggles with complex instructions or composition\n",
                "- **Anatomical Accuracy**: Often produces anatomical errors, especially with humans\n",
                "- **Coherence**: May have issues with logical consistency in complex scenes\n",
                "- **Latent Space Compression**: Some fine details might be lost in the latent space\n",
                "- **Biases**: Models may reflect dataset biases in their outputs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "- Rombach, R., et al. (2022). [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752). CVPR 2022.\n",
                "- Podell, D., et al. (2023). [SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://arxiv.org/abs/2307.01952). arXiv:2307.01952.\n",
                "- Zhang, L., et al. (2023). [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543). ICCV 2023.\n",
                "- Dhariwal, P., & Nichol, A. (2021). [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233). NeurIPS 2021.\n",
                "- Sohl-Dickstein, J., et al. (2015). [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585). ICML 2015.\n",
                "- Radford, A., et al. (2021). [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020). ICML 2021."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "py312sb3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
