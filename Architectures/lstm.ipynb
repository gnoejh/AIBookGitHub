{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Long Short-Term Memory (LSTM) Networks\n",
                "\n",
                "Long Short-Term Memory (LSTM) networks are a specialized form of Recurrent Neural Networks (RNNs) designed to overcome the vanishing gradient problem that affects standard RNNs. Developed by Hochreiter & Schmidhuber in 1997, LSTMs have become fundamental building blocks for many sequence modeling tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Vanishing Gradient Problem\n",
                "\n",
                "Standard RNNs suffer from the vanishing gradient problem when dealing with long sequences:\n",
                "\n",
                "- During backpropagation through time, gradients are multiplied many times by the recurrent weight matrix\n",
                "- If eigenvalues are < 1, gradients shrink exponentially with sequence length\n",
                "- If eigenvalues are > 1, gradients explode\n",
                "- This makes learning long-range dependencies extremely difficult\n",
                "\n",
                "LSTMs were specifically designed to address this limitation by introducing a memory cell with gating mechanisms."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. LSTM Architecture\n",
                "\n",
                "The core innovation of LSTMs is the cell state (memory cell) that runs through the entire sequence, with gates controlling information flow.\n",
                "\n",
                "### Key Components\n",
                "\n",
                "An LSTM cell contains:\n",
                "\n",
                "1. **Cell State (C)**: Long-term memory that passes through the entire sequence with minimal modification\n",
                "2. **Hidden State (h)**: Short-term memory/output at each time step\n",
                "3. **Forget Gate (f)**: Controls what information to discard from the cell state\n",
                "4. **Input Gate (i)**: Controls what new information to store in the cell state\n",
                "5. **Output Gate (o)**: Controls what parts of the cell state to output\n",
                "\n",
                "![LSTM Cell Architecture](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
                "*Image source: Christopher Olah's blog*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. LSTM Operations\n",
                "\n",
                "The LSTM performs the following operations at each time step $t$:\n",
                "\n",
                "### Forget Gate\n",
                "Decides what information to throw away from the cell state:\n",
                "\n",
                "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
                "\n",
                "### Input Gate\n",
                "Decides what new information to store in the cell state:\n",
                "\n",
                "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
                "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
                "\n",
                "### Cell State Update\n",
                "Updates the old cell state into the new cell state:\n",
                "\n",
                "$$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
                "\n",
                "### Output Gate\n",
                "Decides what parts of the cell state to output:\n",
                "\n",
                "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
                "$$h_t = o_t * \\tanh(C_t)$$\n",
                "\n",
                "Where:\n",
                "- $\\sigma$ is the sigmoid function\n",
                "- $\\tanh$ is the hyperbolic tangent function\n",
                "- $*$ represents element-wise multiplication"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. LSTM Variants\n",
                "\n",
                "Several LSTM variants have been proposed to simplify or enhance the original architecture:\n",
                "\n",
                "### Peephole Connections\n",
                "- Allows gates to \"peek\" at the cell state by adding connections from cell state to gates\n",
                "- Can help with precise timing tasks\n",
                "\n",
                "### Coupled Forget and Input Gates\n",
                "- Instead of separate decisions about what to forget and what to add, these decisions are coupled\n",
                "- If we forget something, we add something in its place\n",
                "\n",
                "### Gated Recurrent Unit (GRU)\n",
                "- Simplified version of LSTM that combines forget and input gates into a single \"update gate\"\n",
                "- Merges cell state and hidden state\n",
                "- Often performs similarly to LSTM but with fewer parameters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Implementation Example\n",
                "\n",
                "### PyTorch Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output shape: torch.Size([3, 5, 20])\n",
                        "Hidden state shape: torch.Size([2, 3, 20])\n",
                        "Cell state shape: torch.Size([2, 3, 20])\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# LSTM parameters\n",
                "input_size = 10  # Size of input features\n",
                "hidden_size = 20  # Size of hidden state\n",
                "num_layers = 2  # Number of LSTM layers\n",
                "batch_size = 3  # Batch size\n",
                "seq_length = 5  # Sequence length\n",
                "\n",
                "# Create an LSTM layer\n",
                "lstm = nn.LSTM(input_size=input_size, \n",
                "               hidden_size=hidden_size,\n",
                "               num_layers=num_layers,\n",
                "               batch_first=True)  # batch_first=True means input shape is (batch, seq, feature)\n",
                "\n",
                "# Example input: (batch_size, seq_length, input_size)\n",
                "x = torch.randn(batch_size, seq_length, input_size)\n",
                "\n",
                "# Initial hidden state and cell state\n",
                "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
                "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
                "\n",
                "# Forward pass\n",
                "output, (hn, cn) = lstm(x, (h0, c0))\n",
                "\n",
                "print(f\"Output shape: {output.shape}\")  # (batch_size, seq_length, hidden_size)\n",
                "print(f\"Hidden state shape: {hn.shape}\")  # (num_layers, batch_size, hidden_size)\n",
                "print(f\"Cell state shape: {cn.shape}\")  # (num_layers, batch_size, hidden_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TensorFlow/Keras Implementation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Building an LSTM-Based Sequence Model\n",
                "\n",
                "Here's how to build a complete sequence model using LSTMs:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model output shape: torch.Size([8, 15, 5])\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class LSTMSequenceModel(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0):\n",
                "        super(LSTMSequenceModel, self).__init__()\n",
                "        \n",
                "        self.hidden_size = hidden_size\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        # LSTM layer\n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=input_size,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            batch_first=True,\n",
                "            dropout=dropout if num_layers > 1 else 0\n",
                "        )\n",
                "        \n",
                "        # Output layer\n",
                "        self.fc = nn.Linear(hidden_size, output_size)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x shape: (batch_size, seq_length, input_size)\n",
                "        \n",
                "        # Initialize hidden state with zeros\n",
                "        batch_size = x.size(0)\n",
                "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
                "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
                "            \n",
                "        # Forward propagate LSTM\n",
                "        # out shape: (batch_size, seq_length, hidden_size)\n",
                "        out, _ = self.lstm(x, (h0, c0))  \n",
                "        \n",
                "        # Decode the hidden state of the last time step\n",
                "        # For sequence-to-sequence, use all outputs\n",
                "        out = self.fc(out)\n",
                "        return out\n",
                "\n",
                "# Example usage\n",
                "model = LSTMSequenceModel(input_size=10, hidden_size=32, output_size=5, num_layers=2)\n",
                "sample_input = torch.randn(8, 15, 10)  # batch_size=8, seq_length=15, input_size=10\n",
                "output = model(sample_input)\n",
                "print(f\"Model output shape: {output.shape}\")  # Should be [8, 15, 5]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Common Applications\n",
                "\n",
                "LSTMs excel in various sequence modeling tasks:\n",
                "\n",
                "### Natural Language Processing\n",
                "- Language modeling\n",
                "- Machine translation\n",
                "- Text generation\n",
                "- Sentiment analysis\n",
                "- Named entity recognition\n",
                "\n",
                "### Time Series Analysis\n",
                "- Stock price prediction\n",
                "- Weather forecasting\n",
                "- Anomaly detection in sensor data\n",
                "- Energy load forecasting\n",
                "\n",
                "### Speech and Audio\n",
                "- Speech recognition\n",
                "- Speech synthesis\n",
                "- Music generation\n",
                "- Audio classification\n",
                "\n",
                "### Other Applications\n",
                "- Gesture recognition\n",
                "- Video analysis\n",
                "- Handwriting recognition\n",
                "- DNA sequence analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Advantages and Limitations\n",
                "\n",
                "### Advantages\n",
                "- Effectively captures long-range dependencies in sequences\n",
                "- Mitigates the vanishing gradient problem\n",
                "- Maintains relevant information over many time steps\n",
                "- Works well with variable-length sequences\n",
                "- Robust against noise in temporal data\n",
                "\n",
                "### Limitations\n",
                "- Computationally more expensive than standard RNNs\n",
                "- Sequential nature prevents parallelization during training\n",
                "- May struggle with very long sequences (1000+ steps)\n",
                "- Requires careful initialization and regularization\n",
                "- Being supplanted by Transformer models in many NLP applications\n",
                "- Limited receptive field compared to attention-based models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Best Practices for LSTM Models\n",
                "\n",
                "### Architecture Design\n",
                "- Start with 1-2 LSTM layers before adding complexity\n",
                "- Use bidirectional LSTMs for tasks where future context is important\n",
                "- Consider adding residual connections for very deep LSTM networks\n",
                "\n",
                "### Training Tips\n",
                "- Use gradient clipping to prevent exploding gradients\n",
                "- Apply dropout between LSTM layers, not within recurrent connections\n",
                "- Normalize inputs for faster convergence\n",
                "- Consider using different learning rates for LSTM and output layers\n",
                "\n",
                "### Sequence Handling\n",
                "- Use padding and masking for variable-length sequences\n",
                "- Consider applying attention mechanisms for very long sequences\n",
                "- Truncated backpropagation through time can help with memory constraints"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. References and Further Reading\n",
                "\n",
                "1. Hochreiter, S., & Schmidhuber, J. (1997). [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf). Neural Computation, 9(8), 1735-1780.\n",
                "\n",
                "2. Graves, A., & Schmidhuber, J. (2005). [Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures](https://www.cs.toronto.edu/~graves/nn_2005.pdf). Neural Networks, 18(5-6), 602-610.\n",
                "\n",
                "3. Greff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., & Schmidhuber, J. (2016). [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf). IEEE Transactions on Neural Networks and Learning Systems, 28(10), 2222-2232.\n",
                "\n",
                "4. Colah's Blog: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
                "\n",
                "5. Olah, C., & Carter, S. (2016). [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/). Distill.\n",
                "\n",
                "6. Goodfellow, I., Bengio, Y., & Courville, A. (2016). [Deep Learning](https://www.deeplearningbook.org/). MIT Press. (Chapter 10: Sequence Modeling)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "py312sb3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
