{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/deepseek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DeepSeek Architecture\n",
                "\n",
                "## Overview\n",
                "\n",
                "DeepSeek is a family of large language models developed by DeepSeek AI, with particular focus on coding capabilities. The models have gained recognition for their strong performance on code generation, math reasoning, and general language tasks while maintaining an open approach to development.\n",
                "\n",
                "## Key Features\n",
                "\n",
                "- **Code-First Design**: Particularly strong performance on programming tasks across multiple languages\n",
                "- **Math Reasoning**: Enhanced capabilities for mathematical reasoning and problem-solving\n",
                "- **Multiple Size Variants**: Available in various parameter sizes (7B to 67B)\n",
                "- **Open Weights**: Weights publicly available for research and development\n",
                "- **Bilingual Training**: Strong performance in both English and Chinese\n",
                "\n",
                "## Architecture Specifics\n",
                "\n",
                "DeepSeek models are based on the Transformer architecture with several optimizations:\n",
                "\n",
                "- Decoder-only design similar to GPT and Llama architectures\n",
                "- SwiGLU activation function\n",
                "- RMSNorm for layer normalization\n",
                "- Rotary positional embeddings (RoPE)\n",
                "- Group Query Attention for improved efficiency\n",
                "- Extended context length (up to 128K tokens depending on variant)\n",
                "\n",
                "## Variants\n",
                "\n",
                "- **DeepSeek-LLM**: Base models focused on general language capabilities\n",
                "- **DeepSeek-Coder**: Models specifically optimized for programming tasks\n",
                "- **DeepSeek-Math**: Specialized for mathematical reasoning and problem-solving\n",
                "- **DeepSeek-V2**: Updated architecture with enhanced capabilities\n",
                "\n",
                "## Usage Examples\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "\n",
                "# Load the DeepSeek Coder model\n",
                "model_id = \"deepseek-ai/deepseek-coder-7b-instruct\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
                "\n",
                "# Format prompt for code generation\n",
                "messages = [\n",
                "    {\"role\": \"user\", \"content\": \"Write a Python function to find the nth Fibonacci number using memoization.\"}\n",
                "]\n",
                "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
                "\n",
                "# Generate code\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
                "outputs = model.generate(inputs.input_ids, max_length=1024)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                "```\n",
                "\n",
                "## References\n",
                "\n",
                "- DeepSeek AI. (2023). [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954). arXiv.\n",
                "- DeepSeek AI. (2023). [DeepSeek Coder: When the Large Language Model Meets Programming](https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/README.md).\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}