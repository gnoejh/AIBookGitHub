{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/lenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LeNet-5: A Pioneering Convolutional Neural Network Architecture\n",
                "\n",
                "## Introduction\n",
                "\n",
                "LeNet-5 is one of the earliest convolutional neural networks (CNNs), introduced by Yann LeCun and his collaborators in 1998. It was originally designed for handwritten and machine-printed character recognition. Despite its age, the architecture remains influential and forms the foundation for many modern CNN architectures.\n",
                "\n",
                "## Historical Context\n",
                "\n",
                "- Developed at Bell Labs by Yann LeCun et al. in the late 1990s\n",
                "- Published in the paper \"Gradient-Based Learning Applied to Document Recognition\" in 1998\n",
                "- Successfully applied to digit recognition tasks like reading zip codes, checks, etc.\n",
                "- One of the first applications of convolutional neural networks to practical problems"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architecture Overview\n",
                "\n",
                "LeNet-5 is a 7-layer convolutional neural network that takes a 32\u00d732 grayscale image as input and outputs classification probabilities. The architecture consists of:\n",
                "\n",
                "1. **Input Layer**: 32\u00d732 grayscale image\n",
                "2. **C1**: Convolutional layer with 6 feature maps of size 28\u00d728\n",
                "3. **S2**: Subsampling (average pooling) layer with 6 feature maps of size 14\u00d714\n",
                "4. **C3**: Convolutional layer with 16 feature maps of size 10\u00d710\n",
                "5. **S4**: Subsampling (average pooling) layer with 16 feature maps of size 5\u00d75\n",
                "6. **C5**: Convolutional layer with 120 feature maps of size 1\u00d71\n",
                "7. **F6**: Fully connected layer with 84 units\n",
                "8. **Output Layer**: Fully connected layer with 10 units (for digit classification)\n",
                "\n",
                "![LeNet-5 Architecture](https://www.researchgate.net/profile/Vladimir-Golovko-3/publication/313808170/figure/fig3/AS:714087491317770@1547277416263/Architecture-of-LeNet-5_W640.jpg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Check if CUDA is available and set device accordingly\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing LeNet-5 in PyTorch\n",
                "\n",
                "Below is a PyTorch implementation of the LeNet-5 architecture. The implementation follows the original architecture with slight modifications to accommodate modern deep learning practices:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LeNet5(nn.Module):\n",
                "    def __init__(self, num_classes=10):\n",
                "        super(LeNet5, self).__init__()\n",
                "        \n",
                "        # Layer C1: Convolutional layer (6 feature maps of size 28x28)\n",
                "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
                "        # Layer S2: Average pooling layer (6 feature maps of size 14x14)\n",
                "        self.avg_pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
                "        \n",
                "        # Layer C3: Convolutional layer (16 feature maps of size 10x10)\n",
                "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
                "        # Layer S4: Average pooling layer (16 feature maps of size 5x5)\n",
                "        self.avg_pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
                "        \n",
                "        # Layer C5: Convolutional layer (120 feature maps of size 1x1)\n",
                "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)\n",
                "        \n",
                "        # Layer F6: Fully connected layer with 84 units\n",
                "        self.fc1 = nn.Linear(120, 84)\n",
                "        \n",
                "        # Output layer\n",
                "        self.fc2 = nn.Linear(84, num_classes)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # C1 -> S2\n",
                "        x = self.avg_pool1(F.relu(self.conv1(x)))\n",
                "        \n",
                "        # C3 -> S4\n",
                "        x = self.avg_pool2(F.relu(self.conv2(x)))\n",
                "        \n",
                "        # C5\n",
                "        x = F.relu(self.conv3(x))\n",
                "        # Flatten the feature maps\n",
                "        x = x.view(-1, 120)\n",
                "        \n",
                "        # F6\n",
                "        x = F.relu(self.fc1(x))\n",
                "        \n",
                "        # Output layer\n",
                "        x = self.fc2(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Preparation\n",
                "\n",
                "Let's load and prepare the MNIST dataset for training our LeNet-5 model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform operations for the dataset\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((32, 32)),  # LeNet requires 32x32 images\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
                "])\n",
                "\n",
                "# Download and load the MNIST training dataset\n",
                "train_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
                "                                           download=True, transform=transform)\n",
                "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
                "                                          shuffle=True, num_workers=2)\n",
                "\n",
                "# Download and load the MNIST test dataset\n",
                "test_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
                "                                         download=True, transform=transform)\n",
                "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000,\n",
                "                                         shuffle=False, num_workers=2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing Some Samples\n",
                "\n",
                "Let's visualize some samples from the MNIST dataset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get some random training images\n",
                "dataiter = iter(train_loader)\n",
                "images, labels = next(dataiter)\n",
                "\n",
                "# Create a grid of images for visualization\n",
                "img_grid = torchvision.utils.make_grid(images[:25], nrow=5)\n",
                "npimg = img_grid.numpy()\n",
                "\n",
                "# Plot the images\n",
                "plt.figure(figsize=(10, 10))\n",
                "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
                "plt.title('MNIST Training Images')\n",
                "plt.axis('off')\n",
                "plt.show()\n",
                "\n",
                "# Print the labels\n",
                "print('Labels:', ' '.join(f'{labels[i]}' for i in range(25)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training the LeNet-5 Model\n",
                "\n",
                "Now, let's train our LeNet-5 model on the MNIST dataset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instantiate the model and move it to the device\n",
                "model = LeNet5(num_classes=10).to(device)\n",
                "\n",
                "# Define loss function and optimizer\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
                "\n",
                "# Number of epochs\n",
                "num_epochs = 5\n",
                "\n",
                "# Lists to store metrics\n",
                "train_losses = []\n",
                "train_accuracies = []\n",
                "\n",
                "# Training loop\n",
                "for epoch in range(num_epochs):\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for i, data in enumerate(train_loader, 0):\n",
                "        # Get the inputs; data is a list of [inputs, labels]\n",
                "        inputs, labels = data[0].to(device), data[1].to(device)\n",
                "        \n",
                "        # Zero the parameter gradients\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward pass\n",
                "        outputs = model(inputs)\n",
                "        \n",
                "        # Calculate loss\n",
                "        loss = criterion(outputs, labels)\n",
                "        \n",
                "        # Backward pass and optimize\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        # Statistics\n",
                "        running_loss += loss.item()\n",
                "        _, predicted = torch.max(outputs.data, 1)\n",
                "        total += labels.size(0)\n",
                "        correct += (predicted == labels).sum().item()\n",
                "        \n",
                "        # Print statistics every 100 mini-batches\n",
                "        if i % 100 == 99:\n",
                "            print(f'[Epoch {epoch + 1}, Batch {i + 1:5d}] Loss: {running_loss / 100:.3f}, Accuracy: {100 * correct / total:.2f}%')\n",
                "            running_loss = 0.0\n",
                "    \n",
                "    # Calculate epoch statistics\n",
                "    epoch_loss = running_loss / len(train_loader)\n",
                "    epoch_accuracy = 100 * correct / total\n",
                "    train_losses.append(epoch_loss)\n",
                "    train_accuracies.append(epoch_accuracy)\n",
                "    \n",
                "    print(f'Epoch {epoch + 1} completed. Loss: {epoch_loss:.3f}, Accuracy: {epoch_accuracy:.2f}%')\n",
                "\n",
                "print('Finished Training')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating the Model\n",
                "\n",
                "Let's evaluate our trained LeNet-5 model on the test dataset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the model to evaluation mode\n",
                "model.eval()\n",
                "\n",
                "# Test the model\n",
                "correct = 0\n",
                "total = 0\n",
                "class_correct = list(0. for i in range(10))\n",
                "class_total = list(0. for i in range(10))\n",
                "\n",
                "with torch.no_grad():\n",
                "    for data in test_loader:\n",
                "        images, labels = data[0].to(device), data[1].to(device)\n",
                "        outputs = model(images)\n",
                "        _, predicted = torch.max(outputs.data, 1)\n",
                "        total += labels.size(0)\n",
                "        correct += (predicted == labels).sum().item()\n",
                "        \n",
                "        # Calculate accuracy for each class\n",
                "        c = (predicted == labels).squeeze()\n",
                "        for i in range(len(labels)):\n",
                "            label = labels[i]\n",
                "            class_correct[label] += c[i].item()\n",
                "            class_total[label] += 1\n",
                "\n",
                "print(f'Accuracy of the LeNet-5 model on the 10000 test images: {100 * correct / total:.2f}%')\n",
                "\n",
                "# Print accuracy for each class\n",
                "for i in range(10):\n",
                "    print(f'Accuracy of {i}: {100 * class_correct[i] / class_total[i]:.2f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing Predictions\n",
                "\n",
                "Let's visualize some of the model's predictions on the test dataset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get a batch of test images\n",
                "dataiter = iter(test_loader)\n",
                "images, labels = next(dataiter)\n",
                "images = images[:25].to(device)\n",
                "labels = labels[:25]\n",
                "\n",
                "# Get model predictions\n",
                "with torch.no_grad():\n",
                "    outputs = model(images)\n",
                "    _, predicted = torch.max(outputs, 1)\n",
                "\n",
                "# Plot the images with their true and predicted labels\n",
                "fig, axes = plt.subplots(5, 5, figsize=(12, 12))\n",
                "fig.subplots_adjust(hspace=0.5)\n",
                "\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    if i < len(images):\n",
                "        ax.imshow(images[i].cpu().numpy().squeeze(), cmap='gray')\n",
                "        ax.set_title(f'True: {labels[i].item()}, Pred: {predicted[i].item()}')\n",
                "        ax.axis('off')\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "LeNet-5 was a groundbreaking architecture in the field of deep learning, particularly for computer vision tasks. Despite its simplicity compared to modern architectures, it introduced several key concepts that are still used today:\n",
                "\n",
                "1. **Local receptive fields**: Each neuron in a convolutional layer processes data only from a small region of the input volume.\n",
                "2. **Shared weights**: The same set of weights is used across the entire input volume, reducing the number of parameters and making the network more efficient.\n",
                "3. **Subsampling**: Pooling layers reduce the spatial dimensions of the data, making the network more robust to variations in the input.\n",
                "\n",
                "As we've seen, even with its relatively simple architecture, LeNet-5 can achieve high accuracy on the MNIST dataset. Modern CNN architectures have built upon these foundations to achieve even better performance on more complex tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "1. LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.\n",
                "2. LeCun, Y., Cortes, C., & Burges, C. J. (2010). MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/\n",
                "3. PyTorch Documentation. https://pytorch.org/docs/stable/index.html"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}