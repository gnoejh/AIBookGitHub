{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/inception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Inception/GoogLeNet Architecture\n",
                "\n",
                "## Introduction\n",
                "\n",
                "Inception, also known as GoogLeNet (named after LeNet but designed by Google), was introduced in 2014 by Szegedy et al. in their paper \"Going Deeper with Convolutions.\" It was the winner of the ImageNet Large Scale Visual Recognition Challenge 2014 (ILSVRC2014) and marked a significant milestone in the development of convolutional neural networks (CNNs) for computer vision tasks.\n",
                "\n",
                "The Inception architecture was revolutionary for two main reasons:\n",
                "1. It introduced the concept of the \"Inception Module\" with parallel convolutional pathways\n",
                "2. It achieved state-of-the-art performance while being computationally more efficient than predecessors like AlexNet and VGG"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The Inception Module\n",
                "\n",
                "The core innovation of the Inception architecture is the Inception module. Instead of having to choose between different filter sizes (1\u00d71, 3\u00d73, 5\u00d75) for convolutions at each layer, the Inception module performs all of these convolutions in parallel and concatenates the outputs.\n",
                "\n",
                "### Key components of an Inception module:\n",
                "\n",
                "1. **1\u00d71 Convolutions**: These act as dimensionality reduction modules and help reduce the computational cost.\n",
                "2. **Parallel Pathways**: Several convolution operations (1\u00d71, 3\u00d73, 5\u00d75) and a max pooling operation run in parallel.\n",
                "3. **Concatenation**: The outputs from all pathways are concatenated along the channel dimension.\n",
                "\n",
                "![Inception Module](https://miro.medium.com/max/1400/1*ZFPOSAted10TPd3hBQU8iQ.png)\n",
                "\n",
                "### Computational Efficiency\n",
                "\n",
                "A key insight was using 1\u00d71 convolutions before the expensive 3\u00d73 and 5\u00d75 convolutions to reduce the input channel dimensions, making the module computationally efficient. For example:\n",
                "\n",
                "- Input with 256 channels \u2192 1\u00d71 convolution to reduce to 64 channels \u2192 3\u00d73 convolution\n",
                "- This significantly reduces the number of parameters and computations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## GoogLeNet Architecture\n",
                "\n",
                "GoogLeNet, the first implementation of the Inception architecture, is a 22-layer deep network (27 if counting the pooling layers). Key features include:\n",
                "\n",
                "1. **Network in Network Philosophy**: Embedding micro-networks (Inception modules) within the larger network\n",
                "2. **No Fully Connected Layers**: Unlike earlier CNN architectures, GoogLeNet uses global average pooling at the end instead of fully-connected layers, significantly reducing parameters\n",
                "3. **Auxiliary Classifiers**: During training, additional softmax classifiers connected to intermediate layers to combat vanishing gradient problems\n",
                "4. **Nine Inception Modules**: Stacked with occasional max-pooling layers in between to reduce spatial dimensions\n",
                "\n",
                "### Network Architecture Overview\n",
                "\n",
                "```\n",
                "Input Image (224\u00d7224\u00d73)\n",
                "\u2502\n",
                "\u251c\u2500\u2500 Conv 7\u00d77+2(S) (112\u00d7112\u00d764)\n",
                "\u2502   \u2514\u2500\u2500 MaxPool 3\u00d73+2(S) (56\u00d756\u00d764)\n",
                "\u2502       \u2514\u2500\u2500 Conv 1\u00d71+1(V) (56\u00d756\u00d764)\n",
                "\u2502           \u2514\u2500\u2500 Conv 3\u00d73+1(S) (56\u00d756\u00d7192)\n",
                "\u2502               \u2514\u2500\u2500 MaxPool 3\u00d73+2(S) (28\u00d728\u00d7192)\n",
                "\u2502                   \u2514\u2500\u2500 Inception(3a) (28\u00d728\u00d7256)\n",
                "\u2502                       \u2514\u2500\u2500 Inception(3b) (28\u00d728\u00d7480)\n",
                "\u2502                           \u2514\u2500\u2500 MaxPool 3\u00d73+2(S) (14\u00d714\u00d7480)\n",
                "\u2502                               \u2514\u2500\u2500 Inception(4a) (14\u00d714\u00d7512)\n",
                "\u2502                                   \u2514\u2500\u2500 Inception(4b) (14\u00d714\u00d7512)\n",
                "\u2502                                       \u2514\u2500\u2500 Inception(4c) (14\u00d714\u00d7512)\n",
                "\u2502                                           \u2514\u2500\u2500 Inception(4d) (14\u00d714\u00d7528)\n",
                "\u2502                                               \u2514\u2500\u2500 Inception(4e) (14\u00d714\u00d7832)\n",
                "\u2502                                                   \u2514\u2500\u2500 MaxPool 3\u00d73+2(S) (7\u00d77\u00d7832)\n",
                "\u2502                                                       \u2514\u2500\u2500 Inception(5a) (7\u00d77\u00d7832)\n",
                "\u2502                                                           \u2514\u2500\u2500 Inception(5b) (7\u00d77\u00d71024)\n",
                "\u2502                                                               \u2514\u2500\u2500 AvgPool 7\u00d77+1(V) (1\u00d71\u00d71024)\n",
                "\u2502                                                                   \u2514\u2500\u2500 Dropout (40%)\n",
                "\u2502                                                                       \u2514\u2500\u2500 Linear (1\u00d71\u00d71000)\n",
                "\u2502                                                                           \u2514\u2500\u2500 Softmax output\n",
                "```\n",
                "\n",
                "The notation \"Conv 7\u00d77+2(S)\" indicates a 7\u00d77 convolutional filter with stride 2 and same padding."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evolution: Inception v2, v3, v4\n",
                "\n",
                "The Inception architecture continued to evolve after the original GoogLeNet (Inception v1):\n",
                "\n",
                "### Inception v2 and v3\n",
                "- Factorized convolutions: Replaced larger convolutions with multiple smaller ones (e.g., 5\u00d75 \u2192 two 3\u00d73)\n",
                "- Asymmetric convolutions: Factorized n\u00d7n convolutions into 1\u00d7n followed by n\u00d71\n",
                "- Expanded filter banks\n",
                "- Auxiliary classifiers as regularizers rather than for combating vanishing gradients\n",
                "- Label smoothing: A regularization technique for the softmax classifier\n",
                "\n",
                "### Inception v4 and Inception-ResNet\n",
                "- Combined Inception modules with residual connections from ResNet\n",
                "- More uniform simplified architecture\n",
                "- Inception-ResNet showed faster training with similar accuracy to Inception v4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Implementing a basic Inception module in PyTorch\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class InceptionModule(nn.Module):\n",
                "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
                "        super(InceptionModule, self).__init__()\n",
                "        \n",
                "        # 1x1 conv branch\n",
                "        self.branch1 = nn.Conv2d(in_channels, ch1x1, kernel_size=1)\n",
                "        \n",
                "        # 1x1 -> 3x3 conv branch\n",
                "        self.branch2 = nn.Sequential(\n",
                "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
                "        )\n",
                "        \n",
                "        # 1x1 -> 5x5 conv branch\n",
                "        self.branch3 = nn.Sequential(\n",
                "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
                "        )\n",
                "        \n",
                "        # 3x3 pool -> 1x1 conv branch\n",
                "        self.branch4 = nn.Sequential(\n",
                "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
                "            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        branch1 = F.relu(self.branch1(x))\n",
                "        branch2 = F.relu(self.branch2(x))\n",
                "        branch3 = F.relu(self.branch3(x))\n",
                "        branch4 = F.relu(self.branch4(x))\n",
                "        \n",
                "        # Concatenate along channel dimension\n",
                "        outputs = [branch1, branch2, branch3, branch4]\n",
                "        return torch.cat(outputs, 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([1, 192, 28, 28])\n",
                        "Output shape: torch.Size([1, 256, 28, 28])\n"
                    ]
                }
            ],
            "source": [
                "# Testing the Inception module with GoogLeNet's Inception (3a) parameters\n",
                "batch_size = 1\n",
                "in_channels = 192\n",
                "height, width = 28, 28\n",
                "\n",
                "# Parameters for Inception(3a)\n",
                "ch1x1 = 64\n",
                "ch3x3red = 96\n",
                "ch3x3 = 128\n",
                "ch5x5red = 16\n",
                "ch5x5 = 32\n",
                "pool_proj = 32\n",
                "\n",
                "# Create a sample input tensor\n",
                "x = torch.randn(batch_size, in_channels, height, width)\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "\n",
                "# Create and apply the Inception module\n",
                "inception_3a = InceptionModule(in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj)\n",
                "output = inception_3a(x)\n",
                "print(f\"Output shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: InceptionV3\n",
                        "Total Parameters: 23,851,784\n",
                        "Imagenet Top-5 Accuracy: 94.4%\n"
                    ]
                }
            ],
            "source": [
                "# Using pre-trained Inception models with TensorFlow/Keras\n",
                "\n",
                "# Note: This is just for demonstration, executing this requires TensorFlow to be installed\n",
                "# Import the Inception models\n",
                "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
                "# model = InceptionV3(weights='imagenet', include_top=True)\n",
                "\n",
                "# Instead of real execution, just show information about the model\n",
                "print(\"Model: InceptionV3\")\n",
                "print(\"Total Parameters: 23,851,784\")\n",
                "print(\"Imagenet Top-5 Accuracy: 94.4%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications and Impact\n",
                "\n",
                "### Applications\n",
                "Inception architectures have been successfully applied to a variety of computer vision tasks:\n",
                "\n",
                "1. **Image Classification**: The original purpose for which it was designed\n",
                "2. **Object Detection**: As a backbone in frameworks like SSD (Single Shot MultiBox Detector)\n",
                "3. **Image Segmentation**: In DeepLab and other segmentation networks\n",
                "4. **Transfer Learning**: Pre-trained Inception models are widely used for transferring to domain-specific tasks\n",
                "5. **Computational Photography**: Used in image enhancement and computational imaging\n",
                "\n",
                "### Historical Impact\n",
                "\n",
                "The Inception architecture introduced several innovations that have widely influenced deep learning:\n",
                "\n",
                "1. **Split-Transform-Merge Strategy**: Breaking down complex tasks into simpler, parallel operations\n",
                "2. **Dimensionality Reduction**: Strategic use of 1\u00d71 convolutions for computational efficiency\n",
                "3. **Network Design Philosophy**: The idea that carefully crafted network modules can optimize both accuracy and computation\n",
                "4. **Successor Influence**: Concepts from Inception influenced later architectures like ResNeXt and the design of efficient CNN models\n",
                "\n",
                "### Limitations\n",
                "\n",
                "Despite its success, Inception has some limitations:\n",
                "\n",
                "1. **Complex Architecture**: Hard to modify, adapt, or interpret compared to more uniform designs\n",
                "2. **Manual Design**: The module structures required significant hand-engineering\n",
                "3. **Superseded Performance**: Later architectures like ResNets and EfficientNets have surpassed Inception in some benchmarks\n",
                "4. **Higher Memory Usage**: Due to the parallel pathways, Inception can require more memory during training than sequential designs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "1. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., & Rabinovich, A. (2015). [Going deeper with convolutions](https://arxiv.org/abs/1409.4842). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n",
                "\n",
                "2. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n",
                "\n",
                "3. Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261). In AAAI Conference on Artificial Intelligence.\n",
                "\n",
                "4. Chollet, F. (2017). [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (A related architecture inspired by Inception)\n",
                "\n",
                "5. Christian Szegedy's Google Scholar profile: [https://scholar.google.com/citations?user=3QeF7mAAAAAJ](https://scholar.google.com/citations?user=3QeF7mAAAAAJ)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}