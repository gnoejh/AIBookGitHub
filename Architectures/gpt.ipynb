{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generative Pre-trained Transformer (GPT)\n",
                "\n",
                "The GPT architecture represents a family of large-scale language models based on the Transformer architecture that have revolutionized natural language processing. Developed by OpenAI, GPT models are trained on vast amounts of text data and have demonstrated remarkable capabilities in text generation, understanding, and various language tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architectural Overview\n",
                "\n",
                "GPT is built on the decoder portion of the Transformer architecture with some key characteristics:\n",
                "\n",
                "### Key Components\n",
                "\n",
                "1. **Transformer Decoder Blocks**: Multiple layers of transformer decoder blocks stacked together\n",
                "2. **Self-Attention Mechanism**: Masked self-attention that prevents the model from attending to future tokens\n",
                "3. **Feed-Forward Networks**: Position-wise fully connected layers for additional processing\n",
                "4. **Layer Normalization**: Applied before each sub-block for training stability\n",
                "5. **Positional Encodings**: To capture token position information within sequences\n",
                "\n",
                "### Architecture Diagram\n",
                "\n",
                "```\n",
                "Input Sequence \u2192 Embedding + Positional Encoding \u2192\n",
                "                                                   \u2193\n",
                "                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "                                      \u2502 Masked Self-Attention\u2502\n",
                "                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                                                   \u2193\n",
                "                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \n",
                "                                      \u2502   Feed-Forward NN   \u2502 \n",
                "                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                                                   \u2193\n",
                "                                             (Repeat N times)\n",
                "                                                   \u2193\n",
                "                                              Output Layer\n",
                "```\n",
                "\n",
                "Unlike BERT which looks at both left and right context (bidirectional), GPT uses a unidirectional (left-to-right) attention mechanism, meaning each token can only attend to previous tokens and itself."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Methodology\n",
                "\n",
                "GPT models follow a two-phase training approach:\n",
                "\n",
                "### 1. Pre-training (Generative)\n",
                "\n",
                "The model is trained on a massive corpus of text using unsupervised learning to predict the next token in a sequence given all the previous tokens.\n",
                "\n",
                "**Objective**: Maximize the likelihood of the next word given the previous context:\n",
                "\n",
                "$$L_1(U) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
                "\n",
                "Where:\n",
                "- $U$ is the unlabeled text corpus\n",
                "- $u_i$ is the current token\n",
                "- $k$ is the context window size\n",
                "- $\\Theta$ represents the model parameters\n",
                "\n",
                "### 2. Fine-tuning\n",
                "\n",
                "The pre-trained model is then fine-tuned on specific downstream tasks using supervised learning with labeled data.\n",
                "\n",
                "**Objective**: Maximize the likelihood of the correct output given the input:\n",
                "\n",
                "$$L_2(C) = \\sum_{(x,y) \\in C} \\log P(y | x; \\Theta)$$\n",
                "\n",
                "Where:\n",
                "- $C$ is the labeled dataset for the specific task\n",
                "- $x$ is the input sequence\n",
                "- $y$ is the target output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evolution of GPT Models\n",
                "\n",
                "The GPT architecture has evolved significantly across different versions:\n",
                "\n",
                "### GPT-1 (2018)\n",
                "- 117 million parameters\n",
                "- 12 transformer layers\n",
                "- First to demonstrate effective transfer learning in NLP\n",
                "- Trained on BookCorpus (about 4.5GB of text)\n",
                "\n",
                "### GPT-2 (2019)\n",
                "- Up to 1.5 billion parameters\n",
                "- 48 transformer layers in largest model\n",
                "- Improved text generation capabilities\n",
                "- Trained on WebText (40GB of text)\n",
                "- Famous for its ability to generate coherent, lengthy passages\n",
                "\n",
                "### GPT-3 (2020)\n",
                "- 175 billion parameters\n",
                "- 96 attention layers\n",
                "- Demonstrated few-shot and zero-shot learning capabilities\n",
                "- Trained on Common Crawl, WebText2, Books1, Books2, Wikipedia (about 570GB)\n",
                "- Capable of performing tasks without explicit fine-tuning\n",
                "\n",
                "### GPT-4 (2023)\n",
                "- Estimated 1 trillion+ parameters (exact size undisclosed)\n",
                "- Multimodal capabilities (can process both text and images)\n",
                "- Significantly improved reasoning and safety features\n",
                "- Reduced hallucinations and better factual accuracy\n",
                "\n",
                "Each iteration has demonstrated substantial improvements in performance, capabilities, and breadth of applications."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications of GPT\n",
                "\n",
                "GPT models excel in a wide range of natural language processing tasks:\n",
                "\n",
                "### Text Generation\n",
                "- Creative writing (stories, poetry, scripts)\n",
                "- Content creation for articles and marketing\n",
                "- Code generation and completion\n",
                "\n",
                "### Language Understanding\n",
                "- Question answering\n",
                "- Summarization\n",
                "- Translation\n",
                "- Sentiment analysis\n",
                "\n",
                "### Conversational AI\n",
                "- Chatbots and virtual assistants\n",
                "- Customer support automation\n",
                "- Interactive storytelling\n",
                "\n",
                "### Specialized Applications\n",
                "- Legal document analysis\n",
                "- Medical text processing\n",
                "- Educational content generation\n",
                "- Scientific research assistance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation Example\n",
                "\n",
                "Here's how to use GPT models from the Hugging Face library:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install transformers torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
                "\n",
                "# Load pre-trained model and tokenizer\n",
                "model_name = \"gpt2\"  # Options: gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
                "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
                "\n",
                "# Generate text\n",
                "def generate_text(prompt, max_length=100):\n",
                "    # Encode the prompt\n",
                "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
                "    \n",
                "    # Generate text\n",
                "    output = model.generate(\n",
                "        input_ids,\n",
                "        max_length=max_length,\n",
                "        num_return_sequences=1,\n",
                "        no_repeat_ngram_size=2,\n",
                "        top_k=50,\n",
                "        top_p=0.95,\n",
                "        temperature=0.8,\n",
                "        do_sample=True\n",
                "    )\n",
                "    \n",
                "    # Decode and return the generated text\n",
                "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
                "    return generated_text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example usage\n",
                "prompt = \"Artificial intelligence will\"\n",
                "generated_text = generate_text(prompt)\n",
                "print(generated_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fine-tuning GPT for Specific Tasks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Example: Fine-tuning GPT-2 on a custom dataset\n",
                "def finetune_gpt(dataset_path, output_dir=\"./finetuned-gpt\"):\n",
                "    # Load dataset\n",
                "    dataset = load_dataset(dataset_path)\n",
                "    \n",
                "    # Tokenize dataset\n",
                "    def tokenize_function(examples):\n",
                "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
                "    \n",
                "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
                "    \n",
                "    # Set up training arguments\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir=output_dir,\n",
                "        overwrite_output_dir=True,\n",
                "        num_train_epochs=3,\n",
                "        per_device_train_batch_size=8,\n",
                "        per_device_eval_batch_size=8,\n",
                "        save_steps=10_000,\n",
                "        save_total_limit=2,\n",
                "    )\n",
                "    \n",
                "    # Data collator for language modeling\n",
                "    data_collator = DataCollatorForLanguageModeling(\n",
                "        tokenizer=tokenizer, mlm=False\n",
                "    )\n",
                "    \n",
                "    # Initialize trainer\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=tokenized_dataset[\"train\"],\n",
                "        eval_dataset=tokenized_dataset[\"validation\"] if \"validation\" in tokenized_dataset else None,\n",
                "        data_collator=data_collator,\n",
                "    )\n",
                "    \n",
                "    # Train the model\n",
                "    trainer.train()\n",
                "    \n",
                "    # Save the model\n",
                "    trainer.save_model()\n",
                "    \n",
                "    return trainer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advantages and Limitations\n",
                "\n",
                "### Advantages\n",
                "- Powerful zero-shot and few-shot learning capabilities\n",
                "- Excellent at generating coherent, contextually relevant text\n",
                "- Versatile across multiple language tasks without specific architecture modifications\n",
                "- Can understand nuanced prompts and follow complex instructions\n",
                "\n",
                "### Limitations\n",
                "- Unidirectional attention limits certain contextual understanding\n",
                "- Can produce plausible-sounding but factually incorrect information (\"hallucinations\")\n",
                "- Computationally expensive to train and run (especially larger models)\n",
                "- Lacks explicit knowledge representation or reasoning capabilities\n",
                "- Training data cutoff creates temporal limitations\n",
                "- Ethical concerns around bias, misuse for misinformation, etc."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "1. Radford, A., et al. (2018). [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf). OpenAI.\n",
                "\n",
                "2. Radford, A., et al. (2019). [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). OpenAI.\n",
                "\n",
                "3. Brown, T. B., et al. (2020). [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165). arXiv preprint arXiv:2005.14165.\n",
                "\n",
                "4. OpenAI (2023). [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774). arXiv preprint arXiv:2303.08774.\n",
                "\n",
                "5. Vaswani, A., et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). NeurIPS.\n",
                "\n",
                "6. Wolf, T., et al. (2020). [Transformers: State-of-the-Art Natural Language Processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6/). EMNLP."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}