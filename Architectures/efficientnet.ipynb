{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/efficient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EfficientNet\n",
                "\n",
                "## Introduction\n",
                "\n",
                "EfficientNet is a family of convolutional neural networks introduced by Tan and Le from Google Research in 2019. The architecture set new state-of-the-art accuracy on ImageNet classification while being significantly more efficient in terms of parameters and computational cost than previous models."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Innovation: Compound Scaling\n",
                "\n",
                "EfficientNet's primary innovation is **compound scaling**, a principled method for scaling neural networks across three dimensions:\n",
                "\n",
                "1. **Depth** - number of layers\n",
                "2. **Width** - number of channels in each layer\n",
                "3. **Resolution** - input image size\n",
                "\n",
                "Previous approaches typically scaled only one dimension (e.g., making networks deeper), which leads to diminishing returns. EfficientNet scales all three dimensions simultaneously using a compound coefficient:\n",
                "\n",
                "* Depth: $d = \\alpha^\\phi$\n",
                "* Width: $w = \\beta^\\phi$\n",
                "* Resolution: $r = \\gamma^\\phi$\n",
                "\n",
                "Where $\\alpha$, $\\beta$, and $\\gamma$ are constants determined through a grid search, and $\\phi$ is the compound coefficient that controls the available resources.\n",
                "\n",
                "![EfficientNet Compound Scaling](https://miro.medium.com/max/1400/1*CnNorCR4Zdq7pVchdsRGxA.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## EfficientNet Baseline Architecture\n",
                "\n",
                "EfficientNet-B0 serves as the baseline architecture, designed through neural architecture search (NAS) optimizing for both accuracy and efficiency. It consists primarily of **mobile inverted bottleneck convolution (MBConv)** blocks, first introduced in MobileNetV2.\n",
                "\n",
                "### Architecture of EfficientNet-B0\n",
                "\n",
                "| Stage | Operator | Resolution | Channels | Layers |\n",
                "|-------|----------|------------|----------|--------|\n",
                "| 1 | Conv3x3 | 224\u00d7224 | 32 | 1 |\n",
                "| 2 | MBConv1, k3x3 | 112\u00d7112 | 16 | 1 |\n",
                "| 3 | MBConv6, k3x3 | 112\u00d7112 | 24 | 2 |\n",
                "| 4 | MBConv6, k5x5 | 56\u00d756 | 40 | 2 |\n",
                "| 5 | MBConv6, k3x3 | 28\u00d728 | 80 | 3 |\n",
                "| 6 | MBConv6, k5x5 | 14\u00d714 | 112 | 3 |\n",
                "| 7 | MBConv6, k5x5 | 14\u00d714 | 192 | 4 |\n",
                "| 8 | MBConv6, k3x3 | 7\u00d77 | 320 | 1 |\n",
                "| 9 | Conv1x1 & Pooling & FC | 7\u00d77 | 1280 | 1 |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## EfficientNet Variants\n",
                "\n",
                "EfficientNet has multiple variants, scaled from the baseline EfficientNet-B0 using the compound scaling approach:\n",
                "\n",
                "| Model | Resolution | Parameters | FLOPS | Top-1 Accuracy (%) |\n",
                "|-------|------------|------------|-------|--------------------|\n",
                "| EfficientNet-B0 | 224\u00d7224 | 5.3M | 0.39B | 77.1 |\n",
                "| EfficientNet-B1 | 240\u00d7240 | 7.8M | 0.70B | 79.1 |\n",
                "| EfficientNet-B2 | 260\u00d7260 | 9.2M | 1.0B | 80.1 |\n",
                "| EfficientNet-B3 | 300\u00d7300 | 12M | 1.8B | 81.6 |\n",
                "| EfficientNet-B4 | 380\u00d7380 | 19M | 4.2B | 82.9 |\n",
                "| EfficientNet-B5 | 456\u00d7456 | 30M | 9.9B | 83.6 |\n",
                "| EfficientNet-B6 | 528\u00d7528 | 43M | 19B | 84.0 |\n",
                "| EfficientNet-B7 | 600\u00d7600 | 66M | 37B | 84.3 |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MBConv Block\n",
                "\n",
                "The core building block of EfficientNet is the **Mobile Inverted Bottleneck Convolution (MBConv)** with squeeze-and-excitation optimization:\n",
                "\n",
                "1. **Expansion layer**: 1x1 convolution that increases the number of channels\n",
                "2. **Depthwise convolution**: Spatial convolution applied to each channel separately\n",
                "3. **Squeeze-and-excitation block**: Adaptively recalibrates channel-wise features\n",
                "4. **Projection layer**: 1x1 convolution that reduces channels back to output size\n",
                "\n",
                "MBConv also implements **residual connections** similar to ResNet when input and output dimensions match.\n",
                "\n",
                "![MBConv Block](https://miro.medium.com/max/1400/1*ExS7GpW-C5zBJENw9-mZrQ.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation Example\n",
                "\n",
                "### Using TensorFlow/Keras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow.keras.applications import EfficientNetB0\n",
                "\n",
                "# Load pre-trained EfficientNet model\n",
                "model = EfficientNetB0(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
                "\n",
                "# Summary of the model architecture\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inference example\n",
                "import numpy as np\n",
                "from tensorflow.keras.preprocessing import image\n",
                "from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n",
                "\n",
                "# Load and preprocess image\n",
                "img_path = 'path_to_image.jpg'\n",
                "img = image.load_img(img_path, target_size=(224, 224))\n",
                "x = image.img_to_array(img)\n",
                "x = np.expand_dims(x, axis=0)\n",
                "x = preprocess_input(x)\n",
                "\n",
                "# Make prediction\n",
                "preds = model.predict(x)\n",
                "print('Predicted:', decode_predictions(preds, top=5)[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using PyTorch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torchvision.models as models\n",
                "\n",
                "# Load pre-trained EfficientNet model\n",
                "model = models.efficientnet_b0(pretrained=True)\n",
                "model.eval()\n",
                "\n",
                "# Print model architecture\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## EfficientNetV2 and Beyond\n",
                "\n",
                "In 2021, the authors released **EfficientNetV2**, which improves training speed and efficiency while maintaining accuracy. Key improvements include:\n",
                "\n",
                "1. **Fused-MBConv blocks** that replace some MBConv blocks\n",
                "2. **Progressive learning** with gradually increasing image sizes during training\n",
                "3. **Improved training techniques** like smaller expansion ratios in early layers\n",
                "4. **Reduced activation memory** usage through network architecture refinements\n",
                "\n",
                "EfficientNetV2 achieves better performance with 6.8\u00d7 faster training speed than the original EfficientNet."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications\n",
                "\n",
                "EfficientNet has found widespread use across numerous applications:\n",
                "\n",
                "1. **Mobile vision tasks** - Due to its efficiency on resource-constrained devices\n",
                "2. **Medical image analysis** - Adapting to various medical imaging modalities\n",
                "3. **Transfer learning tasks** - As a feature extractor for downstream tasks\n",
                "4. **Object detection backbones** - In frameworks like EfficientDet\n",
                "5. **Edge computing** - For deploying computer vision on IoT devices\n",
                "\n",
                "The efficiency-accuracy trade-off makes EfficientNet particularly valuable for real-world deployment scenarios where computational resources are limited."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison with Other Architectures\n",
                "\n",
                "![Comparison Chart](https://miro.medium.com/max/1400/1*0J4QEJtXO_-lG-HQRUBQHw.png)\n",
                "\n",
                "* **vs. ResNet**: Much higher accuracy with far fewer parameters\n",
                "* **vs. MobileNet**: Higher accuracy while maintaining computational efficiency\n",
                "* **vs. SENet**: Better performance with refined use of squeeze-and-excitation blocks\n",
                "* **vs. NASNet**: Similar performance but with more principled scaling approach"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "1. EfficientNet revolutionized network scaling with its compound scaling method\n",
                "2. Scale depth, width, and resolution in a balanced, principled way\n",
                "3. Utilize MBConv blocks with squeeze-and-excitation for efficiency\n",
                "4. Consider the appropriate EfficientNet variant based on your resource constraints\n",
                "5. Adapt scaling coefficients to your specific problem domain when fine-tuning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "1. Tan, M., & Le, Q. (2019). [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946). ICML.\n",
                "2. Tan, M., & Le, Q. (2021). [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298). ICML.\n",
                "3. Sandler, M., et al. (2018). [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381). CVPR.\n",
                "4. Hu, J., et al. (2018). [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507). CVPR.\n",
                "5. Tan, M., et al. (2020). [EfficientDet: Scalable and Efficient Object Detection](https://arxiv.org/abs/1911.09070). CVPR."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}