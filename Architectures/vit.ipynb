{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Vision Transformer (ViT)\n",
                "\n",
                "Vision Transformer (ViT) represents a paradigm shift in computer vision by applying the transformer architecture, originally designed for natural language processing, to image recognition tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction\n",
                "\n",
                "Introduced in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (Dosovitskiy et al., 2020), Vision Transformers challenged the long-standing dominance of convolutional neural networks (CNNs) in computer vision.\n",
                "\n",
                "The key insight of ViT is to treat an image as a sequence of patches, similar to how words are treated in NLP, and then process these patches using transformer encoders. This approach eliminates the inductive bias of CNNs (locality and translation equivariance) in favor of learning relationships between image patches through self-attention mechanisms."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architecture\n",
                "\n",
                "![Vision Transformer Architecture](https://storage.googleapis.com/tfjs-examples/assets/vit/vit.png)\n",
                "\n",
                "The Vision Transformer architecture consists of the following components:\n",
                "\n",
                "1. **Patch Embedding**:\n",
                "   - The input image is divided into fixed-size patches (typically 16\u00d716 pixels)\n",
                "   - Each patch is flattened and linearly projected to obtain patch embeddings\n",
                "\n",
                "2. **Position Embeddings**:\n",
                "   - Learnable positional embeddings are added to patch embeddings to retain spatial information\n",
                "   - A special [CLS] token is prepended to the sequence (used for classification)\n",
                "\n",
                "3. **Transformer Encoder**:\n",
                "   - Standard transformer encoder blocks with:\n",
                "     - Multi-head self-attention (MSA)\n",
                "     - MLP blocks (Normalization, Dense layers, GELU activation)\n",
                "     - Residual connections\n",
                "\n",
                "4. **Classification Head**:\n",
                "   - The output embedding of the [CLS] token is used for final classification\n",
                "   - A simple MLP head maps the embedding to class predictions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation Example\n",
                "\n",
                "Below is a simplified implementation of Vision Transformer in PyTorch:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class PatchEmbedding(nn.Module):\n",
                "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
                "        super().__init__()\n",
                "        self.img_size = img_size\n",
                "        self.patch_size = patch_size\n",
                "        self.n_patches = (img_size // patch_size) ** 2\n",
                "        \n",
                "        # Linear projection of flattened patches\n",
                "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # x: (batch_size, in_channels, img_size, img_size)\n",
                "        x = self.proj(x)  # (batch_size, embed_dim, h, w) where h=w=img_size//patch_size\n",
                "        x = x.flatten(2)  # (batch_size, embed_dim, n_patches)\n",
                "        x = x.transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n",
                "        return x\n",
                "\n",
                "class TransformerEncoder(nn.Module):\n",
                "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.ln1 = nn.LayerNorm(embed_dim)\n",
                "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
                "        self.ln2 = nn.LayerNorm(embed_dim)\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
                "            nn.GELU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
                "            nn.Dropout(dropout)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Self-attention block\n",
                "        ln_x = self.ln1(x)\n",
                "        attn_output, _ = self.attn(ln_x, ln_x, ln_x)\n",
                "        x = x + attn_output\n",
                "        \n",
                "        # MLP block\n",
                "        ln_x = self.ln2(x)\n",
                "        x = x + self.mlp(ln_x)\n",
                "        return x\n",
                "\n",
                "class VisionTransformer(nn.Module):\n",
                "    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n",
                "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0,\n",
                "                 num_classes=1000, dropout=0.1):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Patch embedding\n",
                "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
                "        num_patches = self.patch_embed.n_patches\n",
                "        \n",
                "        # Class token and position embeddings\n",
                "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
                "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
                "        self.pos_drop = nn.Dropout(dropout)\n",
                "        \n",
                "        # Transformer encoder layers\n",
                "        self.blocks = nn.ModuleList([\n",
                "            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout) \n",
                "            for _ in range(depth)\n",
                "        ])\n",
                "        \n",
                "        # Classification head\n",
                "        self.norm = nn.LayerNorm(embed_dim)\n",
                "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
                "        \n",
                "        # Initialize weights\n",
                "        self._init_weights()\n",
                "        \n",
                "    def _init_weights(self):\n",
                "        # Initialize patch_embed, cls_token, pos_embed, and linear layers\n",
                "        nn.init.normal_(self.cls_token, std=0.02)\n",
                "        nn.init.normal_(self.pos_embed, std=0.02)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Get patch embeddings\n",
                "        x = self.patch_embed(x)  # (B, N, D)\n",
                "        \n",
                "        # Add class token\n",
                "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # (B, 1, D)\n",
                "        x = torch.cat((cls_token, x), dim=1)  # (B, N+1, D)\n",
                "        \n",
                "        # Add position embedding\n",
                "        x = x + self.pos_embed\n",
                "        x = self.pos_drop(x)\n",
                "        \n",
                "        # Apply transformer blocks\n",
                "        for block in self.blocks:\n",
                "            x = block(x)\n",
                "        \n",
                "        # Classification from [CLS] token\n",
                "        x = self.norm(x)\n",
                "        x = x[:, 0]  # Take only the cls token embedding\n",
                "        x = self.head(x)\n",
                "        \n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using a Pre-trained Vision Transformer (ViT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using timm library (PyTorch Image Models)\n",
                "import timm\n",
                "import torch\n",
                "from PIL import Image\n",
                "import torchvision.transforms as transforms\n",
                "\n",
                "# Load a pretrained ViT model\n",
                "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
                "model.eval()\n",
                "\n",
                "# Image preprocessing\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
                "])\n",
                "\n",
                "# Example inference (requires an image file)\n",
                "# img = Image.open('example.jpg')\n",
                "# img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
                "# with torch.no_grad():\n",
                "#     output = model(img_tensor)\n",
                "# predicted_class = output.argmax().item()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Innovations and Advantages\n",
                "\n",
                "1. **Global Context**: Unlike CNNs that build features hierarchically, ViT processes global image context from the first layer through self-attention.\n",
                "\n",
                "2. **Scalability**: ViT scales extremely well with increased data and model size, often outperforming CNNs when trained on large datasets.\n",
                "\n",
                "3. **Transfer Learning**: Pre-training on large datasets (like JFT-300M) enables strong transfer learning capabilities to downstream tasks.\n",
                "\n",
                "4. **Architectural Simplicity**: ViT uses a standard Transformer encoder without specialized components for vision tasks.\n",
                "\n",
                "5. **Data Efficiency**: While the original ViT requires large datasets, subsequent variants have improved data efficiency.\n",
                "\n",
                "## Limitations\n",
                "\n",
                "1. **Data Hunger**: Original ViT performs poorly when trained from scratch on smaller datasets (e.g., ImageNet-1K) without pre-training on larger datasets.\n",
                "\n",
                "2. **Computational Cost**: Self-attention has quadratic complexity with respect to the number of patches, limiting resolution.\n",
                "\n",
                "3. **Inductive Bias**: ViT lacks the inductive biases of CNNs (locality, translation equivariance), requiring more data to learn these patterns.\n",
                "\n",
                "## Variants and Evolution\n",
                "\n",
                "Since the original ViT paper, several variants have been proposed:\n",
                "\n",
                "- **DeiT** (Data-efficient Image Transformers): Improves training efficiency through distillation from CNN teachers.\n",
                "- **Swin Transformer**: Introduces hierarchical structure with shifted windows, combining CNN and Transformer strengths.\n",
                "- **MViT** (Multiscale Vision Transformers): Uses a pyramidal structure with pooling attention.\n",
                "- **ViT-G/14**: Scaling up to giant models (1.8B parameters) for state-of-the-art performance.\n",
                "- **MAE** (Masked Autoencoders): Self-supervised pre-training by reconstructing masked image patches.\n",
                "- **ViTDet**: Adaptation of ViT for object detection tasks.\n",
                "- **Segment Anything Model (SAM)**: Uses ViT backbone for universal image segmentation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison: CNNs vs. Vision Transformers\n",
                "\n",
                "| Aspect | CNNs | Vision Transformers |\n",
                "|--------|------|--------------------|\n",
                "| **Inductive Bias** | Strong spatial locality and translation equivariance | Minimal inductive bias, learns spatial relationships |\n",
                "| **Data Efficiency** | Better with limited data | Requires more data unless special techniques are used |\n",
                "| **Scalability** | Diminishing returns with scale | Scales very well with more data and parameters |\n",
                "| **Global Context** | Requires deep networks to capture global context | Global context from first layer via self-attention |\n",
                "| **Computational Cost** | Linear scaling with image size | Quadratic complexity with sequence length |\n",
                "| **Interpretability** | Feature maps have spatial correspondence | Attention maps can show patch relationships |\n",
                "| **Architecture** | Domain-specific design (kernels, pooling) | Generic transformer blocks |\n",
                "| **State-of-the-art** | Historically dominated computer vision | Increasingly competitive, especially at scale |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications\n",
                "\n",
                "Vision Transformers have been successfully applied to numerous computer vision tasks:\n",
                "\n",
                "- **Image Classification**: Achieving state-of-the-art on benchmarks like ImageNet\n",
                "- **Object Detection**: ViTDet adapts Vision Transformers for detection tasks\n",
                "- **Semantic Segmentation**: Models like SETR use ViT for pixel-level predictions\n",
                "- **Image Generation**: Combined with diffusion models for high-quality image synthesis\n",
                "- **Video Understanding**: Extended to process video frames with space-time attention\n",
                "- **Multi-modal Learning**: Foundation for models like CLIP that connect vision and language\n",
                "- **Self-supervised Learning**: Core component in masked autoencoding approaches like MAE"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "1. Dosovitskiy, A., et al. (2020). [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929). ICLR 2021.\n",
                "\n",
                "2. Touvron, H., et al. (2021). [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877). ICML 2021.\n",
                "\n",
                "3. Liu, Z., et al. (2021). [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030). ICCV 2021.\n",
                "\n",
                "4. He, K., et al. (2022). [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377). CVPR 2022.\n",
                "\n",
                "5. Zhai, X., et al. (2022). [Scaling Vision Transformers](https://arxiv.org/abs/2106.04560). CVPR 2022.\n",
                "\n",
                "6. Radford, A., et al. (2021). [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020). ICML 2021."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}