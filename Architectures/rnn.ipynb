{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Recurrent Neural Networks (RNNs)\n",
                "\n",
                "Recurrent Neural Networks are a class of artificial neural networks designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or numerical time series data. Unlike traditional feedforward neural networks, RNNs maintain an internal state (memory) that captures information about what has been calculated so far."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Basic Architecture\n",
                "\n",
                "The fundamental feature of an RNN is the feedback loop within the network that allows information to persist. This creates a form of memory that enables processing of sequences.\n",
                "\n",
                "### Key Components\n",
                "\n",
                "- **Input Layer**: Receives data at each time step\n",
                "- **Hidden State**: Maintains memory between time steps\n",
                "- **Output Layer**: Produces prediction at each time step\n",
                "- **Recurrent Connection**: Passes the hidden state from one time step to the next\n",
                "\n",
                "### Mathematical Representation\n",
                "\n",
                "At each time step $t$, an RNN processes an input $x_t$ and updates its hidden state $h_t$:\n",
                "\n",
                "$h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$\n",
                "\n",
                "The output $y_t$ is then computed as:\n",
                "\n",
                "$y_t = W_{hy}h_t + b_y$\n",
                "\n",
                "Where:\n",
                "- $W_{xh}$: Input-to-hidden weights\n",
                "- $W_{hh}$: Hidden-to-hidden (recurrent) weights\n",
                "- $W_{hy}$: Hidden-to-output weights\n",
                "- $b_h$ and $b_y$: Bias terms"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## RNN Variants\n",
                "\n",
                "### Simple/Vanilla RNN\n",
                "The basic form described above, which suffers from vanishing/exploding gradient problems.\n",
                "\n",
                "### Bidirectional RNN\n",
                "Processes sequences in both forward and backward directions, capturing context from both past and future.\n",
                "\n",
                "### Deep (Stacked) RNN\n",
                "Multiple recurrent layers stacked on top of each other, allowing the network to learn more complex patterns.\n",
                "\n",
                "### Gated RNNs\n",
                "- **LSTM (Long Short-Term Memory)**: Introduces cell state and gating mechanisms to combat vanishing gradients\n",
                "- **GRU (Gated Recurrent Unit)**: Simplified version of LSTM with fewer parameters\n",
                "\n",
                "These variants address the limitations of vanilla RNNs and are more commonly used in practice."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training RNNs\n",
                "\n",
                "RNNs are typically trained using Backpropagation Through Time (BPTT), which unfolds the recurrent network across time steps and applies standard backpropagation.\n",
                "\n",
                "### Challenges in Training\n",
                "\n",
                "1. **Vanishing Gradients**: Over many time steps, gradients can become extremely small, preventing learning of long-term dependencies.\n",
                "2. **Exploding Gradients**: Conversely, gradients can become extremely large, causing instability in training.\n",
                "3. **Long Training Times**: Processing sequences sequentially is computationally intensive.\n",
                "\n",
                "### Techniques to Address Challenges\n",
                "\n",
                "- **Gradient Clipping**: Prevents exploding gradients by scaling gradients when they exceed a threshold.\n",
                "- **Truncated BPTT**: Limits backpropagation to a fixed number of time steps.\n",
                "- **Gated Architectures**: LSTMs and GRUs help mitigate vanishing gradients."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications\n",
                "\n",
                "RNNs excel at sequence-based tasks including:\n",
                "\n",
                "1. **Natural Language Processing**:\n",
                "   - Language modeling\n",
                "   - Text generation\n",
                "   - Machine translation\n",
                "   - Sentiment analysis\n",
                "\n",
                "2. **Speech Recognition**:\n",
                "   - Converting speech to text\n",
                "   - Speaker identification\n",
                "\n",
                "3. **Time Series Analysis**:\n",
                "   - Stock price prediction\n",
                "   - Weather forecasting\n",
                "   - Sensor data analysis\n",
                "\n",
                "4. **Music Generation**:\n",
                "   - Composing melodies and harmonies\n",
                "   - Predicting musical sequences\n",
                "\n",
                "5. **Anomaly Detection**:\n",
                "   - Identifying unusual patterns in sequential data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple RNN implementation in TensorFlow/Keras\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import SimpleRNN, Dense\n",
                "\n",
                "# Define a basic RNN model\n",
                "def create_rnn_model(input_shape, output_size):\n",
                "    model = Sequential([\n",
                "        SimpleRNN(64, input_shape=input_shape, return_sequences=True),\n",
                "        SimpleRNN(32),\n",
                "        Dense(output_size, activation='softmax')\n",
                "    ])\n",
                "    \n",
                "    model.compile(optimizer='adam',\n",
                "                  loss='categorical_crossentropy',\n",
                "                  metrics=['accuracy'])\n",
                "    return model\n",
                "\n",
                "# Example usage\n",
                "# model = create_rnn_model((sequence_length, features), num_classes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RNN implementation in PyTorch\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class SimpleRNN(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        super(SimpleRNN, self).__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
                "        self.fc = nn.Linear(hidden_size, output_size)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # x shape: (batch_size, sequence_length, input_size)\n",
                "        batch_size = x.size(0)\n",
                "        \n",
                "        # Initialize hidden state\n",
                "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
                "        \n",
                "        # Forward propagate RNN\n",
                "        out, _ = self.rnn(x, h0)\n",
                "        \n",
                "        # Get output from last time step\n",
                "        out = self.fc(out[:, -1, :])\n",
                "        return out\n",
                "\n",
                "# Example usage\n",
                "# model = SimpleRNN(input_size=10, hidden_size=64, output_size=5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advantages and Limitations\n",
                "\n",
                "### Advantages:\n",
                "- Natural fit for sequential data processing\n",
                "- Ability to handle variable-length sequences\n",
                "- Parameter sharing across time steps enables generalization\n",
                "- Can be extended to bidirectional processing for capturing context\n",
                "\n",
                "### Limitations:\n",
                "- Difficulty learning long-term dependencies (vanilla RNNs)\n",
                "- Computationally expensive for very long sequences\n",
                "- Sequential nature limits parallelization during training\n",
                "- Often outperformed by Transformer-based architectures for many sequence tasks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Historical Context\n",
                "\n",
                "- **1980s**: Early forms of RNNs proposed by John Hopfield and others\n",
                "- **1990**: Backpropagation Through Time (BPTT) formalized by Paul Werbos\n",
                "- **1997**: LSTM architecture introduced by Hochreiter and Schmidhuber to address vanishing gradients\n",
                "- **2000s**: RNNs gained prominence in speech recognition and NLP\n",
                "- **2014**: GRU architecture proposed by Cho et al. as a simplified alternative to LSTMs\n",
                "- **2015-2017**: Peak of RNN usage in sequence modeling tasks\n",
                "- **2017-Present**: Transformers begin to replace RNNs in many applications, though RNNs remain relevant for specific use cases"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "- Elman, J. L. (1990). [Finding structure in time](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1). Cognitive Science, 14(2), 179-211.\n",
                "- Hochreiter, S., & Schmidhuber, J. (1997). [Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf). Neural Computation, 9(8), 1735\u20131780.\n",
                "- Cho, K., et al. (2014). [Learning phrase representations using RNN encoder-decoder for statistical machine translation](https://arxiv.org/abs/1406.1078). EMNLP.\n",
                "- Graves, A. (2012). [Supervised Sequence Labelling with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/preprint.pdf). Studies in Computational Intelligence.\n",
                "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). [Deep Learning](https://www.deeplearningbook.org/). MIT Press. Chapter 10.\n",
                "- Karpathy, A. (2015). [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Blog post."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}