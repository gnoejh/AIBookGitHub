{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Llama Architecture\n",
                "\n",
                "## Overview\n",
                "\n",
                "Llama (Large Language Model Meta AI) is a family of open-weight large language models developed by Meta (formerly Facebook). The Llama models have gained significant attention for their performance capabilities while being more accessible than many competing closed-source alternatives.\n",
                "\n",
                "## Key Features\n",
                "\n",
                "- **Open-Weight Design**: Unlike many proprietary LLMs, Llama models have their weights publicly available (under license)\n",
                "- **Efficient Architecture**: Uses optimized Transformer architecture with improvements for computational efficiency\n",
                "- **Multiple Size Variants**: Available in various parameter sizes (7B, 13B, 34B, 70B in Llama 2)\n",
                "- **Context Length**: Supports context windows of 4K tokens (extended in later versions)\n",
                "- **Instruction Tuning**: Llama 2-Chat variants specifically fine-tuned for conversation and instruction following\n",
                "\n",
                "## Architecture Specifics\n",
                "\n",
                "Llama is based on the Transformer architecture with decoder-only design and incorporates several optimizations:\n",
                "\n",
                "- Pre-normalization using RMSNorm\n",
                "- SwiGLU activation function instead of ReLU\n",
                "- Rotary positional embeddings (RoPE)\n",
                "- Vocabulary size of 32K tokens\n",
                "- Trained using AdamW optimizer\n",
                "\n",
                "## Evolution\n",
                "\n",
                "- **Llama 1** (2023): Initial release with 7B-65B parameters\n",
                "- **Llama 2** (2023): Improved architecture with better performance and safety features\n",
                "- **Llama 2-Chat**: Fine-tuned specifically for conversational AI use cases\n",
                "- **Llama 3** (2024): Further improvements in performance and capabilities\n",
                "\n",
                "## Usage Examples\n",
                "\n",
                "```python\n",
                "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
                "\n",
                "# Load pre-trained model and tokenizer\n",
                "model_id = \"meta-llama/Llama-2-7b\"\n",
                "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
                "model = LlamaForCausalLM.from_pretrained(model_id)\n",
                "\n",
                "# Generate text\n",
                "inputs = tokenizer(\"Explain how neural networks work:\", return_tensors=\"pt\")\n",
                "outputs = model.generate(inputs.input_ids, max_length=200)\n",
                "print(tokenizer.decode(outputs[0]))\n",
                "```\n",
                "\n",
                "## References\n",
                "\n",
                "- Touvron, H., et al. (2023). [Llama: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971). arXiv.\n",
                "- Touvron, H., et al. (2023). [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288). arXiv.\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}