{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformer Architecture\n",
                "\n",
                "## Introduction\n",
                "\n",
                "The Transformer is a neural network architecture introduced in the paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017. It revolutionized natural language processing by eliminating recurrence and convolutions entirely, relying solely on attention mechanisms to draw global dependencies between input and output. This architecture became the foundation for models like BERT, GPT, and other state-of-the-art language models."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Historical Context\n",
                "\n",
                "Before Transformers, sequence modeling primarily relied on:\n",
                "- **RNNs/LSTMs/GRUs**: Suffered from sequential computation limitations\n",
                "- **CNNs for sequences**: Required many layers to capture long-range dependencies\n",
                "\n",
                "The key innovation of Transformers was addressing these limitations by processing all positions simultaneously while maintaining the ability to model dependencies regardless of distance in the sequence."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architecture Details\n",
                "\n",
                "The Transformer architecture consists of an encoder and a decoder, each composed of stacks of identical layers.\n",
                "\n",
                "### Core Components\n",
                "\n",
                "![Transformer Architecture](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_12.17.05_AM_IaJn9v9.png)\n",
                "\n",
                "#### 1. Multi-Head Attention\n",
                "\n",
                "The key innovation in Transformers is the attention mechanism, specifically **self-attention**:\n",
                "\n",
                "- **Query (Q)**, **Key (K)**, and **Value (V)** matrices derived from input embeddings\n",
                "- Attention weights calculated as: $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
                "- **Multi-head attention** performs this operation in parallel across different representation subspaces\n",
                "\n",
                "#### 2. Position-wise Feed-Forward Networks\n",
                "\n",
                "Each encoder and decoder layer contains a fully connected feed-forward network applied to each position separately:\n",
                "- Two linear transformations with a ReLU activation in between\n",
                "- $\\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2$\n",
                "\n",
                "#### 3. Positional Encoding\n",
                "\n",
                "Since the model contains no recurrence or convolution, positional encodings are added to provide information about token positions:\n",
                "- Sine and cosine functions of different frequencies\n",
                "- $PE_{(pos,2i)} = \\sin(pos/10000^{2i/d_{model}})$\n",
                "- $PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d_{model}})$\n",
                "\n",
                "#### 4. Residual Connections and Layer Normalization\n",
                "\n",
                "Each sublayer has:\n",
                "- Residual connections (skip connections)\n",
                "- Layer normalization\n",
                "- $\\text{LayerNorm}(x + \\text{Sublayer}(x))$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Innovations\n",
                "\n",
                "1. **Parallelization**: Unlike RNNs, Transformers process all tokens simultaneously, enabling much faster training\n",
                "2. **Attention Mechanism**: Can directly model dependencies between any positions regardless of distance\n",
                "3. **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces\n",
                "4. **No Vanishing Gradient**: Direct connections between any positions help with gradient flow\n",
                "5. **Constant Path Length**: The maximum path length between any two positions is O(1) instead of O(n) with RNNs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications\n",
                "\n",
                "Transformers have become dominant in many natural language processing tasks:\n",
                "\n",
                "1. **Machine Translation**: The original application shown in the paper\n",
                "2. **Text Generation**: GPT family models use decoder-only Transformers\n",
                "3. **Language Understanding**: BERT uses encoder-only Transformers \n",
                "4. **Summarization**: Models like BART and T5 use the full encoder-decoder architecture\n",
                "5. **Speech Recognition**: Models like Whisper use Transformers for audio processing\n",
                "6. **Computer Vision**: Vision Transformers (ViT) adapt the architecture for image tasks\n",
                "\n",
                "The versatility of Transformers has led to their adoption across multiple domains beyond NLP."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation Example\n",
                "\n",
                "### Basic Transformer Implementation in PyTorch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import math\n",
                "\n",
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, d_model, num_heads):\n",
                "        super().__init__()\n",
                "        assert d_model % num_heads == 0\n",
                "        \n",
                "        self.d_model = d_model\n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        \n",
                "        self.wq = nn.Linear(d_model, d_model)\n",
                "        self.wk = nn.Linear(d_model, d_model)\n",
                "        self.wv = nn.Linear(d_model, d_model)\n",
                "        self.wo = nn.Linear(d_model, d_model)\n",
                "        \n",
                "    def forward(self, q, k, v, mask=None):\n",
                "        batch_size = q.size(0)\n",
                "        \n",
                "        # Apply linear projections\n",
                "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        # Calculate attention scores\n",
                "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
                "        \n",
                "        # Apply mask if provided\n",
                "        if mask is not None:\n",
                "            scores = scores.masked_fill(mask == 0, -1e9)\n",
                "        \n",
                "        # Apply softmax and multiply with values\n",
                "        attention = torch.softmax(scores, dim=-1)\n",
                "        output = torch.matmul(attention, v)\n",
                "        \n",
                "        # Reshape and apply final linear layer\n",
                "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
                "        return self.wo(output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionwiseFeedForward(nn.Module):\n",
                "    def __init__(self, d_model, d_ff):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(d_model, d_ff)\n",
                "        self.fc2 = nn.Linear(d_ff, d_model)\n",
                "        self.relu = nn.ReLU()\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.fc2(self.relu(self.fc1(x)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderLayer(nn.Module):\n",
                "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
                "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
                "        self.norm1 = nn.LayerNorm(d_model)\n",
                "        self.norm2 = nn.LayerNorm(d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "    def forward(self, x, mask=None):\n",
                "        # Self-attention with residual connection and layer norm\n",
                "        attn_output = self.self_attn(x, x, x, mask)\n",
                "        x = self.norm1(x + self.dropout(attn_output))\n",
                "        \n",
                "        # Feed-forward with residual connection and layer norm\n",
                "        ff_output = self.feed_forward(x)\n",
                "        x = self.norm2(x + self.dropout(ff_output))\n",
                "        \n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "    def __init__(self, d_model, max_len=5000):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Create positional encoding matrix\n",
                "        pe = torch.zeros(max_len, d_model)\n",
                "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
                "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
                "        \n",
                "        # Apply sine to even indices and cosine to odd indices\n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "        \n",
                "        # Register as buffer (not a parameter, but part of the module)\n",
                "        self.register_buffer('pe', pe.unsqueeze(0))\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Add positional encoding to input embeddings\n",
                "        return x + self.pe[:, :x.size(1), :]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using Pre-trained Transformers with Hugging Face"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pip install transformers\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "\n",
                "# Load pre-trained BERT model (transformer encoder)\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
                "\n",
                "# Example usage\n",
                "text = \"Transformers are powerful neural network architectures.\"\n",
                "inputs = tokenizer(text, return_tensors=\"pt\")\n",
                "outputs = model(**inputs)\n",
                "\n",
                "# Get the embeddings from the last hidden state\n",
                "last_hidden_states = outputs.last_hidden_state"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Variants and Extensions\n",
                "\n",
                "The original Transformer architecture has evolved into several variants:\n",
                "\n",
                "1. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
                "   - Uses only the encoder portion of the Transformer\n",
                "   - Pre-trained using masked language modeling and next sentence prediction\n",
                "   \n",
                "2. **GPT (Generative Pre-trained Transformer)**\n",
                "   - Uses only the decoder portion (with modifications)\n",
                "   - Auto-regressive language model trained to predict next tokens\n",
                "   \n",
                "3. **T5 (Text-to-Text Transfer Transformer)**\n",
                "   - Encoder-decoder architecture\n",
                "   - Frames all NLP tasks as text-to-text problems\n",
                "   \n",
                "4. **Vision Transformer (ViT)**\n",
                "   - Adapts Transformer architecture for computer vision\n",
                "   - Splits images into patches and processes them as token sequences\n",
                "   \n",
                "5. **Efficient Transformers**\n",
                "   - Reformer, Linformer, Performer, etc.\n",
                "   - Address the quadratic complexity of self-attention through various approximations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advantages and Limitations\n",
                "\n",
                "### Advantages\n",
                "\n",
                "- **Parallelization**: Much faster training than sequential models\n",
                "- **Long-range dependencies**: Can model relationships between distant tokens\n",
                "- **Versatility**: Applicable to various tasks and domains\n",
                "- **Scalability**: Performance continues to improve with larger models and more data\n",
                "\n",
                "### Limitations\n",
                "\n",
                "- **Quadratic complexity**: Self-attention is O(n²) with sequence length\n",
                "- **Fixed context window**: Standard Transformers have a limited input length\n",
                "- **Positional encoding**: Less intuitive handling of sequential information compared to RNNs\n",
                "- **Resource intensive**: Large Transformer models require significant computational resources"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References and Further Reading\n",
                "\n",
                "- Vaswani, A., et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). NIPS.\n",
                "- Devlin, J., et al. (2018). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). arXiv.\n",
                "- Radford, A., et al. (2018). [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf). OpenAI.\n",
                "- Brown, T., et al. (2020). [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165). NeurIPS.\n",
                "- Dosovitskiy, A., et al. (2020). [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929). ICLR.\n",
                "- Tay, Y., et al. (2022). [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732). ACM Computing Surveys."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "py312sb3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
