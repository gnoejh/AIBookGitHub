{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/dbn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Belief Networks (DBN)\n",
    "\n",
    "Deep Belief Networks are probabilistic generative models composed of multiple layers of stochastic, latent variables. They represent one of the earliest successful deep learning architectures and pioneered many of the techniques used in modern deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Context\n",
    "\n",
    "Deep Belief Networks were introduced by Geoffrey Hinton and colleagues in 2006, during what is now called the \"Deep Learning Renaissance.\" Prior to this work:\n",
    "\n",
    "1. Training deep neural networks was extremely difficult due to the vanishing gradient problem\n",
    "2. There was no effective way to pre-train deep architectures\n",
    "3. Unsupervised learning in deep networks was largely unexplored\n",
    "\n",
    "Hinton's breakthrough came with the realization that deep networks could be trained effectively by:\n",
    "\n",
    "1. Pre-training each layer as a Restricted Boltzmann Machine (RBM)\n",
    "2. Stacking these RBMs to form a deep architecture\n",
    "3. Fine-tuning the entire network using supervised learning\n",
    "\n",
    "This work helped revitalize neural network research and laid the groundwork for the deep learning revolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "A Deep Belief Network consists of:\n",
    "\n",
    "1. **Multiple Layers** of Restricted Boltzmann Machines (RBMs)\n",
    "2. **Visible Layer** that receives input data\n",
    "3. **Multiple Hidden Layers** that capture increasingly abstract features\n",
    "\n",
    "![DBN Architecture](https://miro.medium.com/max/700/1*WGBv7xMNL-StLmfX7qoGZA.png)\n",
    "\n",
    "The key insight behind DBNs is that each layer can be trained greedily, one at a time, to learn a representation of its input. The trained layers are then stacked to form a deep network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machines (RBMs)\n",
    "\n",
    "Before diving deeper into DBNs, we need to understand the building blocks: Restricted Boltzmann Machines.\n",
    "\n",
    "An RBM is a two-layer, undirected graphical model consisting of:\n",
    "- A visible (input) layer\n",
    "- A hidden layer\n",
    "- Connections between the visible and hidden layers (but no connections within each layer)\n",
    "\n",
    "The energy function of an RBM is defined as:\n",
    "\n",
    "$$E(\\mathbf{v}, \\mathbf{h}) = -\\sum_i a_i v_i - \\sum_j b_j h_j - \\sum_{i,j} v_i w_{ij} h_j$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{v}$ is the vector of visible units\n",
    "- $\\mathbf{h}$ is the vector of hidden units\n",
    "- $a_i$ and $b_j$ are the biases\n",
    "- $w_{ij}$ are the weights connecting visible and hidden units\n",
    "\n",
    "From this energy function, we can derive the joint probability distribution:\n",
    "\n",
    "$$P(\\mathbf{v}, \\mathbf{h}) = \\frac{1}{Z} e^{-E(\\mathbf{v}, \\mathbf{h})}$$\n",
    "\n",
    "Where $Z$ is the partition function (normalizing constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RBMs with Contrastive Divergence\n",
    "\n",
    "RBMs are trained using an algorithm called Contrastive Divergence (CD), which approximates the gradient of the log-likelihood. The steps are:\n",
    "\n",
    "1. Start with a training example $\\mathbf{v}^{(0)}$ (data phase)\n",
    "2. Sample hidden units $\\mathbf{h}^{(0)}$ given $\\mathbf{v}^{(0)}$\n",
    "3. Sample visible units $\\mathbf{v}^{(1)}$ given $\\mathbf{h}^{(0)}$ (reconstruction phase)\n",
    "4. Sample hidden units $\\mathbf{h}^{(1)}$ given $\\mathbf{v}^{(1)}$\n",
    "5. Update weights: $\\Delta w_{ij} = \\epsilon (v_i^{(0)} h_j^{(0)} - v_i^{(1)} h_j^{(1)})$\n",
    "\n",
    "In practice, CD-k is often used, where k steps of alternating Gibbs sampling are performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, num_visible, num_hidden, learning_rate=0.1):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.normal(0, 0.1, (num_visible, num_hidden))\n",
    "        self.visible_bias = np.zeros(num_visible)\n",
    "        self.hidden_bias = np.zeros(num_hidden)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sample_hidden(self, visible_prob):\n",
    "        # Calculate hidden activation\n",
    "        hidden_activation = np.dot(visible_prob, self.W) + self.hidden_bias\n",
    "        hidden_prob = self.sigmoid(hidden_activation)\n",
    "        hidden_states = np.random.binomial(1, hidden_prob)\n",
    "        return hidden_prob, hidden_states\n",
    "    \n",
    "    def sample_visible(self, hidden_prob):\n",
    "        # Calculate visible activation\n",
    "        visible_activation = np.dot(hidden_prob, self.W.T) + self.visible_bias\n",
    "        visible_prob = self.sigmoid(visible_activation)\n",
    "        visible_states = np.random.binomial(1, visible_prob)\n",
    "        return visible_prob, visible_states\n",
    "    \n",
    "    def contrastive_divergence(self, data, k=1):\n",
    "        # Positive phase\n",
    "        pos_hidden_prob, pos_hidden_states = self.sample_hidden(data)\n",
    "        \n",
    "        # Initialize chain for negative phase\n",
    "        hidden_states = pos_hidden_states\n",
    "        \n",
    "        # Gibbs sampling\n",
    "        for _ in range(k):\n",
    "            vis_prob, _ = self.sample_visible(hidden_states)\n",
    "            hidden_prob, hidden_states = self.sample_hidden(vis_prob)\n",
    "        \n",
    "        # Compute gradients and update weights\n",
    "        pos_associations = np.dot(data.T, pos_hidden_prob)\n",
    "        neg_associations = np.dot(vis_prob.T, hidden_prob)\n",
    "        \n",
    "        self.W += self.learning_rate * (pos_associations - neg_associations) / data.shape[0]\n",
    "        self.visible_bias += self.learning_rate * np.mean(data - vis_prob, axis=0)\n",
    "        self.hidden_bias += self.learning_rate * np.mean(pos_hidden_prob - hidden_prob, axis=0)\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        error = np.mean(np.sum((data - vis_prob) ** 2, axis=1))\n",
    "        return error\n",
    "    \n",
    "    def train(self, data, epochs=100, batch_size=10, k=1):\n",
    "        num_samples = data.shape[0]\n",
    "        num_batches = num_samples // batch_size\n",
    "        \n",
    "        errors = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_error = 0\n",
    "            np.random.shuffle(data)\n",
    "            \n",
    "            for batch in range(num_batches):\n",
    "                start = batch * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_data = data[start:end]\n",
    "                \n",
    "                batch_error = self.contrastive_divergence(batch_data, k)\n",
    "                epoch_error += batch_error\n",
    "            \n",
    "            avg_error = epoch_error / num_batches\n",
    "            errors.append(avg_error)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Reconstruction Error: {avg_error:.4f}\")\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def reconstruct(self, data):\n",
    "        hidden_prob, _ = self.sample_hidden(data)\n",
    "        visible_prob, _ = self.sample_visible(hidden_prob)\n",
    "        return visible_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Belief Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.1):\n",
    "        \"\"\"Initialize a Deep Belief Network.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of layer sizes, from input to output\n",
    "            learning_rate: Learning rate for RBMs\n",
    "        \"\"\"\n",
    "        self.rbm_layers = []\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Create RBM layers\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.rbm_layers.append(RBM(layer_sizes[i], layer_sizes[i+1], learning_rate))\n",
    "    \n",
    "    def pretrain(self, data, epochs=10, batch_size=10, k=1):\n",
    "        \"\"\"Greedy layer-wise pretraining of DBN.\"\"\"\n",
    "        input_data = data\n",
    "        \n",
    "        for i, rbm in enumerate(self.rbm_layers):\n",
    "            print(f\"\\nTraining RBM layer {i+1}/{len(self.rbm_layers)}\")\n",
    "            rbm.train(input_data, epochs=epochs, batch_size=batch_size, k=k)\n",
    "            \n",
    "            # Transform data for next layer\n",
    "            hidden_probs = []\n",
    "            for batch_start in range(0, len(input_data), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(input_data))\n",
    "                batch_data = input_data[batch_start:batch_end]\n",
    "                hidden_prob, _ = rbm.sample_hidden(batch_data)\n",
    "                hidden_probs.append(hidden_prob)\n",
    "            \n",
    "            input_data = np.vstack(hidden_probs)\n",
    "    \n",
    "    def generate(self, num_samples=1, k=1000):\n",
    "        \"\"\"Generate samples from the DBN.\"\"\"\n",
    "        # Initialize with random values in the top layer\n",
    "        top_layer_size = self.layer_sizes[-1]\n",
    "        samples = np.random.binomial(1, 0.5, size=(num_samples, top_layer_size))\n",
    "        \n",
    "        # Generate from the top-level RBM with Gibbs sampling\n",
    "        top_rbm = self.rbm_layers[-1]\n",
    "        for _ in range(k):\n",
    "            visible_prob, _ = top_rbm.sample_visible(samples)\n",
    "            hidden_prob, samples = top_rbm.sample_hidden(visible_prob)\n",
    "        \n",
    "        # Propagate downwards through the network\n",
    "        for i in range(len(self.rbm_layers) - 2, -1, -1):\n",
    "            rbm = self.rbm_layers[i]\n",
    "            visible_prob, samples = rbm.sample_visible(samples)\n",
    "        \n",
    "        return visible_prob\n",
    "    \n",
    "    def reconstruct(self, data):\n",
    "        \"\"\"Reconstruct data through the DBN.\"\"\"\n",
    "        # Forward pass\n",
    "        h = data\n",
    "        for rbm in self.rbm_layers:\n",
    "            hidden_prob, _ = rbm.sample_hidden(h)\n",
    "            h = hidden_prob\n",
    "        \n",
    "        # Backward pass\n",
    "        for i in range(len(self.rbm_layers) - 1, -1, -1):\n",
    "            rbm = self.rbm_layers[i]\n",
    "            visible_prob, _ = rbm.sample_visible(h)\n",
    "            h = visible_prob\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST Digit Recognition\n",
    "\n",
    "Let's train a DBN on the MNIST dataset and visualize the learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "try:\n",
    "    X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
    "except:\n",
    "    # Fallback if OpenML is not available\n",
    "    from sklearn.datasets import load_digits\n",
    "    digits = load_digits()\n",
    "    X = digits.data / 16.0  # Scale to [0,1]\n",
    "    y = digits.target\n",
    "    print(\"Using sklearn digits dataset (8x8 images) as fallback\")\n",
    "else:\n",
    "    # Scale MNIST to [0,1]\n",
    "    X = X / 255.0\n",
    "    print(\"Loaded MNIST dataset successfully\")\n",
    "\n",
    "# Take a subset for faster training\n",
    "n_samples = 5000\n",
    "X_subset = X[:n_samples]\n",
    "\n",
    "# Create and train DBN\n",
    "input_size = X_subset.shape[1]  # 784 for MNIST, 64 for digits\n",
    "dbn = DBN(layer_sizes=[input_size, 250, 100], learning_rate=0.01)\n",
    "\n",
    "# Train with small number of epochs for demonstration\n",
    "dbn.pretrain(X_subset, epochs=5, batch_size=10, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Learned Features\n",
    "\n",
    "Let's visualize what the first layer of the DBN has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weights, img_shape, grid_size):\n",
    "    \"\"\"Plot weights as images in a grid.\"\"\"\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(12, 12))\n",
    "    \n",
    "    # Reshape and scale weights for visualization\n",
    "    weights = weights.reshape([-1] + list(img_shape))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(weights):\n",
    "            # Plot weights\n",
    "            ax.imshow(weights[i], cmap='gray', interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"RBM Hidden Unit Filters\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Get the weights from the first RBM layer\n",
    "weights = dbn.rbm_layers[0].W\n",
    "\n",
    "# Determine image shape (28x28 for MNIST, 8x8 for digits)\n",
    "if input_size == 784:\n",
    "    img_shape = (28, 28)\n",
    "else:  # For the digits dataset\n",
    "    img_shape = (8, 8)\n",
    "\n",
    "# Plot 25 hidden units (5x5 grid)\n",
    "plot_weights(weights.T[:25], img_shape, (5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct Images\n",
    "\n",
    "Let's see how well our DBN can reconstruct some input images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select test images\n",
    "test_images = X[:10]  # First 10 images\n",
    "\n",
    "# Reconstruct images\n",
    "reconstructed = dbn.reconstruct(test_images)\n",
    "\n",
    "# Plot original and reconstructed images\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "\n",
    "for i in range(10):\n",
    "    # Original image\n",
    "    if input_size == 784:\n",
    "        axes[0, i].imshow(test_images[i].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "    else:  # For digits dataset\n",
    "        axes[0, i].imshow(test_images[i].reshape(8, 8), cmap='gray')\n",
    "        axes[1, i].imshow(reconstructed[i].reshape(8, 8), cmap='gray')\n",
    "    \n",
    "    axes[0, i].set_xticks([])\n",
    "    axes[0, i].set_yticks([])\n",
    "    axes[1, i].set_xticks([])\n",
    "    axes[1, i].set_yticks([])\n",
    "\n",
    "axes[0, 0].set_ylabel('Original')\n",
    "axes[1, 0].set_ylabel('Reconstructed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning DBNs for Classification\n",
    "\n",
    "After pre-training, a DBN can be fine-tuned by adding a classification layer and training with backpropagation. This approach was common before the widespread adoption of end-to-end training methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBNClassifier(DBN):\n",
    "    def __init__(self, layer_sizes, num_classes, learning_rate=0.1):\n",
    "        \"\"\"Initialize a DBN for classification.\"\"\"\n",
    "        # Initialize the base DBN\n",
    "        super().__init__(layer_sizes, learning_rate)\n",
    "        \n",
    "        # Add a classification layer\n",
    "        self.num_classes = num_classes\n",
    "        self.output_weights = np.random.normal(0, 0.1, (layer_sizes[-1], num_classes))\n",
    "        self.output_bias = np.zeros(num_classes)\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def classify(self, data):\n",
    "        \"\"\"Forward pass through the network for classification.\"\"\"\n",
    "        # Pass through DBN layers\n",
    "        h = data\n",
    "        for rbm in self.rbm_layers:\n",
    "            h, _ = rbm.sample_hidden(h)\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = np.dot(h, self.output_weights) + self.output_bias\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def fine_tune(self, X_train, y_train, epochs=100, batch_size=32, learning_rate=0.01):\n",
    "        \"\"\"Fine-tune the DBN for classification.\"\"\"\n",
    "        # Convert labels to one-hot encoding\n",
    "        y_one_hot = np.zeros((len(y_train), self.num_classes))\n",
    "        for i, label in enumerate(y_train):\n",
    "            y_one_hot[i, int(label)] = 1\n",
    "        \n",
    "        num_samples = X_train.shape[0]\n",
    "        num_batches = num_samples // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_one_hot[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for batch in range(num_batches):\n",
    "                start = batch * batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                \n",
    "                # Forward pass\n",
    "                h = X_batch.copy()\n",
    "                activations = [h]\n",
    "                \n",
    "                for rbm in self.rbm_layers:\n",
    "                    h, _ = rbm.sample_hidden(h)\n",
    "                    activations.append(h)\n",
    "                \n",
    "                # Output layer\n",
    "                logits = np.dot(h, self.output_weights) + self.output_bias\n",
    "                probs = self.softmax(logits)\n",
    "                \n",
    "                # Compute loss (cross entropy)\n",
    "                batch_loss = -np.sum(y_batch * np.log(probs + 1e-10)) / batch_size\n",
    "                epoch_loss += batch_loss\n",
    "                \n",
    "                # Backward pass (simplified backpropagation)\n",
    "                # Output layer gradients\n",
    "                delta = probs - y_batch\n",
    "                output_grad = np.dot(activations[-1].T, delta) / batch_size\n",
    "                output_bias_grad = np.mean(delta, axis=0)\n",
    "                \n",
    "                # Update output layer\n",
    "                self.output_weights -= learning_rate * output_grad\n",
    "                self.output_bias -= learning_rate * output_bias_grad\n",
    "                \n",
    "                # For simplicity, we're only updating the output layer here\n",
    "                # In a full implementation, we would backpropagate through all layers\n",
    "            \n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                # Evaluate accuracy\n",
    "                predictions = np.argmax(self.classify(X_train), axis=1)\n",
    "                accuracy = np.mean(predictions == y_train)\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate the classifier on test data.\"\"\"\n",
    "        predictions = np.argmax(self.classify(X_test), axis=1)\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DBN Classifier on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset for faster demonstration\n",
    "n_train = 5000\n",
    "X_train = X[:n_train]\n",
    "y_train = np.array([int(label) for label in y[:n_train]])\n",
    "\n",
    "# Create DBN classifier\n",
    "input_size = X_train.shape[1]  # 784 for MNIST, 64 for digits\n",
    "dbn_classifier = DBNClassifier(\n",
    "    layer_sizes=[input_size, 250, 100], \n",
    "    num_classes=10, \n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Pre-train the DBN\n",
    "print(\"Pre-training DBN layers...\")\n",
    "dbn_classifier.pretrain(X_train, epochs=3, batch_size=10, k=1)\n",
    "\n",
    "# Fine-tune for classification\n",
    "print(\"\\nFine-tuning for classification...\")\n",
    "dbn_classifier.fine_tune(X_train, y_train, epochs=50, batch_size=32, learning_rate=0.01)\n",
    "\n",
    "# Evaluate on test set (we'll use a subset for speed)\n",
    "n_test = 1000\n",
    "X_test = X[n_train:n_train+n_test]\n",
    "y_test = np.array([int(label) for label in y[n_train:n_train+n_test]])\n",
    "\n",
    "accuracy = dbn_classifier.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
