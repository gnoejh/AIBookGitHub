{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/dalle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DALL-E Architecture\n",
    "\n",
    "## Overview\n",
    "\n",
    "DALL-E is a multimodal AI model developed by OpenAI that generates images from text descriptions. Named as a portmanteau of Salvador Dal√≠ (the surrealist artist) and WALL-E (the Pixar character), DALL-E represents a significant breakthrough in text-to-image synthesis capability.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Text-to-Image Generation**: Creates detailed images based on natural language descriptions\n",
    "- **Zero-Shot Visual Reasoning**: Can combine concepts, attributes, and styles it hasn't explicitly seen together\n",
    "- **Multimodal Understanding**: Bridges natural language processing with visual generation\n",
    "- **Progressive Enhancement**: Each version shows significant improvements in resolution, realism, and understanding\n",
    "- **Style Control**: Can generate images in specific artistic styles or visual aesthetics\n",
    "\n",
    "## Architecture Specifics\n",
    "\n",
    "### DALL-E 1\n",
    "The original DALL-E combined two key components:\n",
    "\n",
    "1. **Discrete VAE (dVAE)**: Compresses images into a lower-dimensional discrete latent space\n",
    "2. **Transformer Model**: 12-billion parameter autoregressive transformer that generates image tokens based on text tokens\n",
    "\n",
    "### DALL-E 2 and 3\n",
    "Later versions use a different architecture:\n",
    "\n",
    "1. **CLIP**: Contrastive Language-Image Pre-training to understand text-image relationships\n",
    "2. **Diffusion Models**: Generate images by gradually denoising random noise guided by text embeddings\n",
    "3. **Prior Model**: Maps text embeddings to image embeddings that guide the diffusion model\n",
    "\n",
    "The most recent versions incorporate additional refinement models to improve coherence, especially for text rendering within images.\n",
    "\n",
    "## Evolution\n",
    "\n",
    "- **DALL-E 1** (January 2021): Initial release with discrete VAE and transformer approach\n",
    "- **DALL-E 2** (April 2022): New architecture with CLIP and diffusion models, higher resolution and realism\n",
    "- **DALL-E 3** (October 2023): Significantly improved text rendering, visual quality, and prompt following\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "# Generate image with DALL-E\n",
    "response = openai.Image.create(\n",
    "    prompt=\"A surrealist painting of a cat playing chess with a robot on Mars\",\n",
    "    n=1,\n",
    "    size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "# Get the image URL\n",
    "image_url = response['data'][0]['url']\n",
    "\n",
    "# Download and display the image\n",
    "image_response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(image_response.content))\n",
    "img.show()\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "- Ramesh, A., et al. (2021). [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092). arXiv.\n",
    "- Ramesh, A., et al. (2022). [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125). arXiv.\n",
    "- OpenAI. (2023). [DALL-E 3 System Card](https://cdn.openai.com/papers/DALL-E_3_System_Card.pdf).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
