{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Architectures/gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gated Recurrent Unit (GRU)\n",
                "\n",
                "## Overview\n",
                "\n",
                "The Gated Recurrent Unit (GRU) is a gating mechanism in recurrent neural networks, introduced in 2014 by Cho et al. as a simpler alternative to the LSTM. GRUs aim to solve the vanishing gradient problem that comes with standard recurrent neural networks, while being more computationally efficient than LSTMs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Historical Context\n",
                "\n",
                "- **Introduced**: 2014 by Kyunghyun Cho and colleagues in their paper \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\"\n",
                "- **Motivation**: To create a more computationally efficient alternative to LSTMs without sacrificing too much performance\n",
                "- **Impact**: Quickly became popular for sequence modeling tasks including machine translation and speech processing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architecture\n",
                "\n",
                "GRU uses two gates to control the flow of information:\n",
                "\n",
                "1. **Reset Gate**: Determines how much of the previous state to forget\n",
                "2. **Update Gate**: Controls how much of the candidate activation to use in updating the cell state\n",
                "\n",
                "Unlike LSTM which has three gates (input, output, and forget gates) and separate cell state and hidden state, GRU combines these mechanisms into a simpler form.\n",
                "\n",
                "![GRU Architecture](https://www.researchgate.net/profile/Amir-Gandomi/publication/335969095/figure/fig3/AS:804158287929344@1568800566656/The-architecture-of-GRU-cell-At-time-step-t-the-GRU-cell-takes-xt-and-h-t-1-as-input.png)\n",
                "\n",
                "*Note: The above image is a reference - if used in actual implementation, ensure proper attribution.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Mathematical Formulation\n",
                "\n",
                "For a given time step $t$, a GRU computes the following:\n",
                "\n",
                "**Update Gate**:\n",
                "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
                "\n",
                "**Reset Gate**:\n",
                "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
                "\n",
                "**Candidate Hidden State**:\n",
                "$$\\tilde{h}_t = \\tanh(W \\cdot [r_t * h_{t-1}, x_t] + b)$$\n",
                "\n",
                "**Final Hidden State**:\n",
                "$$h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t$$\n",
                "\n",
                "Where:\n",
                "- $x_t$ is the input at time step $t$\n",
                "- $h_{t-1}$ is the previous hidden state\n",
                "- $z_t$ is the update gate vector\n",
                "- $r_t$ is the reset gate vector\n",
                "- $\\tilde{h}_t$ is the candidate hidden state\n",
                "- $\\sigma$ is the sigmoid activation function\n",
                "- $*$ denotes element-wise multiplication"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison with LSTM\n",
                "\n",
                "| Feature | GRU | LSTM |\n",
                "|---------|-----|------|\n",
                "| Number of gates | 2 (reset and update) | 3 (input, output, forget) |\n",
                "| State representation | Single hidden state | Separate cell state and hidden state |\n",
                "| Parameter count | Fewer parameters | More parameters |\n",
                "| Computational efficiency | Generally faster | More computationally intensive |\n",
                "| Performance on long sequences | Good | Slightly better in some cases |\n",
                "| Memory requirements | Lower | Higher |\n",
                "\n",
                "In practice, GRUs often achieve comparable performance to LSTMs with lower computational cost, making them preferable for applications with limited computational resources or when training on large datasets."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation Example\n",
                "\n",
                "### PyTorch Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class GRUModel(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
                "        super(GRUModel, self).__init__()\n",
                "        \n",
                "        # Defining the layers\n",
                "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
                "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
                "        self.relu = nn.ReLU()\n",
                "        \n",
                "    def forward(self, x, h):\n",
                "        # Forward pass through GRU\n",
                "        out, h = self.gru(x, h)        \n",
                "        # Use the final hidden state\n",
                "        out = self.fc(self.relu(out[:, -1]))\n",
                "        \n",
                "        return out, h\n",
                "    \n",
                "    def init_hidden(self, batch_size):\n",
                "        # Initialize hidden state with zeros\n",
                "        weight = next(self.parameters()).data\n",
                "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
                "        return hidden"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TensorFlow/Keras Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
                "\n",
                "def create_gru_model(input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
                "    model = Sequential()\n",
                "    \n",
                "    # Add GRU layers\n",
                "    for i in range(n_layers):\n",
                "        if i == 0:\n",
                "            model.add(GRU(hidden_dim, return_sequences=(i < n_layers-1), input_shape=(None, input_dim)))\n",
                "        else:\n",
                "            model.add(GRU(hidden_dim, return_sequences=(i < n_layers-1)))\n",
                "        \n",
                "        model.add(Dropout(drop_prob))\n",
                "    \n",
                "    # Output layer\n",
                "    model.add(Dense(output_dim, activation='softmax'))\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applications\n",
                "\n",
                "GRUs have been successfully applied in various domains:\n",
                "\n",
                "1. **Natural Language Processing**\n",
                "   - Machine translation\n",
                "   - Text generation\n",
                "   - Speech recognition\n",
                "   - Sentiment analysis\n",
                "\n",
                "2. **Time Series Analysis**\n",
                "   - Stock price prediction\n",
                "   - Weather forecasting\n",
                "   - Anomaly detection\n",
                "\n",
                "3. **Bioinformatics**\n",
                "   - Protein structure prediction\n",
                "   - Gene expression analysis\n",
                "\n",
                "4. **Robotics**\n",
                "   - Movement prediction\n",
                "   - Reinforcement learning agents"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advantages and Limitations\n",
                "\n",
                "### Advantages\n",
                "- More computationally efficient than LSTMs\n",
                "- Fewer parameters to train\n",
                "- Effectively captures medium-range dependencies\n",
                "- Less prone to overfitting on small datasets\n",
                "\n",
                "### Limitations\n",
                "- May not capture long-term dependencies as effectively as LSTMs in some cases\n",
                "- Still vulnerable to vanishing gradients over very long sequences\n",
                "- Limited context integration compared to attention-based models like Transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "- Cho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). [Learning phrase representations using RNN encoder-decoder for statistical machine translation](https://arxiv.org/abs/1406.1078). EMNLP.\n",
                "- Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555). NIPS Deep Learning Workshop.\n",
                "- Jozefowicz, R., Zaremba, W., & Sutskever, I. (2015). [An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf). ICML."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}