{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Components/artificial neuron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single Neuron ANN Model\n",
    "\n",
    "This presentation explains a single neuron ANN model including its mathematical formulation, a diagrammatic representation, and an implementation in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical Model\n",
    "\n",
    "We consider a single neuron described by an ANN model. In our context, the neuron computes a weighted sum of its input and then applies a nonlinear activation function.\n",
    "\n",
    "### Notation:\n",
    "\n",
    "- **x**: Input vector\n",
    "- **W**: Weight vector (parameters of the neuron)\n",
    "- **b**: Bias term\n",
    "- **f**: Activation function\n",
    "- **y**: Output of the neuron\n",
    "\n",
    "### Mathematical Expression\n",
    "\n",
    "$$ y = f(W \\cdot x + b) $$\n",
    "\n",
    "For example, if we use a sigmoid activation:\n",
    "\n",
    "$$ f(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "This model is fundamental in neural network theory and forms the basis of many more complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neuron\n",
    "\n",
    "Below is a Mermaid representation of a single neuron:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    x1[x1] --> neuron[Neuron]\n",
    "    x2[x2] --> neuron[Neuron]\n",
    "    dots[...] --> neuron[Neuron]\n",
    "    xn[xn] --> neuron[Neuron]\n",
    "    neuron[Neuron] --> y[y]\n",
    "    b[b] --> neuron[Neuron]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single Neuron ANN Model\n",
    "\n",
    "We can implement the single neuron using PyTorchâ€™s `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      " tensor([[ 0.8487,  0.6920, -0.3160],\n",
      "        [-2.1152, -0.3561,  0.4372],\n",
      "        [ 0.4913, -0.2041,  0.1198],\n",
      "        [ 1.2377,  1.1168, -0.2473],\n",
      "        [-1.0438, -1.3453, -0.4927],\n",
      "        [ 0.2484,  0.4397,  0.1124],\n",
      "        [ 0.6408,  0.4412, -0.2159],\n",
      "        [-0.7425, -0.2897,  0.0525],\n",
      "        [ 0.5229,  2.3022, -1.4689],\n",
      "        [-1.5867,  1.2032,  0.0845]])\n",
      "Output predictions:\n",
      " tensor([[0.4840],\n",
      "        [0.3244],\n",
      "        [0.3665],\n",
      "        [0.5083],\n",
      "        [0.3537],\n",
      "        [0.4150],\n",
      "        [0.4530],\n",
      "        [0.3690],\n",
      "        [0.7279],\n",
      "        [0.4786]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SingleNeuron(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SingleNeuron, self).__init__()\n",
    "        # Define a linear transformation: y = Wx + b\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the weighted sum plus bias\n",
    "        z = self.linear(x)\n",
    "        # Apply a sigmoid activation function\n",
    "        y = torch.sigmoid(z)\n",
    "        return y\n",
    "\n",
    "# Test the model with dummy data\n",
    "if __name__ == '__main__':\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Define input dimension\n",
    "    input_dim = 3  \n",
    "\n",
    "    # Create the model\n",
    "    model = SingleNeuron(input_dim=input_dim)\n",
    "    \n",
    "    # Create dummy input data: a batch of 10 samples\n",
    "    X = torch.randn(10, input_dim)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "\n",
    "    print('Input data:\\n', X)\n",
    "    print('Output predictions:\\n', outputs)\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
