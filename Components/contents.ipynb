{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Building Blocks\n",
    "\n",
    "This document catalogs the fundamental components that serve as building blocks for constructing neural networks. Think of these as the basic data types and operations that can be composed to create various network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Library\n",
    "\n",
    "### Data Representation Components\n",
    "1. [Input Coding](./coding.ipynb) - Methods for transforming raw data into network-compatible formats\n",
    "2. [Embedding](./embedding.ipynb) - Vector representations for categorical/discrete data types\n",
    "\n",
    "### Processing Units\n",
    "3. [Artificial Neuron](./artificial neuron.ipynb) - Basic computation units that form the foundation of networks\n",
    "4. [Activation Function](./activation.ipynb) - Non-linear transformations applied to neuron outputs\n",
    "5. [Convolution](./convolution.ipynb) - Specialized operations for feature extraction from structured data\n",
    "\n",
    "### Signal Manipulation\n",
    "6. [Pooling](./pooling.ipynb) - Downsampling operations to reduce dimensionality\n",
    "7. [Normalization](./normalization.ipynb) - Signal adjustment techniques for training stability\n",
    "8. [Dropout](./dropout.ipynb) - Stochastic signal interruption for regularization\n",
    "\n",
    "### Connection Patterns\n",
    "9. [Skippass](./skippass.ipynb) - Alternative connection pathways between non-adjacent layers\n",
    "10. [Positional Addition](./positional.ipynb) - Location-aware signal modification techniques\n",
    "\n",
    "### Output Processing\n",
    "11. [Softmax](./softmax.ipynb) - Probability distribution transformation for classification tasks\n",
    "12. [Attention](./attention.ipynb) [Multi-head](./attention_multihead.ipynb) [Example](./attention_example.ipynb) - Dynamic weighting mechanisms for input signal importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using These Components\n",
    "\n",
    "These components can be thought of as the primitive data types and operations in neural network construction. Just as programming languages have integers, strings, and arrays, neural networks have neurons, activations, and layers composed of these fundamental building blocks.\n",
    "\n",
    "When constructing a neural network architecture, you'll typically:\n",
    "1. Select appropriate input coding for your data\n",
    "2. Choose processing units for different layers\n",
    "3. Add signal manipulation components as needed\n",
    "4. Design connection patterns between components\n",
    "5. Apply output processing to get the desired results\n",
    "\n",
    "Each component notebook contains implementation details, mathematical foundations, and usage examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Considerations\n",
    "\n",
    "When implementing these components in actual neural network frameworks:\n",
    "\n",
    "### TensorFlow/Keras Implementation\n",
    "Most components are readily available as layers or functions:\n",
    "- Neurons: `Dense` layers\n",
    "- Convolution: `Conv1D`, `Conv2D`, `Conv3D`\n",
    "- Pooling: `MaxPooling2D`, `AveragePooling2D`\n",
    "- Normalization: `BatchNormalization`, `LayerNormalization`\n",
    "- Dropout: `Dropout` layer\n",
    "- Activation Functions: `Activation` layer or function arguments\n",
    "\n",
    "### PyTorch Implementation\n",
    "Components are available through modules:\n",
    "- Neurons: `nn.Linear`\n",
    "- Convolution: `nn.Conv1d`, `nn.Conv2d`, `nn.Conv3d`\n",
    "- Pooling: `nn.MaxPool2d`, `nn.AvgPool2d`\n",
    "- Normalization: `nn.BatchNorm2d`, `nn.LayerNorm`\n",
    "- Dropout: `nn.Dropout`\n",
    "- Activation Functions: `nn.ReLU`, `nn.Sigmoid`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Combinations\n",
    "\n",
    "These building blocks can be combined to create common layer types:\n",
    "\n",
    "- **Fully Connected Layer**: Artificial Neurons + Activation Function\n",
    "- **Convolutional Layer**: Convolution + Activation Function\n",
    "- **Residual Block**: Convolution + Normalization + Activation + Skippass\n",
    "- **Transformer Block**: Attention + Normalization + Feedforward + Positional Addition\n",
    "- **Embedding Layer**: Input Coding + Embedding\n",
    "\n",
    "Understanding these individual components helps in creating custom architectures tailored to specific problem domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). [Deep Learning](https://www.deeplearningbook.org/). MIT Press.\n",
    "- CS231n: [Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\n",
    "- Vaswani, A., et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). NIPS.\n",
    "- He, K., et al. (2015). [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385). CVPR."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
