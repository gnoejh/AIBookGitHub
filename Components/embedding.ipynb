{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gnoejh/ict1022/blob/main/Components/embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Embeddings\n",
    "\n",
    "Embeddings are dense vector representations of discrete variables, allowing us to represent words, sentences, or any other entities in a continuous vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of Mapping to Embedding Space\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input Space\n",
    "        A[\"cat\"] \n",
    "        B[\"dog\"]\n",
    "        C[\"kitten\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Embedding Space\n",
    "        direction LR\n",
    "        D[[\"[0.2, 0.8]\"]] \n",
    "        E[[\"[0.3, 0.6]\"]] \n",
    "        F[[\"[0.1, 0.9]\"]] \n",
    "    end\n",
    "    \n",
    "    A --> D\n",
    "    B --> E\n",
    "    C --> F\n",
    "    \n",
    "    style Input Space fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style Embedding Space fill:#bbf,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "The diagram shows how discrete input items (words) are mapped to continuous vector representations in the embedding space. Similar concepts (like 'cat' and 'kitten') are mapped to nearby points in the embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of Neural Network Embedding Process\n",
    "\n",
    "Concept of the embedding process can be represented as a neural network transformation:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input Layer\n",
    "        X[\"One-hot vector<br/>[1,0,0,...,0]\"] \n",
    "    end\n",
    "    \n",
    "    subgraph Hidden Layer\n",
    "        H1[\"W·X + b\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Output Layer\n",
    "        Y[\"Embedding vector<br/>[0.2, 0.8, ..., 0.3]\"]\n",
    "    end\n",
    "    \n",
    "    X --> |\"W∈ℝ^(d×v)\"| H1\n",
    "    H1 --> |\"activation\"| Y\n",
    "    \n",
    "    style Input Layer fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style Hidden Layer fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style Output Layer fill:#bfb,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $X \\in \\{0,1\\}^v$ is the one-hot input vector (vocabulary size $v$)\n",
    "- $W \\in \\mathbb{R}^{d\\times v}$ is the weight matrix ($d$ is embedding dimension)\n",
    "- $b \\in \\mathbb{R}^d$ is the bias vector\n",
    "- The embedding vector $Y = f(WX + b)$ where $f$ is an activation function\n",
    "\n",
    "This transformation learns to map discrete tokens to continuous vectors while preserving semantic relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Theory\n",
    "\n",
    "An embedding is a mathematical function that maps discrete objects (words, sentences, images) into continuous vector spaces while preserving semantic relationships:\n",
    "\n",
    "$f: X \\rightarrow \\mathbb{R}^n$\n",
    "\n",
    "where $X$ is the input space and $n$ is the dimensionality of the embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network–Based Embedding Methods\n",
    "\n",
    "Below is an organized summary of various embedding methods using neural networks, suitable for inclusion in your Jupyter Notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Fully Connected (FC) Network Embeddings\n",
    "- **Description:**  \n",
    "  Feed-forward networks with one or more hidden layers that transform input data into a lower-dimensional, dense representation.\n",
    "- **Use Cases:**  \n",
    "  - Structured or tabular data  \n",
    "  - Intermediate representations in supervised tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Autoencoder (AE) Embeddings\n",
    "- **Description:**  \n",
    "  Neural networks that learn to compress data into a latent space (encoder) and then reconstruct the original input (decoder).  \n",
    "- **Variations:**  \n",
    "  - **Denoising Autoencoders:** Learn robust features by reconstructing clean inputs from corrupted ones.  \n",
    "  - **Variational Autoencoders (VAEs):** Impose a probabilistic structure on the latent space for smooth and continuous embeddings.\n",
    "- **Use Cases:**  \n",
    "  - Dimensionality reduction  \n",
    "  - Unsupervised feature learning  \n",
    "  - Data compression\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Transformer-Based Embeddings\n",
    "- **Description:**  \n",
    "  Models built on self-attention mechanisms that generate context-aware representations for sequences.  \n",
    "- **Key Components:**  \n",
    "  - **Token Embeddings:** Convert tokens to dense vectors.  \n",
    "  - **Positional Embeddings:** Encode the order of tokens in the sequence.\n",
    "- **Use Cases:**  \n",
    "  - Natural language processing  \n",
    "  - Context-dependent representation learning\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Convolutional Neural Network (CNN) Embeddings\n",
    "- **Description:**  \n",
    "  Networks using convolutional layers to capture spatial hierarchies in data, especially images.\n",
    "- **Use Cases:**  \n",
    "  - Image classification  \n",
    "  - Object detection  \n",
    "  - Other vision-related tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Recurrent Neural Network (RNN) Embeddings\n",
    "- **Description:**  \n",
    "  Networks designed for sequential data processing, using hidden states to capture temporal dependencies.  \n",
    "- **Architectures:**  \n",
    "  - LSTMs (Long Short-Term Memory)  \n",
    "  - GRUs (Gated Recurrent Units)\n",
    "- **Use Cases:**  \n",
    "  - Time series analysis  \n",
    "  - Speech recognition  \n",
    "  - Sequential modeling in NLP\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Graph Neural Network (GNN) Embeddings\n",
    "- **Description:**  \n",
    "  Models that learn representations for nodes, edges, or entire graphs by leveraging the relational structure of graph data.\n",
    "- **Examples:**  \n",
    "  - Graph Convolutional Networks (GCNs)  \n",
    "  - GraphSAGE\n",
    "- **Use Cases:**  \n",
    "  - Social network analysis  \n",
    "  - Recommendation systems  \n",
    "  - Any tasks involving relational data\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Metric Learning & Contrastive Embeddings\n",
    "- **Description:**  \n",
    "  Techniques that optimize embedding spaces by enforcing similarity between related instances and dissimilarity between unrelated ones.\n",
    "- **Techniques:**  \n",
    "  - **Siamese Networks/Triplet Loss:** Learn embeddings that bring similar items closer while pushing dissimilar items apart.  \n",
    "  - **Contrastive Learning:** Methods like SimCLR compare different augmented views of the same instance.\n",
    "- **Use Cases:**  \n",
    "  - Face recognition  \n",
    "  - Image retrieval  \n",
    "  - Similarity-based tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Self-Supervised Learning Embeddings\n",
    "- **Description:**  \n",
    "  Models that learn useful representations from unlabeled data by solving pretext tasks or predicting parts of the input.\n",
    "- **Examples:**  \n",
    "  - **Vision:** BYOL, DINO  \n",
    "  - **NLP:** Masked language modeling\n",
    "- **Use Cases:**  \n",
    "  - Pre-training for downstream tasks  \n",
    "  - Enhancing robustness and generalization\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Hybrid and Domain-Specific Embeddings\n",
    "- **Description:**  \n",
    "  Methods that combine multiple modalities or tailor embeddings to specific domains.\n",
    "- **Examples:**  \n",
    "  - **Multimodal Embeddings:** Joint embeddings of text and images, e.g., CLIP.  \n",
    "  - **Adversarial Networks:** Using intermediate features (e.g., from the discriminator) as embeddings.\n",
    "- **Use Cases:**  \n",
    "  - Multimodal data integration  \n",
    "  - Domain-specific representation learning\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (3, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load sentence transformer model\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create embeddings for sentences\n",
    "sentences = [\n",
    "    'This is a sample sentence.',\n",
    "    'Another different sentence.',\n",
    "    'This sentence is similar to the first one.'\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'I love programming' and 'I enjoy coding': 0.817\n",
      "Similarity between 'The weather is nice' and 'It's a beautiful day': 0.471\n",
      "Similarity between 'The weather is nice' and 'Python is awesome': 0.179\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_similarity(sent1, sent2):\n",
    "    emb1 = model.encode([sent1])\n",
    "    emb2 = model.encode([sent2])\n",
    "    return cosine_similarity(emb1, emb2)[0][0]\n",
    "\n",
    "# Example similarities\n",
    "pairs = [\n",
    "    (\"I love programming\", \"I enjoy coding\"),\n",
    "    (\"The weather is nice\", \"It's a beautiful day\"),\n",
    "    (\"The weather is nice\", \"Python is awesome\")\n",
    "]\n",
    "\n",
    "for s1, s2 in pairs:\n",
    "    sim = compute_similarity(s1, s2)\n",
    "    print(f\"Similarity between '{s1}' and '{s2}': {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: AI and machine learning\n",
      "\n",
      "Score: 0.651 | Document: Machine learning is fascinating\n",
      "Score: 0.582 | Document: AI can solve complex problems\n",
      "Score: 0.485 | Document: Deep neural networks are powerful\n"
     ]
    }
   ],
   "source": [
    "def semantic_search(query, corpus, top_k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    corpus_embeddings = model.encode(corpus)\n",
    "    \n",
    "    similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    return [(corpus[i], similarities[i]) for i in top_results]\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Deep neural networks are powerful\",\n",
    "    \"Natural language processing is amazing\",\n",
    "    \"The cat is sleeping on the mat\",\n",
    "    \"AI can solve complex problems\"\n",
    "]\n",
    "\n",
    "query = \"AI and machine learning\"\n",
    "results = semantic_search(query, corpus)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score:.3f} | Document: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0:\n",
      "- Cats are wonderful pets\n",
      "- Dogs make loyal companions\n",
      "- Kittens play with yarn\n",
      "\n",
      "Cluster 1:\n",
      "- Machine learning models need data\n",
      "- AI requires computational power\n",
      "- Neural networks learn patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n",
      "\n",
      "KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"Machine learning models need data\",\n",
    "    \"AI requires computational power\",\n",
    "    \"Cats are wonderful pets\",\n",
    "    \"Dogs make loyal companions\",\n",
    "    \"Neural networks learn patterns\",\n",
    "    \"Kittens play with yarn\"\n",
    "]\n",
    "\n",
    "# Create embeddings\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "# Cluster documents\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(doc_embeddings)\n",
    "\n",
    "# Print clusters\n",
    "for i in range(n_clusters):\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    for j, doc in enumerate(documents):\n",
    "        if clusters[j] == i:\n",
    "            print(f\"- {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (0.2.54)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: plotly in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (0.0.9)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (2.4.2)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hjeong\\anaconda3\\envs\\py312sb3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "                                 AAPL        MSFT       GOOGL        AMZN  \\\n",
      "Date                                                                        \n",
      "2024-02-27 00:00:00-05:00  181.771698  404.392639  138.378372  173.539993   \n",
      "2024-02-28 00:00:00-05:00  180.567383  404.630798  135.887405  173.160004   \n",
      "2024-02-29 00:00:00-05:00  179.900543  410.505951  137.959885  176.759995   \n",
      "2024-03-01 00:00:00-05:00  178.815659  412.351837  136.644653  178.220001   \n",
      "2024-03-04 00:00:00-05:00  174.277100  411.776245  132.868347  177.580002   \n",
      "\n",
      "                                 TSLA       NVDA        AVGO         AMD  \\\n",
      "Date                                                                       \n",
      "2024-02-27 00:00:00-05:00  199.729996  78.678688  127.900284  178.000000   \n",
      "2024-02-28 00:00:00-05:00  202.039993  77.640991  127.228340  176.539993   \n",
      "2024-02-29 00:00:00-05:00  201.880005  79.089577  128.320618  192.529999   \n",
      "2024-03-01 00:00:00-05:00  202.639999  82.255676  138.057480  202.639999   \n",
      "2024-03-04 00:00:00-05:00  188.139999  85.212830  138.362366  205.360001   \n",
      "\n",
      "                                 QCOM         IBM   IONQ  RGTI       PLTR  \\\n",
      "Date                                                                        \n",
      "2024-02-27 00:00:00-05:00  155.238678  178.855194  11.57  1.83  24.530001   \n",
      "2024-02-28 00:00:00-05:00  153.612366  179.271194  11.26  2.01  24.420000   \n",
      "2024-02-29 00:00:00-05:00  155.524506  179.009995  10.36  1.88  25.080000   \n",
      "2024-03-01 00:00:00-05:00  160.748413  182.076843  10.66  1.86  24.930000   \n",
      "2024-03-04 00:00:00-05:00  164.168579  186.778732  10.09  1.69  24.040001   \n",
      "\n",
      "                           ACHR   RDW   BKSY  VUZI  NNDM   SNDL  TLRY  \n",
      "Date                                                                   \n",
      "2024-02-27 00:00:00-05:00  4.80  3.43  13.12  1.65  2.88  1.425  1.81  \n",
      "2024-02-28 00:00:00-05:00  4.72  3.36  11.76  1.69  2.84  1.380  1.75  \n",
      "2024-02-29 00:00:00-05:00  4.83  3.23  11.36  1.69  2.85  1.350  1.73  \n",
      "2024-03-01 00:00:00-05:00  4.55  3.09  11.36  1.73  2.89  1.380  1.73  \n",
      "2024-03-04 00:00:00-05:00  4.48  3.04  12.08  1.58  2.81  1.360  1.70  \n",
      "Data tensor shape (stocks, time_series_length): torch.Size([20, 251])\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install yfinance plotly scikit-learn\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.express as px\n",
    "\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \n",
    "           \"NVDA\", \"AVGO\", \"AMD\", \"QCOM\", \"IBM\", \n",
    "           \"IONQ\", \"RGTI\", \"PLTR\", \"ACHR\", \"RDW\",\n",
    "           \"BKSY\", \"VUZI\", \"NNDM\", \"SNDL\", \"TLRY\"]\n",
    "\n",
    "# Download closing prices for each ticker\n",
    "data = {}\n",
    "for ticker in tickers:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    df = stock.history(period=\"1y\")\n",
    "    data[ticker] = df[\"Close\"]\n",
    "\n",
    "# Combine data into a single DataFrame (dates as rows, tickers as columns)\n",
    "df_stocks = pd.DataFrame(data)\n",
    "df_stocks = df_stocks.ffill().dropna()  # simple cleaning\n",
    "print(df_stocks.head())\n",
    "\n",
    "# Scale each column (stock) independently\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(df_stocks)\n",
    "\n",
    "# Transpose so each row represents one stock’s time series\n",
    "# df_stocks.shape: (time_steps, tickers) -> (tickers, time_steps)\n",
    "data_scaled = data_scaled.T\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data_tensor = torch.tensor(data_scaled, dtype=torch.float32)\n",
    "print(\"Data tensor shape (stocks, time_series_length):\", data_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=251, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=251, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 100/1000, Loss: 0.022534\n",
      "Epoch 200/1000, Loss: 0.010809\n",
      "Epoch 300/1000, Loss: 0.008030\n",
      "Epoch 400/1000, Loss: 0.007145\n",
      "Epoch 500/1000, Loss: 0.005608\n",
      "Epoch 600/1000, Loss: 0.004437\n",
      "Epoch 700/1000, Loss: 0.003480\n",
      "Epoch 800/1000, Loss: 0.002709\n",
      "Epoch 900/1000, Loss: 0.002147\n",
      "Epoch 1000/1000, Loss: 0.001686\n",
      "Embeddings shape: (20, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Dim1=%{x}<br>Dim2=%{y}<br>Dim3=%{z}<br>Ticker=%{text}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "",
         "scene": "scene",
         "showlegend": false,
         "text": [
          "AAPL",
          "MSFT",
          "GOOGL",
          "AMZN",
          "TSLA",
          "NVDA",
          "AVGO",
          "AMD",
          "QCOM",
          "IBM",
          "IONQ",
          "RGTI",
          "PLTR",
          "ACHR",
          "RDW",
          "BKSY",
          "VUZI",
          "NNDM",
          "SNDL",
          "TLRY"
         ],
         "type": "scatter3d",
         "x": [
          5.5666656494140625,
          4.493614673614502,
          4.324203968048096,
          3.146815538406372,
          2.9320805072784424,
          6.1388258934021,
          3.464465856552124,
          3.2464003562927246,
          3.3077127933502197,
          3.386993885040283,
          1.3511359691619873,
          0.4390975832939148,
          1.855781078338623,
          1.0765002965927124,
          2.151942729949951,
          1.523228406906128,
          1.0101350545883179,
          1.3935751914978027,
          3.691295862197876,
          2.7504236698150635
         ],
         "y": [
          -2.6991848945617676,
          -0.23611019551753998,
          -1.357709288597107,
          -1.735843300819397,
          -1.878117561340332,
          -2.378218412399292,
          -1.7207207679748535,
          2.1272807121276855,
          1.6312922239303589,
          -2.9018571376800537,
          -1.7424496412277222,
          -0.8618274331092834,
          -1.7841607332229614,
          -1.4076848030090332,
          -1.5931365489959717,
          -0.8288359642028809,
          -1.0459636449813843,
          1.7961212396621704,
          -0.6878610253334045,
          1.1452388763427734
         ],
         "z": [
          -0.05261576175689697,
          -1.4755303859710693,
          -1.9075168371200562,
          -0.7253486514091492,
          -0.2063111960887909,
          -0.45277902483940125,
          -0.1784593164920807,
          -4.093658924102783,
          -2.1575238704681396,
          1.022911787033081,
          -0.16631627082824707,
          -0.27040401101112366,
          0.7034494280815125,
          -0.8982517123222351,
          0.33871468901634216,
          -2.18796706199646,
          -0.6181311011314392,
          -4.403567314147949,
          -3.3278403282165527,
          -4.013914585113525
         ]
        }
       ],
       "layout": {
        "height": 1000,
        "legend": {
         "tracegroupgap": 0
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "Dim1"
          }
         },
         "yaxis": {
          "title": {
           "text": "Dim2"
          }
         },
         "zaxis": {
          "title": {
           "text": "Dim3"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "3D Stock Embeddings (Autoencoder)"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: compress input into embedding_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, embedding_dim)\n",
    "        )\n",
    "        # Decoder: reconstruct input from embedding\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, z\n",
    "\n",
    "input_dim = data_tensor.shape[1]  # number of time steps\n",
    "embedding_dim = 3\n",
    "model = Autoencoder(input_dim, embedding_dim)\n",
    "print(model)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    reconstructed, _ = model(data_tensor)\n",
    "    loss = criterion(reconstructed, data_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, embeddings = model(data_tensor)\n",
    "embeddings = embeddings.numpy()\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_embeddings = pd.DataFrame(embeddings, columns=[\"Dim1\", \"Dim2\", \"Dim3\"])\n",
    "df_embeddings[\"Ticker\"] = tickers  # Make sure the order matches\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = px.scatter_3d(df_embeddings, x=\"Dim1\", y=\"Dim2\", z=\"Dim3\", text=\"Ticker\", title=\"3D Stock Embeddings (Autoencoder)\", height=1000)\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
