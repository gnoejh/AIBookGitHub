{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Frameworks\n",
    "\n",
    "This notebook explores popular Deep Learning frameworks, their key features, and provides code comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deep Learning frameworks provide building blocks for designing, training, and evaluating deep neural networks. They abstract away many of the complex implementation details, allowing researchers and developers to focus on model architecture and applications.\n",
    "\n",
    "The most popular frameworks include:\n",
    "- **PyTorch**: Developed by Facebook's AI Research lab, known for its dynamic computation graph and pythonic interface\n",
    "- **TensorFlow/Keras**: Developed by Google, offering both low-level and high-level APIs with strong production deployment capabilities\n",
    "- **HuggingFace Transformers**: Specialized in NLP models and transformers architecture, offering pre-trained models and easy fine-tuning\n",
    "- **JAX**: Developed by Google, focused on high-performance numerical computing with auto-differentiation\n",
    "- **MXNet**: Supported by Amazon, designed for flexibility and efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Deep Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | PyTorch | TensorFlow/Keras | HuggingFace | JAX | MXNet |\n",
    "|---------|---------|-----------------|------------|-----|-------|\n",
    "| **Primary Developer** | Facebook | Google | HuggingFace | Google | Apache/Amazon |\n",
    "| **Core Design** | Dynamic computation graph | Static (TF1) & Dynamic (TF2) graph | Built on PyTorch/TF | Functional programming | Hybrid |\n",
    "| **Ease of Use** | High | Medium (TF), High (Keras) | High | Medium | Medium |\n",
    "| **Debugging** | Easy (pythonic) | Moderate | Easy | Moderate | Moderate |\n",
    "| **Performance** | Good | Very good | Depends on backend | Excellent | Good |\n",
    "| **Production Deployment** | Improving with TorchServe | Excellent (TF Serving) | Via backend | Limited | Good (MXNet Model Server) |\n",
    "| **Mobile Support** | Yes (PyTorch Mobile) | Yes (TFLite) | Limited | Limited | Yes |\n",
    "| **GPU Support** | Excellent, native CUDA integration | Excellent, uses CUDA/cuDNN | Inherits from backend | Good, XLA compilation | Good, multiple GPU support |\n",
    "| **Pre-trained Models** | Many | Many | Extensive | Limited | Some |\n",
    "| **Community Size** | Large | Very large | Growing rapidly | Growing | Moderate |\n",
    "| **Best For** | Research, prototyping | Production, large-scale deployment | NLP tasks, transformers | High performance computing | Cloud & edge deployment |\n",
    "| **Learning Curve** | Gentle | Steeper | Gentle for specific tasks | Steeper | Moderate |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Comparison: Simple Linear Regression\n",
    "\n",
    "Let's implement a simple linear regression model in different frameworks to compare their syntax and approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.0130\n",
      "Epoch 40/100, Loss: 0.0108\n",
      "Epoch 60/100, Loss: 0.0096\n",
      "Epoch 80/100, Loss: 0.0089\n",
      "Epoch 100/100, Loss: 0.0086\n",
      "PyTorch Result: y = 1.8802x + 1.0589\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "# Define the model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_tensor)\n",
    "    loss = criterion(y_pred, y_tensor)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Print results\n",
    "w = model.linear.weight.item()\n",
    "b = model.linear.bias.item()\n",
    "print(f'PyTorch Result: y = {w:.4f}x + {b:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hjeong\\anaconda3\\envs\\py312sb3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.0460\n",
      "Epoch 40/100, Loss: 0.0128\n",
      "Epoch 60/100, Loss: 0.0086\n",
      "Epoch 80/100, Loss: 0.0083\n",
      "Epoch 100/100, Loss: 0.0083\n",
      "TensorFlow Result: y = 1.9884x + 1.0193\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data (same as before)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, input_shape=[1])\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# Print loss every 20 epochs\n",
    "for i in range(19, 100, 20):\n",
    "    print(f\"Epoch {i+1}/100, Loss: {history.history['loss'][i]:.4f}\")\n",
    "\n",
    "# Get the learned parameters\n",
    "w, b = model.get_weights()\n",
    "print(f'TensorFlow Result: y = {w[0][0]:.4f}x + {b[0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace for NLP Tasks\n",
    "\n",
    "While we've shown how to use HuggingFace for a simple regression task, its real strength lies in NLP tasks with pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 13, 768])\n",
      "This represents embeddings for each token in the input sentence\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example text\n",
    "text = \"HuggingFace provides easy access to pre-trained models.\"\n",
    "\n",
    "# Tokenize and encode\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get model outputs (last hidden states)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Print shape of output embeddings\n",
    "print(f\"Output shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"This represents embeddings for each token in the input sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Choosing the right deep learning framework depends on your specific needs:\n",
    "\n",
    "- **PyTorch**: Best for research, rapid prototyping, and when you need maximum flexibility\n",
    "- **TensorFlow/Keras**: Ideal when deployment is a priority or you need a well-established ecosystem\n",
    "- **JAX**: Great for high-performance computing and when you need fine-grained control over computations\n",
    "- **HuggingFace**: Perfect for NLP tasks and when you want to leverage pre-trained transformer models. Note that HuggingFace is **not a standalone framework** but rather a library that builds on PyTorch and TensorFlow\n",
    "\n",
    "For beginners, Keras offers the gentlest learning curve, while PyTorch provides a more pythonic experience that many researchers prefer. HuggingFace is the go-to choice for NLP tasks, making state-of-the-art models accessible with minimal code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
